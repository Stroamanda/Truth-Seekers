{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06db8075-b011-48dc-b55f-dfbe3c53c600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b56a26-dc6a-43bb-8398-a76752038969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 134,198\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH = \"Dataset/Truth_Seeker_Model_Dataset.csv\"\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df = df.sample(frac=1)\n",
    "df = df[~df['5_label_majority_answer'].isin(['NO MAJORITY'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cffe6a-71d1-425e-9f78-a089d4e5e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['target'].astype(str) + ' Statement: ' + df['statement'] + ' | Tweet: ' +df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13803549-d9c5-4d3f-8d3c-c496cba66ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic. | Tweet: @POTUS Biden Blunders - 6 Month Update\\n\\nInflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8caac61c-165e-4325-83cd-08de139abaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"BinaryNumTarget\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53567947-0e0f-4120-8a9b-92c070809b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truthfulness_4way(row):\n",
    "    if row['target'] == True:\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return \"True\"\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return \"False\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return \"Mostly True\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return \"Mostly False\"\n",
    "    else:\n",
    "        if row['5_label_majority_answer'] == 'Agree':\n",
    "            return \"False\"\n",
    "        elif row['5_label_majority_answer'] == 'Disagree':\n",
    "            return \"True\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Agree':\n",
    "            return \"Mostly False\"\n",
    "        elif row['5_label_majority_answer'] == 'Mostly Disagree':\n",
    "            return \"Mostly True\"\n",
    "\n",
    "def generate_truthfulness_2way(row):\n",
    "    if row['target'] == True:\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"True\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"False\"\n",
    "    else:\n",
    "        if row['3_label_majority_answer'] == 'Agree':\n",
    "            return \"False\"\n",
    "        elif row['3_label_majority_answer'] == 'Disagree':\n",
    "            return \"True\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af7d7a7-da69-45f3-a5c2-054e4e63ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2['4-way-label'] = df.apply(lambda x: generate_truthfulness_4way(x), axis=1)\n",
    "df2['2-way-label'] = df.apply(lambda x: generate_truthfulness_2way(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af0b815c-bc6b-4e8a-bbfc-74034db88b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4-way-label</th>\n",
       "      <th>2-way-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13409</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22611</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60665</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26137</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101411</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28901</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33151</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17833</th>\n",
       "      <td>Mostly True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22158</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111593 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        4-way-label 2-way-label\n",
       "13409          True        True\n",
       "22611          True        True\n",
       "60665          True        True\n",
       "26137          True        True\n",
       "101411        False       False\n",
       "...             ...         ...\n",
       "28901          True        True\n",
       "33151   Mostly True        True\n",
       "17833   Mostly True        True\n",
       "22158          True        True\n",
       "5338           True        True\n",
       "\n",
       "[111593 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f91527-667e-4278-8f96-d58d438f7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['2-way-label'] = df2['2-way-label'].replace({'True': 0, 'False': 1})\n",
    "df2['4-way-label'] = df2['4-way-label'].replace({'True': 0, 'False': 1, 'Mostly True': .333, 'Mostly False': .666})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5affeb06-8cd1-4b58-9000-09419efac91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4-way-label</th>\n",
       "      <th>2-way-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13409</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22611</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60665</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26137</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101411</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28901</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33151</th>\n",
       "      <td>0.666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17833</th>\n",
       "      <td>0.666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22158</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111593 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        4-way-label  2-way-label\n",
       "13409         0.000            0\n",
       "22611         0.000            0\n",
       "60665         0.000            0\n",
       "26137         0.000            0\n",
       "101411        1.000            1\n",
       "...             ...          ...\n",
       "28901         0.000            0\n",
       "33151         0.666            0\n",
       "17833         0.666            0\n",
       "22158         0.000            0\n",
       "5338          0.000            0\n",
       "\n",
       "[111593 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15df6ee4-c79f-478a-b893-f5c29195ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        4-way-label  2-way-label\n",
      "13409         0.000            0\n",
      "22611         0.000            0\n",
      "60665         0.000            0\n",
      "26137         0.000            0\n",
      "101411        1.000            1\n",
      "...             ...          ...\n",
      "28901         0.000            0\n",
      "33151         0.666            0\n",
      "17833         0.666            0\n",
      "22158         0.000            0\n",
      "5338          0.000            0\n",
      "\n",
      "[111593 rows x 2 columns]\n",
      "        4-way-label  2-way-label\n",
      "13409         0.000            0\n",
      "22611         0.000            0\n",
      "60665         0.000            0\n",
      "26137         0.000            0\n",
      "101411        1.000            1\n",
      "...             ...          ...\n",
      "28901         0.000            0\n",
      "33151         0.666            0\n",
      "17833         0.666            0\n",
      "22158         0.000            0\n",
      "5338          0.000            0\n",
      "\n",
      "[111593 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2['2-way-label'].plot(kind='hist')\n",
    "print(df2)\n",
    "df2['4-way-label'].plot(kind='hist')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cefbc76-0e2c-4afc-bc90-c1cd2c172cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels = df2['2-way-label'].values\n",
    "labels = df2['2-way-label'].values\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01944a71-51db-47e2-b7f8-41fe07d1609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abate\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Loading Distil BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Load the Distil BERT tokenizer.\n",
    "print('Loading Distil BERT tokenizer...')\n",
    "d_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5031cf4e-3047-4bdd-99d7-90d1f2fb2d87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  True Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic. | Tweet: @POTUS Biden Blunders - 6 Month Update\n",
      "\n",
      "Inflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?\n",
      "Tokenized:  ['true', 'statement', ':', 'end', 'of', 'ev', '##iction', 'mora', '##torium', 'means', 'millions', 'of', 'americans', 'could', 'lose', 'their', 'housing', 'in', 'the', 'middle', 'of', 'a', 'pan', '##de', '##mic', '.', '|', 't', '##wee', '##t', ':', '@', 'pot', '##us', 'bid', '##en', 'blu', '##nder', '##s', '-', '6', 'month', 'update', 'inflation', ',', 'delta', 'mis', '##mana', '##gement', ',', 'co', '##vid', 'for', 'kids', ',', 'abandoning', 'americans', 'in', 'afghanistan', ',', 'arm', '##ing', 'the', 'taliban', ',', 's', '.', 'border', 'crisis', ',', 'breaking', 'job', 'growth', ',', 'abuse', 'of', 'power', '(', 'many', 'ex', '##ec', 'orders', ',', '$', '3', '.', '5', '##t', 'through', 'reconciliation', ',', 'ev', '##iction', 'mora', '##torium', ')', '.', '.', '.', 'what', 'did', 'i', 'miss', '?']\n",
      "Token IDs:  [2995, 4861, 1024, 2203, 1997, 23408, 28097, 26821, 24390, 2965, 8817, 1997, 4841, 2071, 4558, 2037, 3847, 1999, 1996, 2690, 1997, 1037, 6090, 3207, 7712, 1012, 1064, 1056, 28394, 2102, 1024, 1030, 8962, 2271, 7226, 2368, 14154, 11563, 2015, 1011, 1020, 3204, 10651, 14200, 1010, 7160, 28616, 24805, 20511, 1010, 2522, 17258, 2005, 4268, 1010, 19816, 4841, 1999, 7041, 1010, 2849, 2075, 1996, 16597, 1010, 1055, 1012, 3675, 5325, 1010, 4911, 3105, 3930, 1010, 6905, 1997, 2373, 1006, 2116, 4654, 8586, 4449, 1010, 1002, 1017, 1012, 1019, 2102, 2083, 16088, 1010, 23408, 28097, 26821, 24390, 1007, 1012, 1012, 1012, 2054, 2106, 1045, 3335, 1029]\n",
      " Original:  True Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic. | Tweet: @POTUS Biden Blunders - 6 Month Update\n",
      "\n",
      "Inflation, Delta mismanagement, COVID for kids, Abandoning Americans in Afghanistan, Arming the Taliban, S. Border crisis, Breaking job growth, Abuse of power (Many Exec Orders, $3.5T through Reconciliation, Eviction Moratorium)...what did I miss?\n",
      "Tokenized:  ['true', 'statement', ':', 'end', 'of', 'ev', '##iction', 'mora', '##torium', 'means', 'millions', 'of', 'americans', 'could', 'lose', 'their', 'housing', 'in', 'the', 'middle', 'of', 'a', 'pan', '##de', '##mic', '.', '|', 't', '##wee', '##t', ':', '@', 'pot', '##us', 'bid', '##en', 'blu', '##nder', '##s', '-', '6', 'month', 'update', 'inflation', ',', 'delta', 'mis', '##mana', '##gement', ',', 'co', '##vid', 'for', 'kids', ',', 'abandoning', 'americans', 'in', 'afghanistan', ',', 'arm', '##ing', 'the', 'taliban', ',', 's', '.', 'border', 'crisis', ',', 'breaking', 'job', 'growth', ',', 'abuse', 'of', 'power', '(', 'many', 'ex', '##ec', 'orders', ',', '$', '3', '.', '5', '##t', 'through', 'reconciliation', ',', 'ev', '##iction', 'mora', '##torium', ')', '.', '.', '.', 'what', 'did', 'i', 'miss', '?']\n",
      "Token IDs:  [2995, 4861, 1024, 2203, 1997, 23408, 28097, 26821, 24390, 2965, 8817, 1997, 4841, 2071, 4558, 2037, 3847, 1999, 1996, 2690, 1997, 1037, 6090, 3207, 7712, 1012, 1064, 1056, 28394, 2102, 1024, 1030, 8962, 2271, 7226, 2368, 14154, 11563, 2015, 1011, 1020, 3204, 10651, 14200, 1010, 7160, 28616, 24805, 20511, 1010, 2522, 17258, 2005, 4268, 1010, 19816, 4841, 1999, 7041, 1010, 2849, 2075, 1996, 16597, 1010, 1055, 1012, 3675, 5325, 1010, 4911, 3105, 3930, 1010, 6905, 1997, 2373, 1006, 2116, 4654, 8586, 4449, 1010, 1002, 1017, 1012, 1019, 2102, 2083, 16088, 1010, 23408, 28097, 26821, 24390, 1007, 1012, 1012, 1012, 2054, 2106, 1045, 3335, 1029]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n",
    "\n",
    "\n",
    "# Print the original sentence.\n",
    "print(' Original: ', sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', d_tokenizer.tokenize(sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', d_tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea3b9c89-44de-4609-a518-3052a589908f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2722/111593 [00:04<03:05, 586.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m tqdm(sentences):\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Tokenize the text and add `[CLS]` and `[SEP]` tokens.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(sent, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 10\u001b[0m     d_input_ids \u001b[38;5;241m=\u001b[39m d_tokenizer\u001b[38;5;241m.\u001b[39mencode(sent, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Update the maximum sentence length.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_len, \u001b[38;5;28mlen\u001b[39m(input_ids))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2783\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[0m\n\u001b[0;32m   2745\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[0;32m   2746\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[0;32m   2747\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2766\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2767\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m   2768\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2769\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[0;32m   2770\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2781\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[0;32m   2782\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2783\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2784\u001b[0m         text,\n\u001b[0;32m   2785\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   2786\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2787\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2788\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2789\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2790\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2791\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   2792\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2793\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2794\u001b[0m     )\n\u001b[0;32m   2796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3202\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3192\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3193\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3194\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3195\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3200\u001b[0m )\n\u001b[1;32m-> 3202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[0;32m   3203\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3204\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   3205\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3206\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3207\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3208\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3209\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3210\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3211\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3212\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   3213\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3214\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3215\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3216\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3217\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3218\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3219\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3220\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3221\u001b[0m     split_special_tokens\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_special_tokens),\n\u001b[0;32m   3222\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3223\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\transformers\\tokenization_utils.py:798\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    792\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m     )\n\u001b[1;32m--> 798\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    799\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[0;32m    802\u001b[0m     first_ids,\n\u001b[0;32m    803\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    819\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\transformers\\tokenization_utils.py:765\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 765\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    766\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\transformers\\tokenization_utils.py:695\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize(token))\n\u001b[0;32m    696\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\transformers\\models\\distilbert\\tokenization_distilbert.py:167\u001b[0m, in \u001b[0;36mDistilBertTokenizer._tokenize\u001b[1;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[0;32m    165\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(\n\u001b[0;32m    168\u001b[0m         text, never_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m split_special_tokens \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     ):\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[0;32m    172\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\transformers\\models\\distilbert\\tokenization_distilbert.py:353\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# union() returns a new set by concatenating the two sets.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m never_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(never_split)) \u001b[38;5;28;01mif\u001b[39;00m never_split \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\n\u001b[1;32m--> 353\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_text(text)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\transformers\\models\\distilbert\\tokenization_distilbert.py:455\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[0;32m    454\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFFFD\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m _is_control(char):\n\u001b[0;32m    456\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_whitespace(char):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\transformers\\tokenization_utils.py:359\u001b[0m, in \u001b[0;36m_is_control\u001b[1;34m(char)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    358\u001b[0m cat \u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mcategory(char)\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cat\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "d_max_len = 0\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    d_input_ids = d_tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    d_max_len = max(d_max_len, len(d_input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)\n",
    "print('Max sentence length: ', d_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca91cb45-1857-4574-a619-8cbba56273d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/111593 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\abate\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2829: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 111593/111593 [03:44<00:00, 496.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "MAX_SENTENCE_LENGTH = 410\n",
    "d_input_ids = []\n",
    "d_attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in tqdm(sentences):\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # also for distil\n",
    "    d_encoded_dict = d_tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = MAX_SENTENCE_LENGTH,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "      \n",
    "    d_input_ids.append(d_encoded_dict['input_ids'])\n",
    "    d_attention_masks.append(d_encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dba6647-c6d0-4fe5-994b-661ab9b432cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abate\\AppData\\Local\\Temp\\ipykernel_2260\\3016847114.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  d_labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "d_input_ids = torch.cat(d_input_ids, dim=0)\n",
    "d_attention_masks = torch.cat(d_attention_masks, dim=0)\n",
    "d_labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ae78572-5c97-48b2-b8fd-1635b5745830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  True Statement: End of eviction moratorium means millions of Americans could lose their housing in the middle of a pandemic. | Tweet: BREAKING NEWS: Mitch McConnell accuses President Biden of pushing socialism by implementing the eviction moratorium that will stop millions of Americans from being thrown out on the street this month. RT if you think that Mitch is a heartless idiot!\n",
      "Token IDs: tensor([  101,  2995,  4861,  1024,  1000,  2062,  2304, 10834,  2024, 11113,\n",
      "        15613,  1999, 16392,  2084,  2141,  1012,  1000,  1064,  1056, 28394,\n",
      "         2102,  1024,  1030,  3387,  9737, 11589,  2239,  2017,  1005,  1040,\n",
      "         2036,  2228,  1037,  2304,  9220,  2052,  2022,  2062,  4986,  2008,\n",
      "         2062,  2304, 10834,  2024, 11113, 15613,  1999, 16392,  2084,  2304,\n",
      "        10834,  2141,  1012,  2021,  1045,  3984,  2017,  2123,  1005,  1056,\n",
      "         2031,  2051,  2000,  2470,  2216,  2477,  2043,  2115,  2154,  2003,\n",
      "        10202,  6224, 10474, 18856,  5833,  2013,  8271, 15912,  2015,  1012,\n",
      "         1016,  1013,  1016,   102,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Labels: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index = 10\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[index])\n",
    "print('Token IDs:', input_ids[index])\n",
    "print ('Labels:', labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5374dda6-4c51-4179-a733-e5889f8dd0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([111593, 410])\n"
     ]
    }
   ],
   "source": [
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f2e335c-d319-42a0-a4f8-719cc21c29bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89,274 training samples\n",
      "22,319 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "d_dataset = TensorDataset(d_input_ids, d_attention_masks, d_labels)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "d_train_dataset, d_val_dataset = random_split(d_dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f06f5391-9385-4566-9200-92eebe31108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 2\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = RandomSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.,\n",
    "        )\n",
    "\n",
    "\n",
    "# repeat for distil\n",
    "d_train_dataloader = DataLoader(\n",
    "            d_train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(d_train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "d_validation_dataloader = DataLoader(\n",
    "            d_val_dataset, # The validation samples.\n",
    "            sampler = RandomSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6bc13d4-68d2-481a-8431-21764fed64ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "\n",
    "d_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()\n",
    "d_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "402ce2a1-a2bf-43af-86e8-322076484fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08ab44ef-2a35-4b1d-a9a0-83598f033094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "d_optimizer = AdamW(d_model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f73d228-43b8-4a2e-afc4-18df28d17523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "d_total_steps = len(d_train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "d_scheduler = get_linear_schedule_with_warmup(d_optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = d_total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "debfe942-a9b0-4990-88d6-50093725dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "427c284e-eb0e-4e17-a9d0-7f990c56176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46b26116-d8a9-482c-9867-8d1fab53a7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49e6e9bf-6f44-439f-bc7a-97b017247a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = 'Dataset/checkpoints/checkpoint_with_maxlength_410'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c612aa1f-af4e-47df-88b9-97f64475d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abate\\anaconda3\\envs\\truthseeker_gpu\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of  44,637.    Elapsed: 0:00:02. Training loss. 0.6684021353721619 Num fake examples 43 Num true examples 37\n",
      "  Batch    80  of  44,637.    Elapsed: 0:00:03. Training loss. 0.13772344589233398 Num fake examples 77 Num true examples 83\n",
      "  Batch   120  of  44,637.    Elapsed: 0:00:05. Training loss. 1.2736718654632568 Num fake examples 114 Num true examples 126\n",
      "  Batch   160  of  44,637.    Elapsed: 0:00:06. Training loss. 0.03370995447039604 Num fake examples 148 Num true examples 172\n",
      "  Batch   200  of  44,637.    Elapsed: 0:00:08. Training loss. 0.016007795929908752 Num fake examples 187 Num true examples 213\n",
      "  Batch   240  of  44,637.    Elapsed: 0:00:09. Training loss. 0.015445040538907051 Num fake examples 230 Num true examples 250\n",
      "  Batch   280  of  44,637.    Elapsed: 0:00:10. Training loss. 0.009891578927636147 Num fake examples 267 Num true examples 293\n",
      "  Batch   320  of  44,637.    Elapsed: 0:00:12. Training loss. 0.009709976613521576 Num fake examples 312 Num true examples 328\n",
      "  Batch   360  of  44,637.    Elapsed: 0:00:13. Training loss. 0.009695364162325859 Num fake examples 351 Num true examples 369\n",
      "  Batch   400  of  44,637.    Elapsed: 0:00:14. Training loss. 0.007651975378394127 Num fake examples 396 Num true examples 404\n",
      "  Batch   440  of  44,637.    Elapsed: 0:00:16. Training loss. 0.005356886889785528 Num fake examples 441 Num true examples 439\n",
      "  Batch   480  of  44,637.    Elapsed: 0:00:17. Training loss. 0.005084365606307983 Num fake examples 481 Num true examples 479\n",
      "  Batch   520  of  44,637.    Elapsed: 0:00:19. Training loss. 0.004835121333599091 Num fake examples 524 Num true examples 516\n",
      "  Batch   560  of  44,637.    Elapsed: 0:00:20. Training loss. 0.005010481923818588 Num fake examples 563 Num true examples 557\n",
      "  Batch   600  of  44,637.    Elapsed: 0:00:21. Training loss. 0.0037271748296916485 Num fake examples 603 Num true examples 597\n",
      "  Batch   640  of  44,637.    Elapsed: 0:00:23. Training loss. 0.004469984211027622 Num fake examples 646 Num true examples 634\n",
      "  Batch   680  of  44,637.    Elapsed: 0:00:24. Training loss. 0.00399631354957819 Num fake examples 691 Num true examples 669\n",
      "  Batch   720  of  44,637.    Elapsed: 0:00:26. Training loss. 0.003163048066198826 Num fake examples 731 Num true examples 709\n",
      "  Batch   760  of  44,637.    Elapsed: 0:00:27. Training loss. 0.00465057697147131 Num fake examples 762 Num true examples 758\n",
      "  Batch   800  of  44,637.    Elapsed: 0:00:28. Training loss. 0.002919926308095455 Num fake examples 803 Num true examples 797\n",
      "  Batch   840  of  44,637.    Elapsed: 0:00:30. Training loss. 0.0024721852969378233 Num fake examples 836 Num true examples 844\n",
      "  Batch   880  of  44,637.    Elapsed: 0:00:31. Training loss. 0.0033192113041877747 Num fake examples 877 Num true examples 883\n",
      "  Batch   920  of  44,637.    Elapsed: 0:00:33. Training loss. 0.004740999545902014 Num fake examples 914 Num true examples 926\n",
      "  Batch   960  of  44,637.    Elapsed: 0:00:34. Training loss. 0.0027478390838950872 Num fake examples 957 Num true examples 963\n",
      "  Batch 1,000  of  44,637.    Elapsed: 0:00:35. Training loss. 0.0028037673328071833 Num fake examples 988 Num true examples 1012\n",
      "  Batch 1,040  of  44,637.    Elapsed: 0:00:37. Training loss. 0.011662755161523819 Num fake examples 1029 Num true examples 1051\n",
      "  Batch 1,080  of  44,637.    Elapsed: 0:00:38. Training loss. 0.010462500154972076 Num fake examples 1075 Num true examples 1085\n",
      "  Batch 1,120  of  44,637.    Elapsed: 0:00:40. Training loss. 0.011670190840959549 Num fake examples 1115 Num true examples 1125\n",
      "  Batch 1,160  of  44,637.    Elapsed: 0:00:41. Training loss. 0.009211634285748005 Num fake examples 1153 Num true examples 1167\n",
      "  Batch 1,200  of  44,637.    Elapsed: 0:00:42. Training loss. 0.004665641579777002 Num fake examples 1187 Num true examples 1213\n",
      "  Batch 1,240  of  44,637.    Elapsed: 0:00:44. Training loss. 0.00385544216260314 Num fake examples 1231 Num true examples 1249\n",
      "  Batch 1,280  of  44,637.    Elapsed: 0:00:45. Training loss. 0.004049645736813545 Num fake examples 1270 Num true examples 1290\n",
      "  Batch 1,320  of  44,637.    Elapsed: 0:00:47. Training loss. 0.003873698879033327 Num fake examples 1307 Num true examples 1333\n",
      "  Batch 1,360  of  44,637.    Elapsed: 0:00:48. Training loss. 0.005455192178487778 Num fake examples 1352 Num true examples 1368\n",
      "  Batch 1,400  of  44,637.    Elapsed: 0:00:50. Training loss. 0.0033429181203246117 Num fake examples 1393 Num true examples 1407\n",
      "  Batch 1,440  of  44,637.    Elapsed: 0:00:51. Training loss. 0.0030145817436277866 Num fake examples 1426 Num true examples 1454\n",
      "  Batch 1,480  of  44,637.    Elapsed: 0:00:52. Training loss. 0.0024527281057089567 Num fake examples 1470 Num true examples 1490\n",
      "  Batch 1,520  of  44,637.    Elapsed: 0:00:54. Training loss. 0.003610003273934126 Num fake examples 1518 Num true examples 1522\n",
      "  Batch 1,560  of  44,637.    Elapsed: 0:00:55. Training loss. 0.005962627939879894 Num fake examples 1563 Num true examples 1557\n",
      "  Batch 1,600  of  44,637.    Elapsed: 0:00:57. Training loss. 0.0030248272232711315 Num fake examples 1595 Num true examples 1605\n",
      "  Batch 1,640  of  44,637.    Elapsed: 0:00:58. Training loss. 0.0024322057142853737 Num fake examples 1625 Num true examples 1655\n",
      "  Batch 1,680  of  44,637.    Elapsed: 0:00:59. Training loss. 0.003817977150902152 Num fake examples 1663 Num true examples 1697\n",
      "  Batch 1,720  of  44,637.    Elapsed: 0:01:01. Training loss. 0.002729307394474745 Num fake examples 1706 Num true examples 1734\n",
      "  Batch 1,760  of  44,637.    Elapsed: 0:01:02. Training loss. 0.002973716240376234 Num fake examples 1744 Num true examples 1776\n",
      "  Batch 1,800  of  44,637.    Elapsed: 0:01:04. Training loss. 0.0023263883776962757 Num fake examples 1787 Num true examples 1813\n",
      "  Batch 1,840  of  44,637.    Elapsed: 0:01:05. Training loss. 0.002397214062511921 Num fake examples 1823 Num true examples 1857\n",
      "  Batch 1,880  of  44,637.    Elapsed: 0:01:06. Training loss. 0.00978521816432476 Num fake examples 1866 Num true examples 1894\n",
      "  Batch 1,920  of  44,637.    Elapsed: 0:01:08. Training loss. 0.0019283279543742537 Num fake examples 1908 Num true examples 1932\n",
      "  Batch 1,960  of  44,637.    Elapsed: 0:01:09. Training loss. 0.002173890359699726 Num fake examples 1950 Num true examples 1970\n",
      "  Batch 2,000  of  44,637.    Elapsed: 0:01:11. Training loss. 0.0021704118698835373 Num fake examples 1990 Num true examples 2010\n",
      "  Batch 2,040  of  44,637.    Elapsed: 0:01:12. Training loss. 0.0027436274103820324 Num fake examples 2028 Num true examples 2052\n",
      "  Batch 2,080  of  44,637.    Elapsed: 0:01:13. Training loss. 0.0022074102889746428 Num fake examples 2063 Num true examples 2097\n",
      "  Batch 2,120  of  44,637.    Elapsed: 0:01:15. Training loss. 0.0019010440446436405 Num fake examples 2105 Num true examples 2135\n",
      "  Batch 2,160  of  44,637.    Elapsed: 0:01:16. Training loss. 0.0029344549402594566 Num fake examples 2136 Num true examples 2184\n",
      "  Batch 2,200  of  44,637.    Elapsed: 0:01:18. Training loss. 0.004872097633779049 Num fake examples 2176 Num true examples 2224\n",
      "  Batch 2,240  of  44,637.    Elapsed: 0:01:19. Training loss. 0.007781414780765772 Num fake examples 2217 Num true examples 2263\n",
      "  Batch 2,280  of  44,637.    Elapsed: 0:01:21. Training loss. 0.0015691894805058837 Num fake examples 2255 Num true examples 2305\n",
      "  Batch 2,320  of  44,637.    Elapsed: 0:01:22. Training loss. 0.0026307934895157814 Num fake examples 2286 Num true examples 2354\n",
      "  Batch 2,360  of  44,637.    Elapsed: 0:01:23. Training loss. 0.0026850486174225807 Num fake examples 2329 Num true examples 2391\n",
      "  Batch 2,400  of  44,637.    Elapsed: 0:01:25. Training loss. 0.0034050748217850924 Num fake examples 2373 Num true examples 2427\n",
      "  Batch 2,440  of  44,637.    Elapsed: 0:01:26. Training loss. 0.002286361064761877 Num fake examples 2411 Num true examples 2469\n",
      "  Batch 2,480  of  44,637.    Elapsed: 0:01:28. Training loss. 0.0048683686181902885 Num fake examples 2451 Num true examples 2509\n",
      "  Batch 2,520  of  44,637.    Elapsed: 0:01:29. Training loss. 0.003464294597506523 Num fake examples 2484 Num true examples 2556\n",
      "  Batch 2,560  of  44,637.    Elapsed: 0:01:30. Training loss. 0.003354665357619524 Num fake examples 2524 Num true examples 2596\n",
      "  Batch 2,600  of  44,637.    Elapsed: 0:01:32. Training loss. 0.004746885970234871 Num fake examples 2561 Num true examples 2639\n",
      "  Batch 2,640  of  44,637.    Elapsed: 0:01:33. Training loss. 0.004473577253520489 Num fake examples 2606 Num true examples 2674\n",
      "  Batch 2,680  of  44,637.    Elapsed: 0:01:35. Training loss. 0.004034787882119417 Num fake examples 2645 Num true examples 2715\n",
      "  Batch 2,720  of  44,637.    Elapsed: 0:01:36. Training loss. 0.002633224707096815 Num fake examples 2684 Num true examples 2756\n",
      "  Batch 2,760  of  44,637.    Elapsed: 0:01:38. Training loss. 0.0036290292628109455 Num fake examples 2716 Num true examples 2804\n",
      "  Batch 2,800  of  44,637.    Elapsed: 0:01:39. Training loss. 0.005086937919259071 Num fake examples 2747 Num true examples 2853\n",
      "  Batch 2,840  of  44,637.    Elapsed: 0:01:40. Training loss. 0.003824003739282489 Num fake examples 2790 Num true examples 2890\n",
      "  Batch 2,880  of  44,637.    Elapsed: 0:01:42. Training loss. 0.0022688782773911953 Num fake examples 2829 Num true examples 2931\n",
      "  Batch 2,920  of  44,637.    Elapsed: 0:01:43. Training loss. 3.0857720375061035 Num fake examples 2872 Num true examples 2968\n",
      "  Batch 2,960  of  44,637.    Elapsed: 0:01:45. Training loss. 0.003465581452473998 Num fake examples 2913 Num true examples 3007\n",
      "  Batch 3,000  of  44,637.    Elapsed: 0:01:46. Training loss. 0.0030902628786861897 Num fake examples 2952 Num true examples 3048\n",
      "  Batch 3,040  of  44,637.    Elapsed: 0:01:48. Training loss. 0.0019420657772570848 Num fake examples 2986 Num true examples 3094\n",
      "  Batch 3,080  of  44,637.    Elapsed: 0:01:49. Training loss. 0.0014429612783715129 Num fake examples 3021 Num true examples 3139\n",
      "  Batch 3,120  of  44,637.    Elapsed: 0:01:51. Training loss. 0.001905460492707789 Num fake examples 3058 Num true examples 3182\n",
      "  Batch 3,160  of  44,637.    Elapsed: 0:01:52. Training loss. 0.004040076397359371 Num fake examples 3096 Num true examples 3224\n",
      "  Batch 3,200  of  44,637.    Elapsed: 0:01:53. Training loss. 0.0028506158851087093 Num fake examples 3133 Num true examples 3267\n",
      "  Batch 3,240  of  44,637.    Elapsed: 0:01:55. Training loss. 0.0026544793508946896 Num fake examples 3178 Num true examples 3302\n",
      "  Batch 3,280  of  44,637.    Elapsed: 0:01:56. Training loss. 0.002482824958860874 Num fake examples 3223 Num true examples 3337\n",
      "  Batch 3,320  of  44,637.    Elapsed: 0:01:58. Training loss. 0.0033372947946190834 Num fake examples 3258 Num true examples 3382\n",
      "  Batch 3,360  of  44,637.    Elapsed: 0:01:59. Training loss. 0.00803680345416069 Num fake examples 3299 Num true examples 3421\n",
      "  Batch 3,400  of  44,637.    Elapsed: 0:02:00. Training loss. 0.0033439453691244125 Num fake examples 3340 Num true examples 3460\n",
      "  Batch 3,440  of  44,637.    Elapsed: 0:02:02. Training loss. 0.0028880764730274677 Num fake examples 3385 Num true examples 3495\n",
      "  Batch 3,480  of  44,637.    Elapsed: 0:02:03. Training loss. 0.002003211760893464 Num fake examples 3418 Num true examples 3542\n",
      "  Batch 3,520  of  44,637.    Elapsed: 0:02:05. Training loss. 0.002047909889370203 Num fake examples 3463 Num true examples 3577\n",
      "  Batch 3,560  of  44,637.    Elapsed: 0:02:06. Training loss. 0.004861027933657169 Num fake examples 3496 Num true examples 3624\n",
      "  Batch 3,600  of  44,637.    Elapsed: 0:02:08. Training loss. 0.0019462768686935306 Num fake examples 3537 Num true examples 3663\n",
      "  Batch 3,640  of  44,637.    Elapsed: 0:02:09. Training loss. 0.0027860519476234913 Num fake examples 3579 Num true examples 3701\n",
      "  Batch 3,680  of  44,637.    Elapsed: 0:02:10. Training loss. 0.004034483339637518 Num fake examples 3620 Num true examples 3740\n",
      "  Batch 3,720  of  44,637.    Elapsed: 0:02:12. Training loss. 0.0034617630299180746 Num fake examples 3667 Num true examples 3773\n",
      "  Batch 3,760  of  44,637.    Elapsed: 0:02:13. Training loss. 0.002736757742241025 Num fake examples 3711 Num true examples 3809\n",
      "  Batch 3,800  of  44,637.    Elapsed: 0:02:15. Training loss. 0.0036584516055881977 Num fake examples 3747 Num true examples 3853\n",
      "  Batch 3,840  of  44,637.    Elapsed: 0:02:16. Training loss. 0.004125420469790697 Num fake examples 3790 Num true examples 3890\n",
      "  Batch 3,880  of  44,637.    Elapsed: 0:02:18. Training loss. 0.008604813367128372 Num fake examples 3830 Num true examples 3930\n",
      "  Batch 3,920  of  44,637.    Elapsed: 0:02:19. Training loss. 0.006311891600489616 Num fake examples 3863 Num true examples 3977\n",
      "  Batch 3,960  of  44,637.    Elapsed: 0:02:20. Training loss. 0.005878213327378035 Num fake examples 3903 Num true examples 4017\n",
      "  Batch 4,000  of  44,637.    Elapsed: 0:02:22. Training loss. 0.0029149195179343224 Num fake examples 3933 Num true examples 4067\n",
      "  Batch 4,040  of  44,637.    Elapsed: 0:02:23. Training loss. 0.002977791242301464 Num fake examples 3969 Num true examples 4111\n",
      "  Batch 4,080  of  44,637.    Elapsed: 0:02:25. Training loss. 0.005441736429929733 Num fake examples 4006 Num true examples 4154\n",
      "  Batch 4,120  of  44,637.    Elapsed: 0:02:26. Training loss. 0.0038348776288330555 Num fake examples 4052 Num true examples 4188\n",
      "  Batch 4,160  of  44,637.    Elapsed: 0:02:28. Training loss. 0.002595911268144846 Num fake examples 4088 Num true examples 4232\n",
      "  Batch 4,200  of  44,637.    Elapsed: 0:02:29. Training loss. 0.002757241250947118 Num fake examples 4126 Num true examples 4274\n",
      "  Batch 4,240  of  44,637.    Elapsed: 0:02:30. Training loss. 0.0037044959608465433 Num fake examples 4173 Num true examples 4307\n",
      "  Batch 4,280  of  44,637.    Elapsed: 0:02:32. Training loss. 0.0026328815147280693 Num fake examples 4210 Num true examples 4350\n",
      "  Batch 4,320  of  44,637.    Elapsed: 0:02:33. Training loss. 0.0029504504054784775 Num fake examples 4254 Num true examples 4386\n",
      "  Batch 4,360  of  44,637.    Elapsed: 0:02:35. Training loss. 0.0031373125966638327 Num fake examples 4289 Num true examples 4431\n",
      "  Batch 4,400  of  44,637.    Elapsed: 0:02:36. Training loss. 0.0032124770805239677 Num fake examples 4326 Num true examples 4474\n",
      "  Batch 4,440  of  44,637.    Elapsed: 0:02:37. Training loss. 0.002298917854204774 Num fake examples 4355 Num true examples 4525\n",
      "  Batch 4,480  of  44,637.    Elapsed: 0:02:39. Training loss. 0.0032834247685968876 Num fake examples 4391 Num true examples 4569\n",
      "  Batch 4,520  of  44,637.    Elapsed: 0:02:40. Training loss. 0.004135732538998127 Num fake examples 4431 Num true examples 4609\n",
      "  Batch 4,560  of  44,637.    Elapsed: 0:02:42. Training loss. 0.002262959722429514 Num fake examples 4470 Num true examples 4650\n",
      "  Batch 4,600  of  44,637.    Elapsed: 0:02:43. Training loss. 0.00327532971277833 Num fake examples 4509 Num true examples 4691\n",
      "  Batch 4,640  of  44,637.    Elapsed: 0:02:45. Training loss. 0.0024241358041763306 Num fake examples 4550 Num true examples 4730\n",
      "  Batch 4,680  of  44,637.    Elapsed: 0:02:46. Training loss. 0.002318853512406349 Num fake examples 4590 Num true examples 4770\n",
      "  Batch 4,720  of  44,637.    Elapsed: 0:02:47. Training loss. 0.011530818417668343 Num fake examples 4633 Num true examples 4807\n",
      "  Batch 4,760  of  44,637.    Elapsed: 0:02:49. Training loss. 0.004650152288377285 Num fake examples 4671 Num true examples 4849\n",
      "  Batch 4,800  of  44,637.    Elapsed: 0:02:50. Training loss. 0.0030326268170028925 Num fake examples 4703 Num true examples 4897\n",
      "  Batch 4,840  of  44,637.    Elapsed: 0:02:52. Training loss. 0.004029611125588417 Num fake examples 4737 Num true examples 4943\n",
      "  Batch 4,880  of  44,637.    Elapsed: 0:02:53. Training loss. 0.003620513714849949 Num fake examples 4776 Num true examples 4984\n",
      "  Batch 4,920  of  44,637.    Elapsed: 0:02:55. Training loss. 0.005219714716076851 Num fake examples 4816 Num true examples 5024\n",
      "  Batch 4,960  of  44,637.    Elapsed: 0:02:56. Training loss. 0.0023044757544994354 Num fake examples 4854 Num true examples 5066\n",
      "  Batch 5,000  of  44,637.    Elapsed: 0:02:57. Training loss. 0.0024446803145110607 Num fake examples 4889 Num true examples 5111\n",
      "  Batch 5,040  of  44,637.    Elapsed: 0:02:59. Training loss. 0.0021509972866624594 Num fake examples 4922 Num true examples 5158\n",
      "  Batch 5,080  of  44,637.    Elapsed: 0:03:00. Training loss. 0.0018477587727829814 Num fake examples 4966 Num true examples 5194\n",
      "  Batch 5,120  of  44,637.    Elapsed: 0:03:02. Training loss. 0.00341339991427958 Num fake examples 5003 Num true examples 5237\n",
      "  Batch 5,160  of  44,637.    Elapsed: 0:03:03. Training loss. 0.004259060136973858 Num fake examples 5046 Num true examples 5274\n",
      "  Batch 5,200  of  44,637.    Elapsed: 0:03:05. Training loss. 0.005446791648864746 Num fake examples 5091 Num true examples 5309\n",
      "  Batch 5,240  of  44,637.    Elapsed: 0:03:06. Training loss. 0.0016479247715324163 Num fake examples 5133 Num true examples 5347\n",
      "  Batch 5,280  of  44,637.    Elapsed: 0:03:07. Training loss. 0.0025704014115035534 Num fake examples 5162 Num true examples 5398\n",
      "  Batch 5,320  of  44,637.    Elapsed: 0:03:09. Training loss. 0.003453868208453059 Num fake examples 5197 Num true examples 5443\n",
      "  Batch 5,360  of  44,637.    Elapsed: 0:03:10. Training loss. 0.002932594623416662 Num fake examples 5237 Num true examples 5483\n",
      "  Batch 5,400  of  44,637.    Elapsed: 0:03:12. Training loss. 0.002483236137777567 Num fake examples 5272 Num true examples 5528\n",
      "  Batch 5,440  of  44,637.    Elapsed: 0:03:13. Training loss. 0.005175899714231491 Num fake examples 5310 Num true examples 5570\n",
      "  Batch 5,480  of  44,637.    Elapsed: 0:03:14. Training loss. 0.005707814823836088 Num fake examples 5343 Num true examples 5617\n",
      "  Batch 5,520  of  44,637.    Elapsed: 0:03:16. Training loss. 0.0041993726044893265 Num fake examples 5387 Num true examples 5653\n",
      "  Batch 5,560  of  44,637.    Elapsed: 0:03:17. Training loss. 0.00494993943721056 Num fake examples 5418 Num true examples 5702\n",
      "  Batch 5,600  of  44,637.    Elapsed: 0:03:19. Training loss. 0.002484368160367012 Num fake examples 5458 Num true examples 5742\n",
      "  Batch 5,640  of  44,637.    Elapsed: 0:03:20. Training loss. 0.0026705048512667418 Num fake examples 5500 Num true examples 5780\n",
      "  Batch 5,680  of  44,637.    Elapsed: 0:03:22. Training loss. 0.005102619528770447 Num fake examples 5540 Num true examples 5820\n",
      "  Batch 5,720  of  44,637.    Elapsed: 0:03:23. Training loss. 0.0039184316992759705 Num fake examples 5582 Num true examples 5858\n",
      "  Batch 5,760  of  44,637.    Elapsed: 0:03:24. Training loss. 0.002457470865920186 Num fake examples 5627 Num true examples 5893\n",
      "  Batch 5,800  of  44,637.    Elapsed: 0:03:26. Training loss. 0.003091110149398446 Num fake examples 5671 Num true examples 5929\n",
      "  Batch 5,840  of  44,637.    Elapsed: 0:03:27. Training loss. 0.0037162788212299347 Num fake examples 5707 Num true examples 5973\n",
      "  Batch 5,880  of  44,637.    Elapsed: 0:03:29. Training loss. 0.0030335811898112297 Num fake examples 5749 Num true examples 6011\n",
      "  Batch 5,920  of  44,637.    Elapsed: 0:03:30. Training loss. 0.0027589919045567513 Num fake examples 5784 Num true examples 6056\n",
      "  Batch 5,960  of  44,637.    Elapsed: 0:03:31. Training loss. 0.003967119846493006 Num fake examples 5821 Num true examples 6099\n",
      "  Batch 6,000  of  44,637.    Elapsed: 0:03:33. Training loss. 0.0029765237122774124 Num fake examples 5864 Num true examples 6136\n",
      "  Batch 6,040  of  44,637.    Elapsed: 0:03:34. Training loss. 0.004897786304354668 Num fake examples 5903 Num true examples 6177\n",
      "  Batch 6,080  of  44,637.    Elapsed: 0:03:36. Training loss. 0.004074052907526493 Num fake examples 5934 Num true examples 6226\n",
      "  Batch 6,120  of  44,637.    Elapsed: 0:03:37. Training loss. 0.0028627589344978333 Num fake examples 5978 Num true examples 6262\n",
      "  Batch 6,160  of  44,637.    Elapsed: 0:03:39. Training loss. 0.007748332805931568 Num fake examples 6013 Num true examples 6307\n",
      "  Batch 6,200  of  44,637.    Elapsed: 0:03:40. Training loss. 0.002353792078793049 Num fake examples 6047 Num true examples 6353\n",
      "  Batch 6,240  of  44,637.    Elapsed: 0:03:41. Training loss. 0.0018715430051088333 Num fake examples 6088 Num true examples 6392\n",
      "  Batch 6,280  of  44,637.    Elapsed: 0:03:43. Training loss. 0.0015110073145478964 Num fake examples 6126 Num true examples 6434\n",
      "  Batch 6,320  of  44,637.    Elapsed: 0:03:44. Training loss. 0.0033275443129241467 Num fake examples 6159 Num true examples 6481\n",
      "  Batch 6,360  of  44,637.    Elapsed: 0:03:46. Training loss. 0.004233930259943008 Num fake examples 6198 Num true examples 6522\n",
      "  Batch 6,400  of  44,637.    Elapsed: 0:03:47. Training loss. 0.0026824907399713993 Num fake examples 6235 Num true examples 6565\n",
      "  Batch 6,440  of  44,637.    Elapsed: 0:03:49. Training loss. 0.0020893241744488478 Num fake examples 6282 Num true examples 6598\n",
      "  Batch 6,480  of  44,637.    Elapsed: 0:03:50. Training loss. 0.0016452824929729104 Num fake examples 6318 Num true examples 6642\n",
      "  Batch 6,520  of  44,637.    Elapsed: 0:03:51. Training loss. 0.001430537668056786 Num fake examples 6352 Num true examples 6688\n",
      "  Batch 6,560  of  44,637.    Elapsed: 0:03:53. Training loss. 0.0022663394920527935 Num fake examples 6392 Num true examples 6728\n",
      "  Batch 6,600  of  44,637.    Elapsed: 0:03:54. Training loss. 0.001037458423525095 Num fake examples 6430 Num true examples 6770\n",
      "  Batch 6,640  of  44,637.    Elapsed: 0:03:56. Training loss. 0.003391471691429615 Num fake examples 6463 Num true examples 6817\n",
      "  Batch 6,680  of  44,637.    Elapsed: 0:03:57. Training loss. 0.0012313358020037413 Num fake examples 6507 Num true examples 6853\n",
      "  Batch 6,720  of  44,637.    Elapsed: 0:03:58. Training loss. 0.0012827882310375571 Num fake examples 6553 Num true examples 6887\n",
      "  Batch 6,760  of  44,637.    Elapsed: 0:04:00. Training loss. 0.0017656587297096848 Num fake examples 6587 Num true examples 6933\n",
      "  Batch 6,800  of  44,637.    Elapsed: 0:04:01. Training loss. 0.0020610769279301167 Num fake examples 6626 Num true examples 6974\n",
      "  Batch 6,840  of  44,637.    Elapsed: 0:04:03. Training loss. 0.0017381804063916206 Num fake examples 6656 Num true examples 7024\n",
      "  Batch 6,880  of  44,637.    Elapsed: 0:04:04. Training loss. 0.003193112090229988 Num fake examples 6697 Num true examples 7063\n",
      "  Batch 6,920  of  44,637.    Elapsed: 0:04:06. Training loss. 0.0037997409235686064 Num fake examples 6736 Num true examples 7104\n",
      "  Batch 6,960  of  44,637.    Elapsed: 0:04:07. Training loss. 0.0033049732446670532 Num fake examples 6774 Num true examples 7146\n",
      "  Batch 7,000  of  44,637.    Elapsed: 0:04:08. Training loss. 0.0026122531853616238 Num fake examples 6811 Num true examples 7189\n",
      "  Batch 7,040  of  44,637.    Elapsed: 0:04:10. Training loss. 0.002681265352293849 Num fake examples 6853 Num true examples 7227\n",
      "  Batch 7,080  of  44,637.    Elapsed: 0:04:11. Training loss. 0.0021462207660079002 Num fake examples 6888 Num true examples 7272\n",
      "  Batch 7,120  of  44,637.    Elapsed: 0:04:13. Training loss. 0.003901554737240076 Num fake examples 6925 Num true examples 7315\n",
      "  Batch 7,160  of  44,637.    Elapsed: 0:04:14. Training loss. 0.006497212685644627 Num fake examples 6968 Num true examples 7352\n",
      "  Batch 7,200  of  44,637.    Elapsed: 0:04:16. Training loss. 0.003455418162047863 Num fake examples 7013 Num true examples 7387\n",
      "  Batch 7,240  of  44,637.    Elapsed: 0:04:17. Training loss. 0.0037651867605745792 Num fake examples 7053 Num true examples 7427\n",
      "  Batch 7,280  of  44,637.    Elapsed: 0:04:18. Training loss. 0.0026495102792978287 Num fake examples 7089 Num true examples 7471\n",
      "  Batch 7,320  of  44,637.    Elapsed: 0:04:20. Training loss. 0.0037076929584145546 Num fake examples 7130 Num true examples 7510\n",
      "  Batch 7,360  of  44,637.    Elapsed: 0:04:21. Training loss. 0.0038783885538578033 Num fake examples 7171 Num true examples 7549\n",
      "  Batch 7,400  of  44,637.    Elapsed: 0:04:23. Training loss. 0.00531515758484602 Num fake examples 7209 Num true examples 7591\n",
      "  Batch 7,440  of  44,637.    Elapsed: 0:04:24. Training loss. 2.9831063747406006 Num fake examples 7243 Num true examples 7637\n",
      "  Batch 7,480  of  44,637.    Elapsed: 0:04:25. Training loss. 0.004830877762287855 Num fake examples 7287 Num true examples 7673\n",
      "  Batch 7,520  of  44,637.    Elapsed: 0:04:27. Training loss. 0.0034762274008244276 Num fake examples 7321 Num true examples 7719\n",
      "  Batch 7,560  of  44,637.    Elapsed: 0:04:28. Training loss. 0.0074176848866045475 Num fake examples 7356 Num true examples 7764\n",
      "  Batch 7,600  of  44,637.    Elapsed: 0:04:30. Training loss. 0.0051699914038181305 Num fake examples 7403 Num true examples 7797\n",
      "  Batch 7,640  of  44,637.    Elapsed: 0:04:31. Training loss. 0.004981999285519123 Num fake examples 7440 Num true examples 7840\n",
      "  Batch 7,680  of  44,637.    Elapsed: 0:04:33. Training loss. 0.0034795254468917847 Num fake examples 7481 Num true examples 7879\n",
      "  Batch 7,720  of  44,637.    Elapsed: 0:04:34. Training loss. 0.0038078906945884228 Num fake examples 7515 Num true examples 7925\n",
      "  Batch 7,760  of  44,637.    Elapsed: 0:04:35. Training loss. 0.004218738060444593 Num fake examples 7557 Num true examples 7963\n",
      "  Batch 7,800  of  44,637.    Elapsed: 0:04:37. Training loss. 0.00554489903151989 Num fake examples 7603 Num true examples 7997\n",
      "  Batch 7,840  of  44,637.    Elapsed: 0:04:38. Training loss. 0.006345182657241821 Num fake examples 7650 Num true examples 8030\n",
      "  Batch 7,880  of  44,637.    Elapsed: 0:04:40. Training loss. 0.007723676040768623 Num fake examples 7691 Num true examples 8069\n",
      "  Batch 7,920  of  44,637.    Elapsed: 0:04:41. Training loss. 0.006485295481979847 Num fake examples 7733 Num true examples 8107\n",
      "  Batch 7,960  of  44,637.    Elapsed: 0:04:43. Training loss. 0.004835667088627815 Num fake examples 7772 Num true examples 8148\n",
      "  Batch 8,000  of  44,637.    Elapsed: 0:04:44. Training loss. 0.005046249367296696 Num fake examples 7811 Num true examples 8189\n",
      "  Batch 8,040  of  44,637.    Elapsed: 0:04:45. Training loss. 0.004325458779931068 Num fake examples 7853 Num true examples 8227\n",
      "  Batch 8,080  of  44,637.    Elapsed: 0:04:47. Training loss. 0.007384299300611019 Num fake examples 7889 Num true examples 8271\n",
      "  Batch 8,120  of  44,637.    Elapsed: 0:04:48. Training loss. 0.00509135564789176 Num fake examples 7925 Num true examples 8315\n",
      "  Batch 8,160  of  44,637.    Elapsed: 0:04:50. Training loss. 0.00505219167098403 Num fake examples 7960 Num true examples 8360\n",
      "  Batch 8,200  of  44,637.    Elapsed: 0:04:51. Training loss. 0.0026576670352369547 Num fake examples 8001 Num true examples 8399\n",
      "  Batch 8,240  of  44,637.    Elapsed: 0:04:53. Training loss. 0.004381071776151657 Num fake examples 8036 Num true examples 8444\n",
      "  Batch 8,280  of  44,637.    Elapsed: 0:04:54. Training loss. 0.00581962987780571 Num fake examples 8073 Num true examples 8487\n",
      "  Batch 8,320  of  44,637.    Elapsed: 0:04:56. Training loss. 2.517220973968506 Num fake examples 8103 Num true examples 8537\n",
      "  Batch 8,360  of  44,637.    Elapsed: 0:04:57. Training loss. 0.00651945173740387 Num fake examples 8139 Num true examples 8581\n",
      "  Batch 8,400  of  44,637.    Elapsed: 0:04:59. Training loss. 0.005155367776751518 Num fake examples 8177 Num true examples 8623\n",
      "  Batch 8,440  of  44,637.    Elapsed: 0:05:00. Training loss. 0.0036749052815139294 Num fake examples 8211 Num true examples 8669\n",
      "  Batch 8,480  of  44,637.    Elapsed: 0:05:01. Training loss. 0.006133008748292923 Num fake examples 8248 Num true examples 8712\n",
      "  Batch 8,520  of  44,637.    Elapsed: 0:05:03. Training loss. 0.004486266523599625 Num fake examples 8286 Num true examples 8754\n",
      "  Batch 8,560  of  44,637.    Elapsed: 0:05:04. Training loss. 0.003093617968261242 Num fake examples 8328 Num true examples 8792\n",
      "  Batch 8,600  of  44,637.    Elapsed: 0:05:06. Training loss. 0.003633462358266115 Num fake examples 8364 Num true examples 8836\n",
      "  Batch 8,640  of  44,637.    Elapsed: 0:05:07. Training loss. 0.002572626806795597 Num fake examples 8394 Num true examples 8886\n",
      "  Batch 8,680  of  44,637.    Elapsed: 0:05:09. Training loss. 0.0023685633204877377 Num fake examples 8438 Num true examples 8922\n",
      "  Batch 8,720  of  44,637.    Elapsed: 0:05:10. Training loss. 0.0019672459457069635 Num fake examples 8478 Num true examples 8962\n",
      "  Batch 8,760  of  44,637.    Elapsed: 0:05:12. Training loss. 0.003479473292827606 Num fake examples 8518 Num true examples 9002\n",
      "  Batch 8,800  of  44,637.    Elapsed: 0:05:13. Training loss. 0.002288505434989929 Num fake examples 8562 Num true examples 9038\n",
      "  Batch 8,840  of  44,637.    Elapsed: 0:05:14. Training loss. 0.0038434406742453575 Num fake examples 8593 Num true examples 9087\n",
      "  Batch 8,880  of  44,637.    Elapsed: 0:05:16. Training loss. 0.004216780886054039 Num fake examples 8639 Num true examples 9121\n",
      "  Batch 8,920  of  44,637.    Elapsed: 0:05:17. Training loss. 0.003017472568899393 Num fake examples 8679 Num true examples 9161\n",
      "  Batch 8,960  of  44,637.    Elapsed: 0:05:19. Training loss. 0.003273290116339922 Num fake examples 8717 Num true examples 9203\n",
      "  Batch 9,000  of  44,637.    Elapsed: 0:05:20. Training loss. 0.0029343038331717253 Num fake examples 8761 Num true examples 9239\n",
      "  Batch 9,040  of  44,637.    Elapsed: 0:05:22. Training loss. 0.002442339202389121 Num fake examples 8797 Num true examples 9283\n",
      "  Batch 9,080  of  44,637.    Elapsed: 0:05:23. Training loss. 0.004316004924476147 Num fake examples 8841 Num true examples 9319\n",
      "  Batch 9,120  of  44,637.    Elapsed: 0:05:24. Training loss. 0.004129731561988592 Num fake examples 8885 Num true examples 9355\n",
      "  Batch 9,160  of  44,637.    Elapsed: 0:05:26. Training loss. 0.0026331041008234024 Num fake examples 8924 Num true examples 9396\n",
      "  Batch 9,200  of  44,637.    Elapsed: 0:05:27. Training loss. 0.0034734890796244144 Num fake examples 8962 Num true examples 9438\n",
      "  Batch 9,240  of  44,637.    Elapsed: 0:05:29. Training loss. 0.003595564514398575 Num fake examples 9009 Num true examples 9471\n",
      "  Batch 9,280  of  44,637.    Elapsed: 0:05:30. Training loss. 0.004255090840160847 Num fake examples 9041 Num true examples 9519\n",
      "  Batch 9,320  of  44,637.    Elapsed: 0:05:31. Training loss. 0.003225041087716818 Num fake examples 9078 Num true examples 9562\n",
      "  Batch 9,360  of  44,637.    Elapsed: 0:05:33. Training loss. 0.0028076530434191227 Num fake examples 9122 Num true examples 9598\n",
      "  Batch 9,400  of  44,637.    Elapsed: 0:05:34. Training loss. 0.00421709893271327 Num fake examples 9160 Num true examples 9640\n",
      "  Batch 9,440  of  44,637.    Elapsed: 0:05:36. Training loss. 0.003277204465121031 Num fake examples 9199 Num true examples 9681\n",
      "  Batch 9,480  of  44,637.    Elapsed: 0:05:37. Training loss. 0.0034568479750305414 Num fake examples 9245 Num true examples 9715\n",
      "  Batch 9,520  of  44,637.    Elapsed: 0:05:39. Training loss. 0.004866994917392731 Num fake examples 9279 Num true examples 9761\n",
      "  Batch 9,560  of  44,637.    Elapsed: 0:05:40. Training loss. 0.0036991056986153126 Num fake examples 9318 Num true examples 9802\n",
      "  Batch 9,600  of  44,637.    Elapsed: 0:05:41. Training loss. 0.004894019570201635 Num fake examples 9358 Num true examples 9842\n",
      "  Batch 9,640  of  44,637.    Elapsed: 0:05:43. Training loss. 0.00449596019461751 Num fake examples 9399 Num true examples 9881\n",
      "  Batch 9,680  of  44,637.    Elapsed: 0:05:44. Training loss. 0.004145351238548756 Num fake examples 9433 Num true examples 9927\n",
      "  Batch 9,720  of  44,637.    Elapsed: 0:05:46. Training loss. 0.003384110052138567 Num fake examples 9468 Num true examples 9972\n",
      "  Batch 9,760  of  44,637.    Elapsed: 0:05:47. Training loss. 0.0028340723365545273 Num fake examples 9507 Num true examples 10013\n",
      "  Batch 9,800  of  44,637.    Elapsed: 0:05:48. Training loss. 0.0026589585468173027 Num fake examples 9553 Num true examples 10047\n",
      "  Batch 9,840  of  44,637.    Elapsed: 0:05:50. Training loss. 0.002773756394162774 Num fake examples 9589 Num true examples 10091\n",
      "  Batch 9,880  of  44,637.    Elapsed: 0:05:51. Training loss. 0.0025347622577100992 Num fake examples 9628 Num true examples 10132\n",
      "  Batch 9,920  of  44,637.    Elapsed: 0:05:53. Training loss. 0.0031828507781028748 Num fake examples 9670 Num true examples 10170\n",
      "  Batch 9,960  of  44,637.    Elapsed: 0:05:54. Training loss. 0.0035588634200394154 Num fake examples 9714 Num true examples 10206\n",
      "  Batch 10,000  of  44,637.    Elapsed: 0:05:56. Training loss. 0.0018292368622496724 Num fake examples 9757 Num true examples 10243\n",
      "  Batch 10,040  of  44,637.    Elapsed: 0:05:57. Training loss. 0.0038033355958759785 Num fake examples 9793 Num true examples 10287\n",
      "  Batch 10,080  of  44,637.    Elapsed: 0:05:58. Training loss. 0.0026291238609701395 Num fake examples 9831 Num true examples 10329\n",
      "  Batch 10,120  of  44,637.    Elapsed: 0:06:00. Training loss. 0.002722939709201455 Num fake examples 9873 Num true examples 10367\n",
      "  Batch 10,160  of  44,637.    Elapsed: 0:06:01. Training loss. 0.003795247059315443 Num fake examples 9912 Num true examples 10408\n",
      "  Batch 10,200  of  44,637.    Elapsed: 0:06:03. Training loss. 0.0029693937394768 Num fake examples 9950 Num true examples 10450\n",
      "  Batch 10,240  of  44,637.    Elapsed: 0:06:04. Training loss. 0.0034592461306601763 Num fake examples 9997 Num true examples 10483\n",
      "  Batch 10,280  of  44,637.    Elapsed: 0:06:06. Training loss. 0.005532091949135065 Num fake examples 10041 Num true examples 10519\n",
      "  Batch 10,320  of  44,637.    Elapsed: 0:06:07. Training loss. 0.004943522624671459 Num fake examples 10083 Num true examples 10557\n",
      "  Batch 10,360  of  44,637.    Elapsed: 0:06:08. Training loss. 0.0026703684125095606 Num fake examples 10127 Num true examples 10593\n",
      "  Batch 10,400  of  44,637.    Elapsed: 0:06:10. Training loss. 0.0022386135533452034 Num fake examples 10177 Num true examples 10623\n",
      "  Batch 10,440  of  44,637.    Elapsed: 0:06:11. Training loss. 3.3532238006591797 Num fake examples 10220 Num true examples 10660\n",
      "  Batch 10,480  of  44,637.    Elapsed: 0:06:13. Training loss. 0.00847055297344923 Num fake examples 10262 Num true examples 10698\n",
      "  Batch 10,520  of  44,637.    Elapsed: 0:06:14. Training loss. 0.0032186268363147974 Num fake examples 10299 Num true examples 10741\n",
      "  Batch 10,560  of  44,637.    Elapsed: 0:06:16. Training loss. 0.0034307099413126707 Num fake examples 10338 Num true examples 10782\n",
      "  Batch 10,600  of  44,637.    Elapsed: 0:06:17. Training loss. 2.730229377746582 Num fake examples 10373 Num true examples 10827\n",
      "  Batch 10,640  of  44,637.    Elapsed: 0:06:18. Training loss. 0.004313628654927015 Num fake examples 10410 Num true examples 10870\n",
      "  Batch 10,680  of  44,637.    Elapsed: 0:06:20. Training loss. 0.004200170282274485 Num fake examples 10444 Num true examples 10916\n",
      "  Batch 10,720  of  44,637.    Elapsed: 0:06:21. Training loss. 0.006553051993250847 Num fake examples 10480 Num true examples 10960\n",
      "  Batch 10,760  of  44,637.    Elapsed: 0:06:23. Training loss. 0.004145056940615177 Num fake examples 10520 Num true examples 11000\n",
      "  Batch 10,800  of  44,637.    Elapsed: 0:06:24. Training loss. 0.0029982428532093763 Num fake examples 10558 Num true examples 11042\n",
      "  Batch 10,840  of  44,637.    Elapsed: 0:06:25. Training loss. 0.007406203076243401 Num fake examples 10590 Num true examples 11090\n",
      "  Batch 10,880  of  44,637.    Elapsed: 0:06:27. Training loss. 0.006979228928685188 Num fake examples 10621 Num true examples 11139\n",
      "  Batch 10,920  of  44,637.    Elapsed: 0:06:28. Training loss. 0.002578608226031065 Num fake examples 10669 Num true examples 11171\n",
      "  Batch 10,960  of  44,637.    Elapsed: 0:06:30. Training loss. 0.0020923144184052944 Num fake examples 10705 Num true examples 11215\n",
      "  Batch 11,000  of  44,637.    Elapsed: 0:06:31. Training loss. 0.001964148133993149 Num fake examples 10745 Num true examples 11255\n",
      "  Batch 11,040  of  44,637.    Elapsed: 0:06:33. Training loss. 0.002095836214721203 Num fake examples 10785 Num true examples 11295\n",
      "  Batch 11,080  of  44,637.    Elapsed: 0:06:34. Training loss. 0.0032047450076788664 Num fake examples 10823 Num true examples 11337\n",
      "  Batch 11,120  of  44,637.    Elapsed: 0:06:35. Training loss. 0.002054003532975912 Num fake examples 10864 Num true examples 11376\n",
      "  Batch 11,160  of  44,637.    Elapsed: 0:06:37. Training loss. 0.002444998361170292 Num fake examples 10903 Num true examples 11417\n",
      "  Batch 11,200  of  44,637.    Elapsed: 0:06:38. Training loss. 0.0023511622566729784 Num fake examples 10945 Num true examples 11455\n",
      "  Batch 11,240  of  44,637.    Elapsed: 0:06:40. Training loss. 0.0012829945189878345 Num fake examples 10988 Num true examples 11492\n",
      "  Batch 11,280  of  44,637.    Elapsed: 0:06:41. Training loss. 0.0014308409299701452 Num fake examples 11026 Num true examples 11534\n",
      "  Batch 11,320  of  44,637.    Elapsed: 0:06:43. Training loss. 3.2820634841918945 Num fake examples 11061 Num true examples 11579\n",
      "  Batch 11,360  of  44,637.    Elapsed: 0:06:44. Training loss. 0.002526072319597006 Num fake examples 11104 Num true examples 11616\n",
      "  Batch 11,400  of  44,637.    Elapsed: 0:06:45. Training loss. 0.004041299223899841 Num fake examples 11144 Num true examples 11656\n",
      "  Batch 11,440  of  44,637.    Elapsed: 0:06:47. Training loss. 0.0038861960638314486 Num fake examples 11178 Num true examples 11702\n",
      "  Batch 11,480  of  44,637.    Elapsed: 0:06:48. Training loss. 0.006260995753109455 Num fake examples 11218 Num true examples 11742\n",
      "  Batch 11,520  of  44,637.    Elapsed: 0:06:50. Training loss. 0.004970472771674395 Num fake examples 11254 Num true examples 11786\n",
      "  Batch 11,560  of  44,637.    Elapsed: 0:06:51. Training loss. 0.004650230053812265 Num fake examples 11291 Num true examples 11829\n",
      "  Batch 11,600  of  44,637.    Elapsed: 0:06:53. Training loss. 0.004207749851047993 Num fake examples 11336 Num true examples 11864\n",
      "  Batch 11,640  of  44,637.    Elapsed: 0:06:54. Training loss. 0.0033238432370126247 Num fake examples 11378 Num true examples 11902\n",
      "  Batch 11,680  of  44,637.    Elapsed: 0:06:55. Training loss. 0.0022493693977594376 Num fake examples 11423 Num true examples 11937\n",
      "  Batch 11,720  of  44,637.    Elapsed: 0:06:57. Training loss. 0.002997797215357423 Num fake examples 11460 Num true examples 11980\n",
      "  Batch 11,760  of  44,637.    Elapsed: 0:06:58. Training loss. 0.0028795222751796246 Num fake examples 11496 Num true examples 12024\n",
      "  Batch 11,800  of  44,637.    Elapsed: 0:07:00. Training loss. 0.0036244341172277927 Num fake examples 11531 Num true examples 12069\n",
      "  Batch 11,840  of  44,637.    Elapsed: 0:07:01. Training loss. 0.0073960633017122746 Num fake examples 11573 Num true examples 12107\n",
      "  Batch 11,880  of  44,637.    Elapsed: 0:07:02. Training loss. 0.004084033891558647 Num fake examples 11614 Num true examples 12146\n",
      "  Batch 11,920  of  44,637.    Elapsed: 0:07:04. Training loss. 0.003762616543099284 Num fake examples 11655 Num true examples 12185\n",
      "  Batch 11,960  of  44,637.    Elapsed: 0:07:05. Training loss. 0.0057692090049386024 Num fake examples 11698 Num true examples 12222\n",
      "  Batch 12,000  of  44,637.    Elapsed: 0:07:07. Training loss. 0.004259953740984201 Num fake examples 11734 Num true examples 12266\n",
      "  Batch 12,040  of  44,637.    Elapsed: 0:07:08. Training loss. 0.0025128063280135393 Num fake examples 11767 Num true examples 12313\n",
      "  Batch 12,080  of  44,637.    Elapsed: 0:07:10. Training loss. 0.004279772751033306 Num fake examples 11815 Num true examples 12345\n",
      "  Batch 12,120  of  44,637.    Elapsed: 0:07:11. Training loss. 0.0030185594223439693 Num fake examples 11852 Num true examples 12388\n",
      "  Batch 12,160  of  44,637.    Elapsed: 0:07:12. Training loss. 3.439478874206543 Num fake examples 11895 Num true examples 12425\n",
      "  Batch 12,200  of  44,637.    Elapsed: 0:07:14. Training loss. 0.0029577012173831463 Num fake examples 11931 Num true examples 12469\n",
      "  Batch 12,240  of  44,637.    Elapsed: 0:07:15. Training loss. 0.002910762093961239 Num fake examples 11970 Num true examples 12510\n",
      "  Batch 12,280  of  44,637.    Elapsed: 0:07:17. Training loss. 0.003646497381851077 Num fake examples 12007 Num true examples 12553\n",
      "  Batch 12,320  of  44,637.    Elapsed: 0:07:18. Training loss. 0.0019033616408705711 Num fake examples 12042 Num true examples 12598\n",
      "  Batch 12,360  of  44,637.    Elapsed: 0:07:20. Training loss. 0.003080146387219429 Num fake examples 12076 Num true examples 12644\n",
      "  Batch 12,400  of  44,637.    Elapsed: 0:07:21. Training loss. 0.003366435645148158 Num fake examples 12115 Num true examples 12685\n",
      "  Batch 12,440  of  44,637.    Elapsed: 0:07:22. Training loss. 0.002650786191225052 Num fake examples 12152 Num true examples 12728\n",
      "  Batch 12,480  of  44,637.    Elapsed: 0:07:24. Training loss. 0.0025120123755186796 Num fake examples 12186 Num true examples 12774\n",
      "  Batch 12,520  of  44,637.    Elapsed: 0:07:25. Training loss. 0.004089329857379198 Num fake examples 12224 Num true examples 12816\n",
      "  Batch 12,560  of  44,637.    Elapsed: 0:07:27. Training loss. 0.008968823589384556 Num fake examples 12269 Num true examples 12851\n",
      "  Batch 12,600  of  44,637.    Elapsed: 0:07:28. Training loss. 0.004408706910908222 Num fake examples 12304 Num true examples 12896\n",
      "  Batch 12,640  of  44,637.    Elapsed: 0:07:30. Training loss. 0.0033916120883077383 Num fake examples 12338 Num true examples 12942\n",
      "  Batch 12,680  of  44,637.    Elapsed: 0:07:31. Training loss. 0.004521026276051998 Num fake examples 12381 Num true examples 12979\n",
      "  Batch 12,720  of  44,637.    Elapsed: 0:07:32. Training loss. 0.0044855461455881596 Num fake examples 12420 Num true examples 13020\n",
      "  Batch 12,760  of  44,637.    Elapsed: 0:07:34. Training loss. 0.00503808306530118 Num fake examples 12452 Num true examples 13068\n",
      "  Batch 12,800  of  44,637.    Elapsed: 0:07:35. Training loss. 0.003595151472836733 Num fake examples 12482 Num true examples 13118\n",
      "  Batch 12,840  of  44,637.    Elapsed: 0:07:37. Training loss. 0.0017733965069055557 Num fake examples 12512 Num true examples 13168\n",
      "  Batch 12,880  of  44,637.    Elapsed: 0:07:38. Training loss. 0.0018359787063673139 Num fake examples 12550 Num true examples 13210\n",
      "  Batch 12,920  of  44,637.    Elapsed: 0:07:39. Training loss. 0.003070556093007326 Num fake examples 12587 Num true examples 13253\n",
      "  Batch 12,960  of  44,637.    Elapsed: 0:07:41. Training loss. 0.002156301401555538 Num fake examples 12630 Num true examples 13290\n",
      "  Batch 13,000  of  44,637.    Elapsed: 0:07:42. Training loss. 2.9674103260040283 Num fake examples 12666 Num true examples 13334\n",
      "  Batch 13,040  of  44,637.    Elapsed: 0:07:44. Training loss. 0.0018534369301050901 Num fake examples 12705 Num true examples 13375\n",
      "  Batch 13,080  of  44,637.    Elapsed: 0:07:45. Training loss. 0.0025416547432541847 Num fake examples 12742 Num true examples 13418\n",
      "  Batch 13,120  of  44,637.    Elapsed: 0:07:47. Training loss. 0.0016200942918658257 Num fake examples 12786 Num true examples 13454\n",
      "  Batch 13,160  of  44,637.    Elapsed: 0:07:48. Training loss. 0.002575898077338934 Num fake examples 12822 Num true examples 13498\n",
      "  Batch 13,200  of  44,637.    Elapsed: 0:07:49. Training loss. 3.0134572982788086 Num fake examples 12863 Num true examples 13537\n",
      "  Batch 13,240  of  44,637.    Elapsed: 0:07:51. Training loss. 0.0015240473439916968 Num fake examples 12899 Num true examples 13581\n",
      "  Batch 13,280  of  44,637.    Elapsed: 0:07:52. Training loss. 0.00843566469848156 Num fake examples 12935 Num true examples 13625\n",
      "  Batch 13,320  of  44,637.    Elapsed: 0:07:54. Training loss. 0.007641629781574011 Num fake examples 12968 Num true examples 13672\n",
      "  Batch 13,360  of  44,637.    Elapsed: 0:07:55. Training loss. 3.0764307975769043 Num fake examples 13006 Num true examples 13714\n",
      "  Batch 13,400  of  44,637.    Elapsed: 0:07:57. Training loss. 0.0023529776372015476 Num fake examples 13047 Num true examples 13753\n",
      "  Batch 13,440  of  44,637.    Elapsed: 0:07:58. Training loss. 0.0041175950318574905 Num fake examples 13078 Num true examples 13802\n",
      "  Batch 13,480  of  44,637.    Elapsed: 0:07:59. Training loss. 0.0028749490156769753 Num fake examples 13115 Num true examples 13845\n",
      "  Batch 13,520  of  44,637.    Elapsed: 0:08:01. Training loss. 0.002617236226797104 Num fake examples 13158 Num true examples 13882\n",
      "  Batch 13,560  of  44,637.    Elapsed: 0:08:02. Training loss. 0.0025875656865537167 Num fake examples 13193 Num true examples 13927\n",
      "  Batch 13,600  of  44,637.    Elapsed: 0:08:04. Training loss. 0.0026451600715517998 Num fake examples 13232 Num true examples 13968\n",
      "  Batch 13,640  of  44,637.    Elapsed: 0:08:05. Training loss. 0.002480922732502222 Num fake examples 13277 Num true examples 14003\n",
      "  Batch 13,680  of  44,637.    Elapsed: 0:08:07. Training loss. 0.0033489582128822803 Num fake examples 13316 Num true examples 14044\n",
      "  Batch 13,720  of  44,637.    Elapsed: 0:08:08. Training loss. 0.0033573561813682318 Num fake examples 13358 Num true examples 14082\n",
      "  Batch 13,760  of  44,637.    Elapsed: 0:08:09. Training loss. 0.3111426532268524 Num fake examples 13390 Num true examples 14130\n",
      "  Batch 13,800  of  44,637.    Elapsed: 0:08:11. Training loss. 0.003988516051322222 Num fake examples 13431 Num true examples 14169\n",
      "  Batch 13,840  of  44,637.    Elapsed: 0:08:12. Training loss. 0.003973721526563168 Num fake examples 13470 Num true examples 14210\n",
      "  Batch 13,880  of  44,637.    Elapsed: 0:08:14. Training loss. 0.005858099088072777 Num fake examples 13516 Num true examples 14244\n",
      "  Batch 13,920  of  44,637.    Elapsed: 0:08:15. Training loss. 0.0029337077867239714 Num fake examples 13547 Num true examples 14293\n",
      "  Batch 13,960  of  44,637.    Elapsed: 0:08:17. Training loss. 0.0034300906118005514 Num fake examples 13587 Num true examples 14333\n",
      "  Batch 14,000  of  44,637.    Elapsed: 0:08:18. Training loss. 0.0028146146796643734 Num fake examples 13626 Num true examples 14374\n",
      "  Batch 14,040  of  44,637.    Elapsed: 0:08:20. Training loss. 0.0023728974629193544 Num fake examples 13670 Num true examples 14410\n",
      "  Batch 14,080  of  44,637.    Elapsed: 0:08:21. Training loss. 0.002059296006336808 Num fake examples 13713 Num true examples 14447\n",
      "  Batch 14,120  of  44,637.    Elapsed: 0:08:23. Training loss. 0.0019942205399274826 Num fake examples 13746 Num true examples 14494\n",
      "  Batch 14,160  of  44,637.    Elapsed: 0:08:24. Training loss. 0.0013493080623447895 Num fake examples 13789 Num true examples 14531\n",
      "  Batch 14,200  of  44,637.    Elapsed: 0:08:26. Training loss. 0.0024451734498143196 Num fake examples 13821 Num true examples 14579\n",
      "  Batch 14,240  of  44,637.    Elapsed: 0:08:27. Training loss. 0.0025119641795754433 Num fake examples 13860 Num true examples 14620\n",
      "  Batch 14,280  of  44,637.    Elapsed: 0:08:29. Training loss. 0.0024128719232976437 Num fake examples 13895 Num true examples 14665\n",
      "  Batch 14,320  of  44,637.    Elapsed: 0:08:30. Training loss. 0.0037242218386381865 Num fake examples 13933 Num true examples 14707\n",
      "  Batch 14,360  of  44,637.    Elapsed: 0:08:31. Training loss. 0.004251900129020214 Num fake examples 13972 Num true examples 14748\n",
      "  Batch 14,400  of  44,637.    Elapsed: 0:08:33. Training loss. 0.002855972619727254 Num fake examples 14006 Num true examples 14794\n",
      "  Batch 14,440  of  44,637.    Elapsed: 0:08:34. Training loss. 0.00461049098521471 Num fake examples 14046 Num true examples 14834\n",
      "  Batch 14,480  of  44,637.    Elapsed: 0:08:36. Training loss. 0.002461735624819994 Num fake examples 14083 Num true examples 14877\n",
      "  Batch 14,520  of  44,637.    Elapsed: 0:08:37. Training loss. 0.003441988257691264 Num fake examples 14123 Num true examples 14917\n",
      "  Batch 14,560  of  44,637.    Elapsed: 0:08:39. Training loss. 2.878340005874634 Num fake examples 14161 Num true examples 14959\n",
      "  Batch 14,600  of  44,637.    Elapsed: 0:08:40. Training loss. 0.004761646036058664 Num fake examples 14197 Num true examples 15003\n",
      "  Batch 14,640  of  44,637.    Elapsed: 0:08:42. Training loss. 2.9700534343719482 Num fake examples 14235 Num true examples 15045\n",
      "  Batch 14,680  of  44,637.    Elapsed: 0:08:43. Training loss. 2.945342540740967 Num fake examples 14266 Num true examples 15094\n",
      "  Batch 14,720  of  44,637.    Elapsed: 0:08:45. Training loss. 0.0026256097480654716 Num fake examples 14302 Num true examples 15138\n",
      "  Batch 14,760  of  44,637.    Elapsed: 0:08:46. Training loss. 0.002066647633910179 Num fake examples 14343 Num true examples 15177\n",
      "  Batch 14,800  of  44,637.    Elapsed: 0:08:47. Training loss. 0.003872818313539028 Num fake examples 14373 Num true examples 15227\n",
      "  Batch 14,840  of  44,637.    Elapsed: 0:08:49. Training loss. 0.0026059432420879602 Num fake examples 14407 Num true examples 15273\n",
      "  Batch 14,880  of  44,637.    Elapsed: 0:08:50. Training loss. 0.0038612287025898695 Num fake examples 14445 Num true examples 15315\n",
      "  Batch 14,920  of  44,637.    Elapsed: 0:08:52. Training loss. 0.0024330320302397013 Num fake examples 14489 Num true examples 15351\n",
      "  Batch 14,960  of  44,637.    Elapsed: 0:08:53. Training loss. 0.001914245425723493 Num fake examples 14533 Num true examples 15387\n",
      "  Batch 15,000  of  44,637.    Elapsed: 0:08:55. Training loss. 0.0033034859225153923 Num fake examples 14573 Num true examples 15427\n",
      "  Batch 15,040  of  44,637.    Elapsed: 0:08:56. Training loss. 0.002564725000411272 Num fake examples 14603 Num true examples 15477\n",
      "  Batch 15,080  of  44,637.    Elapsed: 0:08:57. Training loss. 3.143212080001831 Num fake examples 14653 Num true examples 15507\n",
      "  Batch 15,120  of  44,637.    Elapsed: 0:08:59. Training loss. 0.003005429171025753 Num fake examples 14697 Num true examples 15543\n",
      "  Batch 15,160  of  44,637.    Elapsed: 0:09:00. Training loss. 0.003681268310174346 Num fake examples 14735 Num true examples 15585\n",
      "  Batch 15,200  of  44,637.    Elapsed: 0:09:02. Training loss. 0.0038284289184957743 Num fake examples 14775 Num true examples 15625\n",
      "  Batch 15,240  of  44,637.    Elapsed: 0:09:03. Training loss. 0.0018013573717325926 Num fake examples 14806 Num true examples 15674\n",
      "  Batch 15,280  of  44,637.    Elapsed: 0:09:05. Training loss. 2.8204598426818848 Num fake examples 14843 Num true examples 15717\n",
      "  Batch 15,320  of  44,637.    Elapsed: 0:09:06. Training loss. 2.9215946197509766 Num fake examples 14879 Num true examples 15761\n",
      "  Batch 15,360  of  44,637.    Elapsed: 0:09:07. Training loss. 2.843951463699341 Num fake examples 14917 Num true examples 15803\n",
      "  Batch 15,400  of  44,637.    Elapsed: 0:09:09. Training loss. 0.0052302260883152485 Num fake examples 14950 Num true examples 15850\n",
      "  Batch 15,440  of  44,637.    Elapsed: 0:09:10. Training loss. 0.005306552164256573 Num fake examples 14998 Num true examples 15882\n",
      "  Batch 15,480  of  44,637.    Elapsed: 0:09:12. Training loss. 0.00373185146600008 Num fake examples 15034 Num true examples 15926\n",
      "  Batch 15,520  of  44,637.    Elapsed: 0:09:13. Training loss. 0.0029519284144043922 Num fake examples 15078 Num true examples 15962\n",
      "  Batch 15,560  of  44,637.    Elapsed: 0:09:15. Training loss. 0.003210133872926235 Num fake examples 15116 Num true examples 16004\n",
      "  Batch 15,600  of  44,637.    Elapsed: 0:09:16. Training loss. 0.003455351572483778 Num fake examples 15153 Num true examples 16047\n",
      "  Batch 15,640  of  44,637.    Elapsed: 0:09:17. Training loss. 0.0026655651163309813 Num fake examples 15198 Num true examples 16082\n",
      "  Batch 15,680  of  44,637.    Elapsed: 0:09:19. Training loss. 0.002697620540857315 Num fake examples 15237 Num true examples 16123\n",
      "  Batch 15,720  of  44,637.    Elapsed: 0:09:20. Training loss. 0.0020476754289120436 Num fake examples 15278 Num true examples 16162\n",
      "  Batch 15,760  of  44,637.    Elapsed: 0:09:22. Training loss. 0.001952653517946601 Num fake examples 15320 Num true examples 16200\n",
      "  Batch 15,800  of  44,637.    Elapsed: 0:09:23. Training loss. 0.0026460252702236176 Num fake examples 15355 Num true examples 16245\n",
      "  Batch 15,840  of  44,637.    Elapsed: 0:09:25. Training loss. 2.920867919921875 Num fake examples 15394 Num true examples 16286\n",
      "  Batch 15,880  of  44,637.    Elapsed: 0:09:26. Training loss. 0.0061088744550943375 Num fake examples 15437 Num true examples 16323\n",
      "  Batch 15,920  of  44,637.    Elapsed: 0:09:27. Training loss. 0.005927323363721371 Num fake examples 15477 Num true examples 16363\n",
      "  Batch 15,960  of  44,637.    Elapsed: 0:09:29. Training loss. 2.2814903259277344 Num fake examples 15513 Num true examples 16407\n",
      "  Batch 16,000  of  44,637.    Elapsed: 0:09:30. Training loss. 0.006770632229745388 Num fake examples 15547 Num true examples 16453\n",
      "  Batch 16,040  of  44,637.    Elapsed: 0:09:32. Training loss. 0.003935523331165314 Num fake examples 15591 Num true examples 16489\n",
      "  Batch 16,080  of  44,637.    Elapsed: 0:09:33. Training loss. 0.003852176945656538 Num fake examples 15633 Num true examples 16527\n",
      "  Batch 16,120  of  44,637.    Elapsed: 0:09:35. Training loss. 0.003496843855828047 Num fake examples 15668 Num true examples 16572\n",
      "  Batch 16,160  of  44,637.    Elapsed: 0:09:36. Training loss. 0.002968183718621731 Num fake examples 15708 Num true examples 16612\n",
      "  Batch 16,200  of  44,637.    Elapsed: 0:09:37. Training loss. 0.0026052980683743954 Num fake examples 15744 Num true examples 16656\n",
      "  Batch 16,240  of  44,637.    Elapsed: 0:09:39. Training loss. 0.0027748323045670986 Num fake examples 15785 Num true examples 16695\n",
      "  Batch 16,280  of  44,637.    Elapsed: 0:09:40. Training loss. 0.003439127467572689 Num fake examples 15814 Num true examples 16746\n",
      "  Batch 16,320  of  44,637.    Elapsed: 0:09:42. Training loss. 0.002773454412817955 Num fake examples 15850 Num true examples 16790\n",
      "  Batch 16,360  of  44,637.    Elapsed: 0:09:43. Training loss. 0.003321825759485364 Num fake examples 15890 Num true examples 16830\n",
      "  Batch 16,400  of  44,637.    Elapsed: 0:09:45. Training loss. 0.0027393479831516743 Num fake examples 15927 Num true examples 16873\n",
      "  Batch 16,440  of  44,637.    Elapsed: 0:09:46. Training loss. 0.0025245482102036476 Num fake examples 15970 Num true examples 16910\n",
      "  Batch 16,480  of  44,637.    Elapsed: 0:09:47. Training loss. 0.002270833123475313 Num fake examples 16007 Num true examples 16953\n",
      "  Batch 16,520  of  44,637.    Elapsed: 0:09:49. Training loss. 0.001548119937069714 Num fake examples 16041 Num true examples 16999\n",
      "  Batch 16,560  of  44,637.    Elapsed: 0:09:50. Training loss. 0.0013455470325425267 Num fake examples 16075 Num true examples 17045\n",
      "  Batch 16,600  of  44,637.    Elapsed: 0:09:52. Training loss. 0.002013277728110552 Num fake examples 16113 Num true examples 17087\n",
      "  Batch 16,640  of  44,637.    Elapsed: 0:09:53. Training loss. 0.002506488934159279 Num fake examples 16157 Num true examples 17123\n",
      "  Batch 16,680  of  44,637.    Elapsed: 0:09:55. Training loss. 0.0015760429669171572 Num fake examples 16201 Num true examples 17159\n",
      "  Batch 16,720  of  44,637.    Elapsed: 0:09:56. Training loss. 0.0013626636937260628 Num fake examples 16232 Num true examples 17208\n",
      "  Batch 16,760  of  44,637.    Elapsed: 0:09:57. Training loss. 0.0028573882300406694 Num fake examples 16265 Num true examples 17255\n",
      "  Batch 16,800  of  44,637.    Elapsed: 0:09:59. Training loss. 0.005351517349481583 Num fake examples 16300 Num true examples 17300\n",
      "  Batch 16,840  of  44,637.    Elapsed: 0:10:00. Training loss. 0.004425707273185253 Num fake examples 16339 Num true examples 17341\n",
      "  Batch 16,880  of  44,637.    Elapsed: 0:10:02. Training loss. 0.0021579661406576633 Num fake examples 16378 Num true examples 17382\n",
      "  Batch 16,920  of  44,637.    Elapsed: 0:10:03. Training loss. 0.0051461635157465935 Num fake examples 16423 Num true examples 17417\n",
      "  Batch 16,960  of  44,637.    Elapsed: 0:10:05. Training loss. 0.004480731673538685 Num fake examples 16458 Num true examples 17462\n",
      "  Batch 17,000  of  44,637.    Elapsed: 0:10:06. Training loss. 0.002287212759256363 Num fake examples 16494 Num true examples 17506\n",
      "  Batch 17,040  of  44,637.    Elapsed: 0:10:07. Training loss. 0.0030036477837711573 Num fake examples 16529 Num true examples 17551\n",
      "  Batch 17,080  of  44,637.    Elapsed: 0:10:09. Training loss. 0.004101722501218319 Num fake examples 16568 Num true examples 17592\n",
      "  Batch 17,120  of  44,637.    Elapsed: 0:10:10. Training loss. 0.004115556366741657 Num fake examples 16607 Num true examples 17633\n",
      "  Batch 17,160  of  44,637.    Elapsed: 0:10:12. Training loss. 0.0026745451614260674 Num fake examples 16641 Num true examples 17679\n",
      "  Batch 17,200  of  44,637.    Elapsed: 0:10:13. Training loss. 0.00285403267480433 Num fake examples 16678 Num true examples 17722\n",
      "  Batch 17,240  of  44,637.    Elapsed: 0:10:14. Training loss. 0.0031628943979740143 Num fake examples 16721 Num true examples 17759\n",
      "  Batch 17,280  of  44,637.    Elapsed: 0:10:16. Training loss. 0.0021884122397750616 Num fake examples 16752 Num true examples 17808\n",
      "  Batch 17,320  of  44,637.    Elapsed: 0:10:17. Training loss. 0.002539019100368023 Num fake examples 16793 Num true examples 17847\n",
      "  Batch 17,360  of  44,637.    Elapsed: 0:10:19. Training loss. 0.0020448174327611923 Num fake examples 16828 Num true examples 17892\n",
      "  Batch 17,400  of  44,637.    Elapsed: 0:10:20. Training loss. 0.002277339342981577 Num fake examples 16870 Num true examples 17930\n",
      "  Batch 17,440  of  44,637.    Elapsed: 0:10:22. Training loss. 0.002012799493968487 Num fake examples 16903 Num true examples 17977\n",
      "  Batch 17,480  of  44,637.    Elapsed: 0:10:23. Training loss. 0.0032839630730450153 Num fake examples 16938 Num true examples 18022\n",
      "  Batch 17,520  of  44,637.    Elapsed: 0:10:25. Training loss. 0.0028797669801861048 Num fake examples 16976 Num true examples 18064\n",
      "  Batch 17,560  of  44,637.    Elapsed: 0:10:26. Training loss. 0.0031703149434179068 Num fake examples 17023 Num true examples 18097\n",
      "  Batch 17,600  of  44,637.    Elapsed: 0:10:27. Training loss. 0.002141316421329975 Num fake examples 17062 Num true examples 18138\n",
      "  Batch 17,640  of  44,637.    Elapsed: 0:10:29. Training loss. 2.6718451976776123 Num fake examples 17100 Num true examples 18180\n",
      "  Batch 17,680  of  44,637.    Elapsed: 0:10:30. Training loss. 0.0048318966291844845 Num fake examples 17146 Num true examples 18214\n",
      "  Batch 17,720  of  44,637.    Elapsed: 0:10:32. Training loss. 0.005079344846308231 Num fake examples 17187 Num true examples 18253\n",
      "  Batch 17,760  of  44,637.    Elapsed: 0:10:33. Training loss. 0.006813070271164179 Num fake examples 17234 Num true examples 18286\n",
      "  Batch 17,800  of  44,637.    Elapsed: 0:10:34. Training loss. 2.485003709793091 Num fake examples 17268 Num true examples 18332\n",
      "  Batch 17,840  of  44,637.    Elapsed: 0:10:36. Training loss. 0.007144549861550331 Num fake examples 17314 Num true examples 18366\n",
      "  Batch 17,880  of  44,637.    Elapsed: 0:10:37. Training loss. 0.006201130338013172 Num fake examples 17352 Num true examples 18408\n",
      "  Batch 17,920  of  44,637.    Elapsed: 0:10:39. Training loss. 0.004030989017337561 Num fake examples 17387 Num true examples 18453\n",
      "  Batch 17,960  of  44,637.    Elapsed: 0:10:40. Training loss. 0.003382110968232155 Num fake examples 17419 Num true examples 18501\n",
      "  Batch 18,000  of  44,637.    Elapsed: 0:10:42. Training loss. 0.004037704784423113 Num fake examples 17470 Num true examples 18530\n",
      "  Batch 18,040  of  44,637.    Elapsed: 0:10:43. Training loss. 0.00575052248314023 Num fake examples 17514 Num true examples 18566\n",
      "  Batch 18,080  of  44,637.    Elapsed: 0:10:44. Training loss. 0.003048239042982459 Num fake examples 17555 Num true examples 18605\n",
      "  Batch 18,120  of  44,637.    Elapsed: 0:10:46. Training loss. 0.005540688522160053 Num fake examples 17603 Num true examples 18637\n",
      "  Batch 18,160  of  44,637.    Elapsed: 0:10:47. Training loss. 0.005010048393160105 Num fake examples 17636 Num true examples 18684\n",
      "  Batch 18,200  of  44,637.    Elapsed: 0:10:49. Training loss. 0.006840533576905727 Num fake examples 17674 Num true examples 18726\n",
      "  Batch 18,240  of  44,637.    Elapsed: 0:10:50. Training loss. 0.003082278650254011 Num fake examples 17711 Num true examples 18769\n",
      "  Batch 18,280  of  44,637.    Elapsed: 0:10:52. Training loss. 0.003718132618814707 Num fake examples 17748 Num true examples 18812\n",
      "  Batch 18,320  of  44,637.    Elapsed: 0:10:53. Training loss. 3.0660288333892822 Num fake examples 17790 Num true examples 18850\n",
      "  Batch 18,360  of  44,637.    Elapsed: 0:10:54. Training loss. 0.0031022229231894016 Num fake examples 17825 Num true examples 18895\n",
      "  Batch 18,400  of  44,637.    Elapsed: 0:10:56. Training loss. 0.0025993087328970432 Num fake examples 17868 Num true examples 18932\n",
      "  Batch 18,440  of  44,637.    Elapsed: 0:10:57. Training loss. 0.0035288273356854916 Num fake examples 17908 Num true examples 18972\n",
      "  Batch 18,480  of  44,637.    Elapsed: 0:10:59. Training loss. 0.002195653272792697 Num fake examples 17953 Num true examples 19007\n",
      "  Batch 18,520  of  44,637.    Elapsed: 0:11:00. Training loss. 0.0025880481116473675 Num fake examples 17987 Num true examples 19053\n",
      "  Batch 18,560  of  44,637.    Elapsed: 0:11:02. Training loss. 0.002145840786397457 Num fake examples 18014 Num true examples 19106\n",
      "  Batch 18,600  of  44,637.    Elapsed: 0:11:03. Training loss. 0.0026840860955417156 Num fake examples 18049 Num true examples 19151\n",
      "  Batch 18,640  of  44,637.    Elapsed: 0:11:04. Training loss. 0.0026102836709469557 Num fake examples 18086 Num true examples 19194\n",
      "  Batch 18,680  of  44,637.    Elapsed: 0:11:06. Training loss. 0.00373080768622458 Num fake examples 18128 Num true examples 19232\n",
      "  Batch 18,720  of  44,637.    Elapsed: 0:11:07. Training loss. 0.002060533966869116 Num fake examples 18162 Num true examples 19278\n",
      "  Batch 18,760  of  44,637.    Elapsed: 0:11:09. Training loss. 0.006727851927280426 Num fake examples 18206 Num true examples 19314\n",
      "  Batch 18,800  of  44,637.    Elapsed: 0:11:10. Training loss. 0.004480365198105574 Num fake examples 18249 Num true examples 19351\n",
      "  Batch 18,840  of  44,637.    Elapsed: 0:11:12. Training loss. 0.0049988762475550175 Num fake examples 18281 Num true examples 19399\n",
      "  Batch 18,880  of  44,637.    Elapsed: 0:11:13. Training loss. 0.002995565999299288 Num fake examples 18321 Num true examples 19439\n",
      "  Batch 18,920  of  44,637.    Elapsed: 0:11:14. Training loss. 0.0034702285192906857 Num fake examples 18355 Num true examples 19485\n",
      "  Batch 18,960  of  44,637.    Elapsed: 0:11:16. Training loss. 0.007095922250300646 Num fake examples 18397 Num true examples 19523\n",
      "  Batch 19,000  of  44,637.    Elapsed: 0:11:17. Training loss. 0.0037779277190566063 Num fake examples 18444 Num true examples 19556\n",
      "  Batch 19,040  of  44,637.    Elapsed: 0:11:19. Training loss. 0.0046139066107571125 Num fake examples 18487 Num true examples 19593\n",
      "  Batch 19,080  of  44,637.    Elapsed: 0:11:20. Training loss. 0.0037691898178309202 Num fake examples 18529 Num true examples 19631\n",
      "  Batch 19,120  of  44,637.    Elapsed: 0:11:22. Training loss. 0.0030444329604506493 Num fake examples 18572 Num true examples 19668\n",
      "  Batch 19,160  of  44,637.    Elapsed: 0:11:23. Training loss. 0.003791920840740204 Num fake examples 18618 Num true examples 19702\n",
      "  Batch 19,200  of  44,637.    Elapsed: 0:11:24. Training loss. 0.0032986528240144253 Num fake examples 18660 Num true examples 19740\n",
      "  Batch 19,240  of  44,637.    Elapsed: 0:11:26. Training loss. 0.003544764593243599 Num fake examples 18698 Num true examples 19782\n",
      "  Batch 19,280  of  44,637.    Elapsed: 0:11:27. Training loss. 0.0036754985339939594 Num fake examples 18743 Num true examples 19817\n",
      "  Batch 19,320  of  44,637.    Elapsed: 0:11:29. Training loss. 0.005050124600529671 Num fake examples 18787 Num true examples 19853\n",
      "  Batch 19,360  of  44,637.    Elapsed: 0:11:30. Training loss. 0.004094663076102734 Num fake examples 18822 Num true examples 19898\n",
      "  Batch 19,400  of  44,637.    Elapsed: 0:11:32. Training loss. 0.005260036792606115 Num fake examples 18865 Num true examples 19935\n",
      "  Batch 19,440  of  44,637.    Elapsed: 0:11:33. Training loss. 2.7548186779022217 Num fake examples 18909 Num true examples 19971\n",
      "  Batch 19,480  of  44,637.    Elapsed: 0:11:35. Training loss. 0.003205259796231985 Num fake examples 18950 Num true examples 20010\n",
      "  Batch 19,520  of  44,637.    Elapsed: 0:11:36. Training loss. 2.843348979949951 Num fake examples 18995 Num true examples 20045\n",
      "  Batch 19,560  of  44,637.    Elapsed: 0:11:38. Training loss. 0.005432115867733955 Num fake examples 19033 Num true examples 20087\n",
      "  Batch 19,600  of  44,637.    Elapsed: 0:11:39. Training loss. 0.007770763244479895 Num fake examples 19073 Num true examples 20127\n",
      "  Batch 19,640  of  44,637.    Elapsed: 0:11:41. Training loss. 0.00414603017270565 Num fake examples 19111 Num true examples 20169\n",
      "  Batch 19,680  of  44,637.    Elapsed: 0:11:42. Training loss. 0.0044099111109972 Num fake examples 19151 Num true examples 20209\n",
      "  Batch 19,720  of  44,637.    Elapsed: 0:11:44. Training loss. 0.0035034508910030127 Num fake examples 19193 Num true examples 20247\n",
      "  Batch 19,760  of  44,637.    Elapsed: 0:11:45. Training loss. 0.0037845605984330177 Num fake examples 19231 Num true examples 20289\n",
      "  Batch 19,800  of  44,637.    Elapsed: 0:11:46. Training loss. 0.004049118608236313 Num fake examples 19267 Num true examples 20333\n",
      "  Batch 19,840  of  44,637.    Elapsed: 0:11:48. Training loss. 0.003489301772788167 Num fake examples 19302 Num true examples 20378\n",
      "  Batch 19,880  of  44,637.    Elapsed: 0:11:49. Training loss. 0.002075654687359929 Num fake examples 19331 Num true examples 20429\n",
      "  Batch 19,920  of  44,637.    Elapsed: 0:11:51. Training loss. 3.0772998332977295 Num fake examples 19366 Num true examples 20474\n",
      "  Batch 19,960  of  44,637.    Elapsed: 0:11:52. Training loss. 0.0020529767498373985 Num fake examples 19405 Num true examples 20515\n",
      "  Batch 20,000  of  44,637.    Elapsed: 0:11:54. Training loss. 0.002946216147392988 Num fake examples 19445 Num true examples 20555\n",
      "  Batch 20,040  of  44,637.    Elapsed: 0:11:55. Training loss. 0.002845961134880781 Num fake examples 19493 Num true examples 20587\n",
      "  Batch 20,080  of  44,637.    Elapsed: 0:11:56. Training loss. 0.0025073629803955555 Num fake examples 19529 Num true examples 20631\n",
      "  Batch 20,120  of  44,637.    Elapsed: 0:11:58. Training loss. 0.002811351092532277 Num fake examples 19564 Num true examples 20676\n",
      "  Batch 20,160  of  44,637.    Elapsed: 0:11:59. Training loss. 0.0037487363442778587 Num fake examples 19602 Num true examples 20718\n",
      "  Batch 20,200  of  44,637.    Elapsed: 0:12:01. Training loss. 0.004041798412799835 Num fake examples 19635 Num true examples 20765\n",
      "  Batch 20,240  of  44,637.    Elapsed: 0:12:02. Training loss. 0.0031040061730891466 Num fake examples 19679 Num true examples 20801\n",
      "  Batch 20,280  of  44,637.    Elapsed: 0:12:04. Training loss. 0.001930916914716363 Num fake examples 19719 Num true examples 20841\n",
      "  Batch 20,320  of  44,637.    Elapsed: 0:12:05. Training loss. 0.0020214628893882036 Num fake examples 19753 Num true examples 20887\n",
      "  Batch 20,360  of  44,637.    Elapsed: 0:12:06. Training loss. 0.004823770374059677 Num fake examples 19790 Num true examples 20930\n",
      "  Batch 20,400  of  44,637.    Elapsed: 0:12:08. Training loss. 0.005489306524395943 Num fake examples 19824 Num true examples 20976\n",
      "  Batch 20,440  of  44,637.    Elapsed: 0:12:09. Training loss. 0.003983840811997652 Num fake examples 19863 Num true examples 21017\n",
      "  Batch 20,480  of  44,637.    Elapsed: 0:12:11. Training loss. 0.004354877397418022 Num fake examples 19902 Num true examples 21058\n",
      "  Batch 20,520  of  44,637.    Elapsed: 0:12:12. Training loss. 0.004128497093915939 Num fake examples 19942 Num true examples 21098\n",
      "  Batch 20,560  of  44,637.    Elapsed: 0:12:14. Training loss. 0.0041255224496126175 Num fake examples 19978 Num true examples 21142\n",
      "  Batch 20,600  of  44,637.    Elapsed: 0:12:15. Training loss. 0.0033820956014096737 Num fake examples 20019 Num true examples 21181\n",
      "  Batch 20,640  of  44,637.    Elapsed: 0:12:17. Training loss. 0.003061553929001093 Num fake examples 20056 Num true examples 21224\n",
      "  Batch 20,680  of  44,637.    Elapsed: 0:12:18. Training loss. 0.0038904135581105947 Num fake examples 20095 Num true examples 21265\n",
      "  Batch 20,720  of  44,637.    Elapsed: 0:12:19. Training loss. 0.004571759607642889 Num fake examples 20130 Num true examples 21310\n",
      "  Batch 20,760  of  44,637.    Elapsed: 0:12:21. Training loss. 0.006240266840904951 Num fake examples 20166 Num true examples 21354\n",
      "  Batch 20,800  of  44,637.    Elapsed: 0:12:22. Training loss. 0.004842772148549557 Num fake examples 20208 Num true examples 21392\n",
      "  Batch 20,840  of  44,637.    Elapsed: 0:12:24. Training loss. 0.0030267410911619663 Num fake examples 20246 Num true examples 21434\n",
      "  Batch 20,880  of  44,637.    Elapsed: 0:12:25. Training loss. 0.003231491893529892 Num fake examples 20282 Num true examples 21478\n",
      "  Batch 20,920  of  44,637.    Elapsed: 0:12:27. Training loss. 0.0028059848118573427 Num fake examples 20324 Num true examples 21516\n",
      "  Batch 20,960  of  44,637.    Elapsed: 0:12:28. Training loss. 0.0021630660630762577 Num fake examples 20355 Num true examples 21565\n",
      "  Batch 21,000  of  44,637.    Elapsed: 0:12:29. Training loss. 0.0023063896223902702 Num fake examples 20399 Num true examples 21601\n",
      "  Batch 21,040  of  44,637.    Elapsed: 0:12:31. Training loss. 2.8538990020751953 Num fake examples 20446 Num true examples 21634\n",
      "  Batch 21,080  of  44,637.    Elapsed: 0:12:32. Training loss. 0.00462502334266901 Num fake examples 20490 Num true examples 21670\n",
      "  Batch 21,120  of  44,637.    Elapsed: 0:12:34. Training loss. 0.0049051446840167046 Num fake examples 20535 Num true examples 21705\n",
      "  Batch 21,160  of  44,637.    Elapsed: 0:12:35. Training loss. 0.0037852665409445763 Num fake examples 20571 Num true examples 21749\n",
      "  Batch 21,200  of  44,637.    Elapsed: 0:12:37. Training loss. 0.00449526309967041 Num fake examples 20615 Num true examples 21785\n",
      "  Batch 21,240  of  44,637.    Elapsed: 0:12:38. Training loss. 0.0027765058912336826 Num fake examples 20654 Num true examples 21826\n",
      "  Batch 21,280  of  44,637.    Elapsed: 0:12:40. Training loss. 0.003485037013888359 Num fake examples 20696 Num true examples 21864\n",
      "  Batch 21,320  of  44,637.    Elapsed: 0:12:41. Training loss. 0.008655419573187828 Num fake examples 20734 Num true examples 21906\n",
      "  Batch 21,360  of  44,637.    Elapsed: 0:12:42. Training loss. 0.006166459061205387 Num fake examples 20770 Num true examples 21950\n",
      "  Batch 21,400  of  44,637.    Elapsed: 0:12:44. Training loss. 0.008258337154984474 Num fake examples 20804 Num true examples 21996\n",
      "  Batch 21,440  of  44,637.    Elapsed: 0:12:45. Training loss. 0.00788641907274723 Num fake examples 20847 Num true examples 22033\n",
      "  Batch 21,480  of  44,637.    Elapsed: 0:12:47. Training loss. 0.004467034712433815 Num fake examples 20887 Num true examples 22073\n",
      "  Batch 21,520  of  44,637.    Elapsed: 0:12:48. Training loss. 0.0055254241451621056 Num fake examples 20920 Num true examples 22120\n",
      "  Batch 21,560  of  44,637.    Elapsed: 0:12:50. Training loss. 0.005240542348474264 Num fake examples 20958 Num true examples 22162\n",
      "  Batch 21,600  of  44,637.    Elapsed: 0:12:51. Training loss. 0.004428619518876076 Num fake examples 20998 Num true examples 22202\n",
      "  Batch 21,640  of  44,637.    Elapsed: 0:12:53. Training loss. 0.004477751441299915 Num fake examples 21039 Num true examples 22241\n",
      "  Batch 21,680  of  44,637.    Elapsed: 0:12:54. Training loss. 0.004887286573648453 Num fake examples 21080 Num true examples 22280\n",
      "  Batch 21,720  of  44,637.    Elapsed: 0:12:56. Training loss. 0.002507096389308572 Num fake examples 21119 Num true examples 22321\n",
      "  Batch 21,760  of  44,637.    Elapsed: 0:12:57. Training loss. 0.005375957582145929 Num fake examples 21154 Num true examples 22366\n",
      "  Batch 21,800  of  44,637.    Elapsed: 0:12:59. Training loss. 0.005371777806431055 Num fake examples 21196 Num true examples 22404\n",
      "  Batch 21,840  of  44,637.    Elapsed: 0:13:00. Training loss. 0.00485795084387064 Num fake examples 21228 Num true examples 22452\n",
      "  Batch 21,880  of  44,637.    Elapsed: 0:13:01. Training loss. 0.002631507348269224 Num fake examples 21266 Num true examples 22494\n",
      "  Batch 21,920  of  44,637.    Elapsed: 0:13:03. Training loss. 0.002955581760033965 Num fake examples 21312 Num true examples 22528\n",
      "  Batch 21,960  of  44,637.    Elapsed: 0:13:04. Training loss. 0.0047878073528409 Num fake examples 21343 Num true examples 22577\n",
      "  Batch 22,000  of  44,637.    Elapsed: 0:13:06. Training loss. 0.003394582075998187 Num fake examples 21376 Num true examples 22624\n",
      "  Batch 22,040  of  44,637.    Elapsed: 0:13:07. Training loss. 0.00374274468049407 Num fake examples 21411 Num true examples 22669\n",
      "  Batch 22,080  of  44,637.    Elapsed: 0:13:09. Training loss. 0.0023970145266503096 Num fake examples 21447 Num true examples 22713\n",
      "  Batch 22,120  of  44,637.    Elapsed: 0:13:10. Training loss. 0.002358697820454836 Num fake examples 21483 Num true examples 22757\n",
      "  Batch 22,160  of  44,637.    Elapsed: 0:13:12. Training loss. 0.0032090579625219107 Num fake examples 21515 Num true examples 22805\n",
      "  Batch 22,200  of  44,637.    Elapsed: 0:13:13. Training loss. 0.003245021915063262 Num fake examples 21544 Num true examples 22856\n",
      "  Batch 22,240  of  44,637.    Elapsed: 0:13:14. Training loss. 3.089949131011963 Num fake examples 21584 Num true examples 22896\n",
      "  Batch 22,280  of  44,637.    Elapsed: 0:13:16. Training loss. 0.0038870051503181458 Num fake examples 21624 Num true examples 22936\n",
      "  Batch 22,320  of  44,637.    Elapsed: 0:13:17. Training loss. 0.005705247633159161 Num fake examples 21669 Num true examples 22971\n",
      "  Batch 22,360  of  44,637.    Elapsed: 0:13:19. Training loss. 0.0035037738271057606 Num fake examples 21706 Num true examples 23014\n",
      "  Batch 22,400  of  44,637.    Elapsed: 0:13:20. Training loss. 0.0031710185576230288 Num fake examples 21749 Num true examples 23051\n",
      "  Batch 22,440  of  44,637.    Elapsed: 0:13:22. Training loss. 0.0032532652840018272 Num fake examples 21794 Num true examples 23086\n",
      "  Batch 22,480  of  44,637.    Elapsed: 0:13:23. Training loss. 0.0027576335705816746 Num fake examples 21841 Num true examples 23119\n",
      "  Batch 22,520  of  44,637.    Elapsed: 0:13:25. Training loss. 0.0032632285729050636 Num fake examples 21886 Num true examples 23154\n",
      "  Batch 22,560  of  44,637.    Elapsed: 0:13:26. Training loss. 0.0027223003562539816 Num fake examples 21925 Num true examples 23195\n",
      "  Batch 22,600  of  44,637.    Elapsed: 0:13:27. Training loss. 0.0033167239744216204 Num fake examples 21967 Num true examples 23233\n",
      "  Batch 22,640  of  44,637.    Elapsed: 0:13:29. Training loss. 0.002929524052888155 Num fake examples 22012 Num true examples 23268\n",
      "  Batch 22,680  of  44,637.    Elapsed: 0:13:30. Training loss. 0.0015585790388286114 Num fake examples 22058 Num true examples 23302\n",
      "  Batch 22,720  of  44,637.    Elapsed: 0:13:32. Training loss. 0.0015358829405158758 Num fake examples 22103 Num true examples 23337\n",
      "  Batch 22,760  of  44,637.    Elapsed: 0:13:33. Training loss. 0.0013735112734138966 Num fake examples 22141 Num true examples 23379\n",
      "  Batch 22,800  of  44,637.    Elapsed: 0:13:35. Training loss. 0.0009123491472564638 Num fake examples 22184 Num true examples 23416\n",
      "  Batch 22,840  of  44,637.    Elapsed: 0:13:36. Training loss. 0.0024104248732328415 Num fake examples 22226 Num true examples 23454\n",
      "  Batch 22,880  of  44,637.    Elapsed: 0:13:37. Training loss. 0.0021145017817616463 Num fake examples 22269 Num true examples 23491\n",
      "  Batch 22,920  of  44,637.    Elapsed: 0:13:39. Training loss. 0.0038370469119399786 Num fake examples 22310 Num true examples 23530\n",
      "  Batch 22,960  of  44,637.    Elapsed: 0:13:40. Training loss. 0.005383886396884918 Num fake examples 22349 Num true examples 23571\n",
      "  Batch 23,000  of  44,637.    Elapsed: 0:13:42. Training loss. 0.004689822904765606 Num fake examples 22395 Num true examples 23605\n",
      "  Batch 23,040  of  44,637.    Elapsed: 0:13:43. Training loss. 0.0025781975127756596 Num fake examples 22436 Num true examples 23644\n",
      "  Batch 23,080  of  44,637.    Elapsed: 0:13:45. Training loss. 0.004032059572637081 Num fake examples 22471 Num true examples 23689\n",
      "  Batch 23,120  of  44,637.    Elapsed: 0:13:46. Training loss. 0.0051338220946490765 Num fake examples 22505 Num true examples 23735\n",
      "  Batch 23,160  of  44,637.    Elapsed: 0:13:48. Training loss. 2.752570629119873 Num fake examples 22550 Num true examples 23770\n",
      "  Batch 23,200  of  44,637.    Elapsed: 0:13:49. Training loss. 0.002717850264161825 Num fake examples 22585 Num true examples 23815\n",
      "  Batch 23,240  of  44,637.    Elapsed: 0:13:50. Training loss. 2.9214985370635986 Num fake examples 22622 Num true examples 23858\n",
      "  Batch 23,280  of  44,637.    Elapsed: 0:13:52. Training loss. 0.003996602725237608 Num fake examples 22660 Num true examples 23900\n",
      "  Batch 23,320  of  44,637.    Elapsed: 0:13:53. Training loss. 0.0038652680814266205 Num fake examples 22699 Num true examples 23941\n",
      "  Batch 23,360  of  44,637.    Elapsed: 0:13:55. Training loss. 0.0029677697457373142 Num fake examples 22743 Num true examples 23977\n",
      "  Batch 23,400  of  44,637.    Elapsed: 0:13:56. Training loss. 0.003075329354032874 Num fake examples 22785 Num true examples 24015\n",
      "  Batch 23,440  of  44,637.    Elapsed: 0:13:58. Training loss. 0.004819991998374462 Num fake examples 22827 Num true examples 24053\n",
      "  Batch 23,480  of  44,637.    Elapsed: 0:13:59. Training loss. 0.002551656449213624 Num fake examples 22863 Num true examples 24097\n",
      "  Batch 23,520  of  44,637.    Elapsed: 0:14:01. Training loss. 0.0016822422621771693 Num fake examples 22897 Num true examples 24143\n",
      "  Batch 23,560  of  44,637.    Elapsed: 0:14:02. Training loss. 0.0042916410602629185 Num fake examples 22935 Num true examples 24185\n",
      "  Batch 23,600  of  44,637.    Elapsed: 0:14:03. Training loss. 0.005083847790956497 Num fake examples 22970 Num true examples 24230\n",
      "  Batch 23,640  of  44,637.    Elapsed: 0:14:05. Training loss. 0.002421402605250478 Num fake examples 23011 Num true examples 24269\n",
      "  Batch 23,680  of  44,637.    Elapsed: 0:14:06. Training loss. 0.0019152669701725245 Num fake examples 23044 Num true examples 24316\n",
      "  Batch 23,720  of  44,637.    Elapsed: 0:14:08. Training loss. 0.004646181594580412 Num fake examples 23083 Num true examples 24357\n",
      "  Batch 23,760  of  44,637.    Elapsed: 0:14:09. Training loss. 0.0024408469907939434 Num fake examples 23123 Num true examples 24397\n",
      "  Batch 23,800  of  44,637.    Elapsed: 0:14:11. Training loss. 0.002439340576529503 Num fake examples 23166 Num true examples 24434\n",
      "  Batch 23,840  of  44,637.    Elapsed: 0:14:12. Training loss. 0.003852676833048463 Num fake examples 23202 Num true examples 24478\n",
      "  Batch 23,880  of  44,637.    Elapsed: 0:14:13. Training loss. 0.002368815243244171 Num fake examples 23246 Num true examples 24514\n",
      "  Batch 23,920  of  44,637.    Elapsed: 0:14:15. Training loss. 0.0031750607304275036 Num fake examples 23276 Num true examples 24564\n",
      "  Batch 23,960  of  44,637.    Elapsed: 0:14:16. Training loss. 0.002933783922344446 Num fake examples 23317 Num true examples 24603\n",
      "  Batch 24,000  of  44,637.    Elapsed: 0:14:18. Training loss. 0.003394033992663026 Num fake examples 23352 Num true examples 24648\n",
      "  Batch 24,040  of  44,637.    Elapsed: 0:14:19. Training loss. 0.003119745757430792 Num fake examples 23392 Num true examples 24688\n",
      "  Batch 24,080  of  44,637.    Elapsed: 0:14:21. Training loss. 0.007022627163678408 Num fake examples 23426 Num true examples 24734\n",
      "  Batch 24,120  of  44,637.    Elapsed: 0:14:22. Training loss. 0.00354852550663054 Num fake examples 23467 Num true examples 24773\n",
      "  Batch 24,160  of  44,637.    Elapsed: 0:14:24. Training loss. 0.0029336512088775635 Num fake examples 23509 Num true examples 24811\n",
      "  Batch 24,200  of  44,637.    Elapsed: 0:14:25. Training loss. 0.002333158627152443 Num fake examples 23544 Num true examples 24856\n",
      "  Batch 24,240  of  44,637.    Elapsed: 0:14:26. Training loss. 0.0240378025919199 Num fake examples 23582 Num true examples 24898\n",
      "  Batch 24,280  of  44,637.    Elapsed: 0:14:28. Training loss. 0.003151330165565014 Num fake examples 23615 Num true examples 24945\n",
      "  Batch 24,320  of  44,637.    Elapsed: 0:14:29. Training loss. 2.8170413970947266 Num fake examples 23658 Num true examples 24982\n",
      "  Batch 24,360  of  44,637.    Elapsed: 0:14:31. Training loss. 0.003287080442532897 Num fake examples 23703 Num true examples 25017\n",
      "  Batch 24,400  of  44,637.    Elapsed: 0:14:32. Training loss. 0.0023965188302099705 Num fake examples 23750 Num true examples 25050\n",
      "  Batch 24,440  of  44,637.    Elapsed: 0:14:34. Training loss. 0.0036718002520501614 Num fake examples 23792 Num true examples 25088\n",
      "  Batch 24,480  of  44,637.    Elapsed: 0:14:35. Training loss. 0.003124210750684142 Num fake examples 23827 Num true examples 25133\n",
      "  Batch 24,520  of  44,637.    Elapsed: 0:14:37. Training loss. 0.003050244180485606 Num fake examples 23861 Num true examples 25179\n",
      "  Batch 24,560  of  44,637.    Elapsed: 0:14:38. Training loss. 0.002845372539013624 Num fake examples 23906 Num true examples 25214\n",
      "  Batch 24,600  of  44,637.    Elapsed: 0:14:39. Training loss. 0.0024622082710266113 Num fake examples 23950 Num true examples 25250\n",
      "  Batch 24,640  of  44,637.    Elapsed: 0:14:41. Training loss. 0.0013313281815499067 Num fake examples 23992 Num true examples 25288\n",
      "  Batch 24,680  of  44,637.    Elapsed: 0:14:42. Training loss. 0.0016269232146441936 Num fake examples 24032 Num true examples 25328\n",
      "  Batch 24,720  of  44,637.    Elapsed: 0:14:44. Training loss. 0.0011686398647725582 Num fake examples 24071 Num true examples 25369\n",
      "  Batch 24,760  of  44,637.    Elapsed: 0:14:45. Training loss. 3.131531238555908 Num fake examples 24113 Num true examples 25407\n",
      "  Batch 24,800  of  44,637.    Elapsed: 0:14:47. Training loss. 0.0025097839534282684 Num fake examples 24154 Num true examples 25446\n",
      "  Batch 24,840  of  44,637.    Elapsed: 0:14:48. Training loss. 0.0022563552483916283 Num fake examples 24196 Num true examples 25484\n",
      "  Batch 24,880  of  44,637.    Elapsed: 0:14:50. Training loss. 3.0686769485473633 Num fake examples 24233 Num true examples 25527\n",
      "  Batch 24,920  of  44,637.    Elapsed: 0:14:51. Training loss. 0.00412567425519228 Num fake examples 24274 Num true examples 25566\n",
      "  Batch 24,960  of  44,637.    Elapsed: 0:14:52. Training loss. 0.0034327926114201546 Num fake examples 24311 Num true examples 25609\n",
      "  Batch 25,000  of  44,637.    Elapsed: 0:14:54. Training loss. 0.0037444746121764183 Num fake examples 24353 Num true examples 25647\n",
      "  Batch 25,040  of  44,637.    Elapsed: 0:14:55. Training loss. 0.003115835366770625 Num fake examples 24382 Num true examples 25698\n",
      "  Batch 25,080  of  44,637.    Elapsed: 0:14:57. Training loss. 0.004274703562259674 Num fake examples 24422 Num true examples 25738\n",
      "  Batch 25,120  of  44,637.    Elapsed: 0:14:58. Training loss. 0.004442851059138775 Num fake examples 24463 Num true examples 25777\n",
      "  Batch 25,160  of  44,637.    Elapsed: 0:15:00. Training loss. 0.0037991534918546677 Num fake examples 24507 Num true examples 25813\n",
      "  Batch 25,200  of  44,637.    Elapsed: 0:15:01. Training loss. 0.002484519500285387 Num fake examples 24540 Num true examples 25860\n",
      "  Batch 25,240  of  44,637.    Elapsed: 0:15:03. Training loss. 0.0029306847136467695 Num fake examples 24580 Num true examples 25900\n",
      "  Batch 25,280  of  44,637.    Elapsed: 0:15:04. Training loss. 2.9976248741149902 Num fake examples 24624 Num true examples 25936\n",
      "  Batch 25,320  of  44,637.    Elapsed: 0:15:06. Training loss. 0.0029577412642538548 Num fake examples 24665 Num true examples 25975\n",
      "  Batch 25,360  of  44,637.    Elapsed: 0:15:07. Training loss. 0.002911468269303441 Num fake examples 24704 Num true examples 26016\n",
      "  Batch 25,400  of  44,637.    Elapsed: 0:15:08. Training loss. 0.002325079869478941 Num fake examples 24748 Num true examples 26052\n",
      "  Batch 25,440  of  44,637.    Elapsed: 0:15:10. Training loss. 0.0019994243048131466 Num fake examples 24791 Num true examples 26089\n",
      "  Batch 25,480  of  44,637.    Elapsed: 0:15:11. Training loss. 0.003290862776339054 Num fake examples 24838 Num true examples 26122\n",
      "  Batch 25,520  of  44,637.    Elapsed: 0:15:13. Training loss. 0.00662253750488162 Num fake examples 24880 Num true examples 26160\n",
      "  Batch 25,560  of  44,637.    Elapsed: 0:15:14. Training loss. 0.005182486027479172 Num fake examples 24923 Num true examples 26197\n",
      "  Batch 25,600  of  44,637.    Elapsed: 0:15:16. Training loss. 0.003940938506275415 Num fake examples 24960 Num true examples 26240\n",
      "  Batch 25,640  of  44,637.    Elapsed: 0:15:17. Training loss. 0.0039711687713861465 Num fake examples 24997 Num true examples 26283\n",
      "  Batch 25,680  of  44,637.    Elapsed: 0:15:18. Training loss. 0.002937313402071595 Num fake examples 25040 Num true examples 26320\n",
      "  Batch 25,720  of  44,637.    Elapsed: 0:15:20. Training loss. 0.002559614833444357 Num fake examples 25079 Num true examples 26361\n",
      "  Batch 25,760  of  44,637.    Elapsed: 0:15:21. Training loss. 0.004964960739016533 Num fake examples 25114 Num true examples 26406\n",
      "  Batch 25,800  of  44,637.    Elapsed: 0:15:23. Training loss. 0.004963413812220097 Num fake examples 25155 Num true examples 26445\n",
      "  Batch 25,840  of  44,637.    Elapsed: 0:15:24. Training loss. 0.003864698112010956 Num fake examples 25193 Num true examples 26487\n",
      "  Batch 25,880  of  44,637.    Elapsed: 0:15:26. Training loss. 2.57476806640625 Num fake examples 25245 Num true examples 26515\n",
      "  Batch 25,920  of  44,637.    Elapsed: 0:15:27. Training loss. 0.006704053841531277 Num fake examples 25292 Num true examples 26548\n",
      "  Batch 25,960  of  44,637.    Elapsed: 0:15:29. Training loss. 0.004376498516649008 Num fake examples 25327 Num true examples 26593\n",
      "  Batch 26,000  of  44,637.    Elapsed: 0:15:30. Training loss. 0.005514763295650482 Num fake examples 25363 Num true examples 26637\n",
      "  Batch 26,040  of  44,637.    Elapsed: 0:15:32. Training loss. 0.006334619130939245 Num fake examples 25396 Num true examples 26684\n",
      "  Batch 26,080  of  44,637.    Elapsed: 0:15:33. Training loss. 0.0027949525974690914 Num fake examples 25433 Num true examples 26727\n",
      "  Batch 26,120  of  44,637.    Elapsed: 0:15:35. Training loss. 0.005167421419173479 Num fake examples 25477 Num true examples 26763\n",
      "  Batch 26,160  of  44,637.    Elapsed: 0:15:36. Training loss. 0.004881509579718113 Num fake examples 25512 Num true examples 26808\n",
      "  Batch 26,200  of  44,637.    Elapsed: 0:15:38. Training loss. 0.004156243521720171 Num fake examples 25551 Num true examples 26849\n",
      "  Batch 26,240  of  44,637.    Elapsed: 0:15:39. Training loss. 0.003900204785168171 Num fake examples 25589 Num true examples 26891\n",
      "  Batch 26,280  of  44,637.    Elapsed: 0:15:41. Training loss. 0.005125792231410742 Num fake examples 25619 Num true examples 26941\n",
      "  Batch 26,320  of  44,637.    Elapsed: 0:15:42. Training loss. 0.003449457697570324 Num fake examples 25654 Num true examples 26986\n",
      "  Batch 26,360  of  44,637.    Elapsed: 0:15:44. Training loss. 0.00403177784755826 Num fake examples 25686 Num true examples 27034\n",
      "  Batch 26,400  of  44,637.    Elapsed: 0:15:45. Training loss. 0.002900112420320511 Num fake examples 25727 Num true examples 27073\n",
      "  Batch 26,440  of  44,637.    Elapsed: 0:15:47. Training loss. 0.006348451599478722 Num fake examples 25768 Num true examples 27112\n",
      "  Batch 26,480  of  44,637.    Elapsed: 0:15:48. Training loss. 0.004860562272369862 Num fake examples 25809 Num true examples 27151\n",
      "  Batch 26,520  of  44,637.    Elapsed: 0:15:50. Training loss. 0.0033605522476136684 Num fake examples 25842 Num true examples 27198\n",
      "  Batch 26,560  of  44,637.    Elapsed: 0:15:51. Training loss. 0.0034545441158115864 Num fake examples 25881 Num true examples 27239\n",
      "  Batch 26,600  of  44,637.    Elapsed: 0:15:53. Training loss. 0.0029270683880895376 Num fake examples 25916 Num true examples 27284\n",
      "  Batch 26,640  of  44,637.    Elapsed: 0:15:54. Training loss. 0.004611732438206673 Num fake examples 25952 Num true examples 27328\n",
      "  Batch 26,680  of  44,637.    Elapsed: 0:15:56. Training loss. 0.0025447860825806856 Num fake examples 25991 Num true examples 27369\n",
      "  Batch 26,720  of  44,637.    Elapsed: 0:15:57. Training loss. 0.003036328125745058 Num fake examples 26030 Num true examples 27410\n",
      "  Batch 26,760  of  44,637.    Elapsed: 0:15:59. Training loss. 0.0027681917417794466 Num fake examples 26075 Num true examples 27445\n",
      "  Batch 26,800  of  44,637.    Elapsed: 0:16:00. Training loss. 0.002611549338325858 Num fake examples 26109 Num true examples 27491\n",
      "  Batch 26,840  of  44,637.    Elapsed: 0:16:02. Training loss. 0.0029168776236474514 Num fake examples 26146 Num true examples 27534\n",
      "  Batch 26,880  of  44,637.    Elapsed: 0:16:03. Training loss. 0.002908020978793502 Num fake examples 26186 Num true examples 27574\n",
      "  Batch 26,920  of  44,637.    Elapsed: 0:16:05. Training loss. 0.00250934693031013 Num fake examples 26210 Num true examples 27630\n",
      "  Batch 26,960  of  44,637.    Elapsed: 0:16:06. Training loss. 0.002453963505104184 Num fake examples 26248 Num true examples 27672\n",
      "  Batch 27,000  of  44,637.    Elapsed: 0:16:08. Training loss. 0.0014886928256601095 Num fake examples 26289 Num true examples 27711\n",
      "  Batch 27,040  of  44,637.    Elapsed: 0:16:09. Training loss. 0.0019058999605476856 Num fake examples 26319 Num true examples 27761\n",
      "  Batch 27,080  of  44,637.    Elapsed: 0:16:11. Training loss. 0.0028690905310213566 Num fake examples 26358 Num true examples 27802\n",
      "  Batch 27,120  of  44,637.    Elapsed: 0:16:12. Training loss. 0.004431994631886482 Num fake examples 26390 Num true examples 27850\n",
      "  Batch 27,160  of  44,637.    Elapsed: 0:16:14. Training loss. 2.7360010147094727 Num fake examples 26428 Num true examples 27892\n",
      "  Batch 27,200  of  44,637.    Elapsed: 0:16:15. Training loss. 0.004831921309232712 Num fake examples 26473 Num true examples 27927\n",
      "  Batch 27,240  of  44,637.    Elapsed: 0:16:17. Training loss. 0.0038032964803278446 Num fake examples 26509 Num true examples 27971\n",
      "  Batch 27,280  of  44,637.    Elapsed: 0:16:18. Training loss. 0.005212871357798576 Num fake examples 26547 Num true examples 28013\n",
      "  Batch 27,320  of  44,637.    Elapsed: 0:16:19. Training loss. 0.003121935762465 Num fake examples 26597 Num true examples 28043\n",
      "  Batch 27,360  of  44,637.    Elapsed: 0:16:21. Training loss. 0.0023648906499147415 Num fake examples 26637 Num true examples 28083\n",
      "  Batch 27,400  of  44,637.    Elapsed: 0:16:22. Training loss. 0.0022929832339286804 Num fake examples 26680 Num true examples 28120\n",
      "  Batch 27,440  of  44,637.    Elapsed: 0:16:24. Training loss. 0.0034210742451250553 Num fake examples 26717 Num true examples 28163\n",
      "  Batch 27,480  of  44,637.    Elapsed: 0:16:25. Training loss. 0.002605128102004528 Num fake examples 26751 Num true examples 28209\n",
      "  Batch 27,520  of  44,637.    Elapsed: 0:16:27. Training loss. 0.0018322712276130915 Num fake examples 26791 Num true examples 28249\n",
      "  Batch 27,560  of  44,637.    Elapsed: 0:16:28. Training loss. 0.001774720149114728 Num fake examples 26832 Num true examples 28288\n",
      "  Batch 27,600  of  44,637.    Elapsed: 0:16:30. Training loss. 0.0018053168896585703 Num fake examples 26870 Num true examples 28330\n",
      "  Batch 27,640  of  44,637.    Elapsed: 0:16:31. Training loss. 0.0011983320582658052 Num fake examples 26909 Num true examples 28371\n",
      "  Batch 27,680  of  44,637.    Elapsed: 0:16:33. Training loss. 0.0022200075909495354 Num fake examples 26949 Num true examples 28411\n",
      "  Batch 27,720  of  44,637.    Elapsed: 0:16:34. Training loss. 0.0018143374472856522 Num fake examples 26992 Num true examples 28448\n",
      "  Batch 27,760  of  44,637.    Elapsed: 0:16:36. Training loss. 0.0019044764339923859 Num fake examples 27038 Num true examples 28482\n",
      "  Batch 27,800  of  44,637.    Elapsed: 0:16:37. Training loss. 0.0023246612399816513 Num fake examples 27066 Num true examples 28534\n",
      "  Batch 27,840  of  44,637.    Elapsed: 0:16:39. Training loss. 0.002963896607980132 Num fake examples 27101 Num true examples 28579\n",
      "  Batch 27,880  of  44,637.    Elapsed: 0:16:40. Training loss. 0.002939794212579727 Num fake examples 27136 Num true examples 28624\n",
      "  Batch 27,920  of  44,637.    Elapsed: 0:16:42. Training loss. 0.002958199940621853 Num fake examples 27179 Num true examples 28661\n",
      "  Batch 27,960  of  44,637.    Elapsed: 0:16:43. Training loss. 0.0030462252907454967 Num fake examples 27215 Num true examples 28705\n",
      "  Batch 28,000  of  44,637.    Elapsed: 0:16:45. Training loss. 0.0027746115811169147 Num fake examples 27255 Num true examples 28745\n",
      "  Batch 28,040  of  44,637.    Elapsed: 0:16:46. Training loss. 0.0027658483013510704 Num fake examples 27295 Num true examples 28785\n",
      "  Batch 28,080  of  44,637.    Elapsed: 0:16:48. Training loss. 0.0038665966130793095 Num fake examples 27326 Num true examples 28834\n",
      "  Batch 28,120  of  44,637.    Elapsed: 0:16:49. Training loss. 0.004421089310199022 Num fake examples 27362 Num true examples 28878\n",
      "  Batch 28,160  of  44,637.    Elapsed: 0:16:51. Training loss. 0.002220523077994585 Num fake examples 27406 Num true examples 28914\n",
      "  Batch 28,200  of  44,637.    Elapsed: 0:16:52. Training loss. 0.003579166019335389 Num fake examples 27440 Num true examples 28960\n",
      "  Batch 28,240  of  44,637.    Elapsed: 0:16:54. Training loss. 0.0032613142393529415 Num fake examples 27480 Num true examples 29000\n",
      "  Batch 28,280  of  44,637.    Elapsed: 0:16:55. Training loss. 0.0030323113314807415 Num fake examples 27519 Num true examples 29041\n",
      "  Batch 28,320  of  44,637.    Elapsed: 0:16:57. Training loss. 0.004392090253531933 Num fake examples 27556 Num true examples 29084\n",
      "  Batch 28,360  of  44,637.    Elapsed: 0:16:58. Training loss. 0.005465506576001644 Num fake examples 27594 Num true examples 29126\n",
      "  Batch 28,400  of  44,637.    Elapsed: 0:17:00. Training loss. 0.005629172548651695 Num fake examples 27639 Num true examples 29161\n",
      "  Batch 28,440  of  44,637.    Elapsed: 0:17:01. Training loss. 0.004142872989177704 Num fake examples 27681 Num true examples 29199\n",
      "  Batch 28,480  of  44,637.    Elapsed: 0:17:03. Training loss. 0.0035585719160735607 Num fake examples 27724 Num true examples 29236\n",
      "  Batch 28,520  of  44,637.    Elapsed: 0:17:04. Training loss. 0.004433429799973965 Num fake examples 27758 Num true examples 29282\n",
      "  Batch 28,560  of  44,637.    Elapsed: 0:17:05. Training loss. 0.0053425380028784275 Num fake examples 27798 Num true examples 29322\n",
      "  Batch 28,600  of  44,637.    Elapsed: 0:17:07. Training loss. 0.004077587276697159 Num fake examples 27831 Num true examples 29369\n",
      "  Batch 28,640  of  44,637.    Elapsed: 0:17:08. Training loss. 0.004645194858312607 Num fake examples 27875 Num true examples 29405\n",
      "  Batch 28,680  of  44,637.    Elapsed: 0:17:10. Training loss. 0.006195232272148132 Num fake examples 27915 Num true examples 29445\n",
      "  Batch 28,720  of  44,637.    Elapsed: 0:17:11. Training loss. 0.002401501638814807 Num fake examples 27960 Num true examples 29480\n",
      "  Batch 28,760  of  44,637.    Elapsed: 0:17:13. Training loss. 0.006224778946489096 Num fake examples 27998 Num true examples 29522\n",
      "  Batch 28,800  of  44,637.    Elapsed: 0:17:14. Training loss. 0.005903381854295731 Num fake examples 28039 Num true examples 29561\n",
      "  Batch 28,840  of  44,637.    Elapsed: 0:17:16. Training loss. 0.0030501801520586014 Num fake examples 28079 Num true examples 29601\n",
      "  Batch 28,880  of  44,637.    Elapsed: 0:17:17. Training loss. 0.0023027556017041206 Num fake examples 28116 Num true examples 29644\n",
      "  Batch 28,920  of  44,637.    Elapsed: 0:17:19. Training loss. 0.002462351694703102 Num fake examples 28159 Num true examples 29681\n",
      "  Batch 28,960  of  44,637.    Elapsed: 0:17:20. Training loss. 3.028050661087036 Num fake examples 28195 Num true examples 29725\n",
      "  Batch 29,000  of  44,637.    Elapsed: 0:17:21. Training loss. 0.003501737955957651 Num fake examples 28231 Num true examples 29769\n",
      "  Batch 29,040  of  44,637.    Elapsed: 0:17:23. Training loss. 0.003175765508785844 Num fake examples 28269 Num true examples 29811\n",
      "  Batch 29,080  of  44,637.    Elapsed: 0:17:24. Training loss. 0.0037452662363648415 Num fake examples 28303 Num true examples 29857\n",
      "  Batch 29,120  of  44,637.    Elapsed: 0:17:26. Training loss. 0.0058994838036596775 Num fake examples 28340 Num true examples 29900\n",
      "  Batch 29,160  of  44,637.    Elapsed: 0:17:27. Training loss. 0.003261236473917961 Num fake examples 28378 Num true examples 29942\n",
      "  Batch 29,200  of  44,637.    Elapsed: 0:17:29. Training loss. 0.003390625352039933 Num fake examples 28412 Num true examples 29988\n",
      "  Batch 29,240  of  44,637.    Elapsed: 0:17:30. Training loss. 0.003145736176520586 Num fake examples 28456 Num true examples 30024\n",
      "  Batch 29,280  of  44,637.    Elapsed: 0:17:32. Training loss. 0.003052043030038476 Num fake examples 28497 Num true examples 30063\n",
      "  Batch 29,320  of  44,637.    Elapsed: 0:17:33. Training loss. 0.0026285075582563877 Num fake examples 28535 Num true examples 30105\n",
      "  Batch 29,360  of  44,637.    Elapsed: 0:17:34. Training loss. 0.0025057909078896046 Num fake examples 28579 Num true examples 30141\n",
      "  Batch 29,400  of  44,637.    Elapsed: 0:17:36. Training loss. 0.0033639369066804647 Num fake examples 28607 Num true examples 30193\n",
      "  Batch 29,440  of  44,637.    Elapsed: 0:17:37. Training loss. 0.0028835213743150234 Num fake examples 28644 Num true examples 30236\n",
      "  Batch 29,480  of  44,637.    Elapsed: 0:17:39. Training loss. 0.002272629179060459 Num fake examples 28677 Num true examples 30283\n",
      "  Batch 29,520  of  44,637.    Elapsed: 0:17:40. Training loss. 0.003275164170190692 Num fake examples 28718 Num true examples 30322\n",
      "  Batch 29,560  of  44,637.    Elapsed: 0:17:42. Training loss. 2.8665969371795654 Num fake examples 28766 Num true examples 30354\n",
      "  Batch 29,600  of  44,637.    Elapsed: 0:17:43. Training loss. 0.001948478864505887 Num fake examples 28807 Num true examples 30393\n",
      "  Batch 29,640  of  44,637.    Elapsed: 0:17:45. Training loss. 0.004880405031144619 Num fake examples 28842 Num true examples 30438\n",
      "  Batch 29,680  of  44,637.    Elapsed: 0:17:46. Training loss. 0.0036274674348533154 Num fake examples 28885 Num true examples 30475\n",
      "  Batch 29,720  of  44,637.    Elapsed: 0:17:48. Training loss. 0.0033205479849129915 Num fake examples 28936 Num true examples 30504\n",
      "  Batch 29,760  of  44,637.    Elapsed: 0:17:49. Training loss. 0.0016136718913912773 Num fake examples 28973 Num true examples 30547\n",
      "  Batch 29,800  of  44,637.    Elapsed: 0:17:50. Training loss. 0.0036704260855913162 Num fake examples 29009 Num true examples 30591\n",
      "  Batch 29,840  of  44,637.    Elapsed: 0:17:52. Training loss. 0.0040748389437794685 Num fake examples 29050 Num true examples 30630\n",
      "  Batch 29,880  of  44,637.    Elapsed: 0:17:53. Training loss. 0.004015597980469465 Num fake examples 29088 Num true examples 30672\n",
      "  Batch 29,920  of  44,637.    Elapsed: 0:17:55. Training loss. 0.006099516525864601 Num fake examples 29125 Num true examples 30715\n",
      "  Batch 29,960  of  44,637.    Elapsed: 0:17:56. Training loss. 0.0038031674921512604 Num fake examples 29166 Num true examples 30754\n",
      "  Batch 30,000  of  44,637.    Elapsed: 0:17:58. Training loss. 0.0020776360761374235 Num fake examples 29202 Num true examples 30798\n",
      "  Batch 30,040  of  44,637.    Elapsed: 0:17:59. Training loss. 0.002050650306046009 Num fake examples 29245 Num true examples 30835\n",
      "  Batch 30,080  of  44,637.    Elapsed: 0:18:01. Training loss. 0.005010656546801329 Num fake examples 29280 Num true examples 30880\n",
      "  Batch 30,120  of  44,637.    Elapsed: 0:18:02. Training loss. 2.7286665439605713 Num fake examples 29316 Num true examples 30924\n",
      "  Batch 30,160  of  44,637.    Elapsed: 0:18:03. Training loss. 0.0027443040162324905 Num fake examples 29358 Num true examples 30962\n",
      "  Batch 30,200  of  44,637.    Elapsed: 0:18:05. Training loss. 0.0025967636611312628 Num fake examples 29393 Num true examples 31007\n",
      "  Batch 30,240  of  44,637.    Elapsed: 0:18:06. Training loss. 0.004277273081243038 Num fake examples 29434 Num true examples 31046\n",
      "  Batch 30,280  of  44,637.    Elapsed: 0:18:08. Training loss. 0.0036765122786164284 Num fake examples 29471 Num true examples 31089\n",
      "  Batch 30,320  of  44,637.    Elapsed: 0:18:09. Training loss. 0.002349512418732047 Num fake examples 29514 Num true examples 31126\n",
      "  Batch 30,360  of  44,637.    Elapsed: 0:18:11. Training loss. 0.002997955773025751 Num fake examples 29547 Num true examples 31173\n",
      "  Batch 30,400  of  44,637.    Elapsed: 0:18:12. Training loss. 0.0027488465420901775 Num fake examples 29587 Num true examples 31213\n",
      "  Batch 30,440  of  44,637.    Elapsed: 0:18:14. Training loss. 0.0033563936594873667 Num fake examples 29622 Num true examples 31258\n",
      "  Batch 30,480  of  44,637.    Elapsed: 0:18:15. Training loss. 0.005724530667066574 Num fake examples 29661 Num true examples 31299\n",
      "  Batch 30,520  of  44,637.    Elapsed: 0:18:16. Training loss. 2.899268388748169 Num fake examples 29700 Num true examples 31340\n",
      "  Batch 30,560  of  44,637.    Elapsed: 0:18:18. Training loss. 0.006019485183060169 Num fake examples 29736 Num true examples 31384\n",
      "  Batch 30,600  of  44,637.    Elapsed: 0:18:19. Training loss. 0.005720849614590406 Num fake examples 29773 Num true examples 31427\n",
      "  Batch 30,640  of  44,637.    Elapsed: 0:18:21. Training loss. 0.0034538842737674713 Num fake examples 29816 Num true examples 31464\n",
      "  Batch 30,680  of  44,637.    Elapsed: 0:18:23. Training loss. 0.003667398588731885 Num fake examples 29855 Num true examples 31505\n",
      "  Batch 30,720  of  44,637.    Elapsed: 0:18:24. Training loss. 0.0024742865934967995 Num fake examples 29902 Num true examples 31538\n",
      "  Batch 30,760  of  44,637.    Elapsed: 0:18:26. Training loss. 0.0034728418104350567 Num fake examples 29942 Num true examples 31578\n",
      "  Batch 30,800  of  44,637.    Elapsed: 0:18:27. Training loss. 0.002610504627227783 Num fake examples 29985 Num true examples 31615\n",
      "  Batch 30,840  of  44,637.    Elapsed: 0:18:29. Training loss. 0.0038048154674470425 Num fake examples 30026 Num true examples 31654\n",
      "  Batch 30,880  of  44,637.    Elapsed: 0:18:30. Training loss. 0.0033062491565942764 Num fake examples 30066 Num true examples 31694\n",
      "  Batch 30,920  of  44,637.    Elapsed: 0:18:32. Training loss. 0.0027989246882498264 Num fake examples 30106 Num true examples 31734\n",
      "  Batch 30,960  of  44,637.    Elapsed: 0:18:33. Training loss. 0.0036477958783507347 Num fake examples 30144 Num true examples 31776\n",
      "  Batch 31,000  of  44,637.    Elapsed: 0:18:35. Training loss. 0.0037201917730271816 Num fake examples 30182 Num true examples 31818\n",
      "  Batch 31,040  of  44,637.    Elapsed: 0:18:36. Training loss. 0.0037582439836114645 Num fake examples 30217 Num true examples 31863\n",
      "  Batch 31,080  of  44,637.    Elapsed: 0:18:38. Training loss. 0.005414220038801432 Num fake examples 30256 Num true examples 31904\n",
      "  Batch 31,120  of  44,637.    Elapsed: 0:18:39. Training loss. 0.002109087072312832 Num fake examples 30296 Num true examples 31944\n",
      "  Batch 31,160  of  44,637.    Elapsed: 0:18:41. Training loss. 0.0022534537129104137 Num fake examples 30343 Num true examples 31977\n",
      "  Batch 31,200  of  44,637.    Elapsed: 0:18:42. Training loss. 0.003082087030634284 Num fake examples 30389 Num true examples 32011\n",
      "  Batch 31,240  of  44,637.    Elapsed: 0:18:44. Training loss. 0.00406658835709095 Num fake examples 30425 Num true examples 32055\n",
      "  Batch 31,280  of  44,637.    Elapsed: 0:18:45. Training loss. 0.003927439916878939 Num fake examples 30464 Num true examples 32096\n",
      "  Batch 31,320  of  44,637.    Elapsed: 0:18:47. Training loss. 0.0032278066501021385 Num fake examples 30504 Num true examples 32136\n",
      "  Batch 31,360  of  44,637.    Elapsed: 0:18:48. Training loss. 0.0035301537718623877 Num fake examples 30539 Num true examples 32181\n",
      "  Batch 31,400  of  44,637.    Elapsed: 0:18:50. Training loss. 0.0016111972508952022 Num fake examples 30578 Num true examples 32222\n",
      "  Batch 31,440  of  44,637.    Elapsed: 0:18:51. Training loss. 0.0037625168915838003 Num fake examples 30616 Num true examples 32264\n",
      "  Batch 31,480  of  44,637.    Elapsed: 0:18:53. Training loss. 0.0033425912261009216 Num fake examples 30649 Num true examples 32311\n",
      "  Batch 31,520  of  44,637.    Elapsed: 0:18:54. Training loss. 0.0038506626151502132 Num fake examples 30681 Num true examples 32359\n",
      "  Batch 31,560  of  44,637.    Elapsed: 0:18:56. Training loss. 0.0058721741661429405 Num fake examples 30719 Num true examples 32401\n",
      "  Batch 31,600  of  44,637.    Elapsed: 0:18:57. Training loss. 0.005735237151384354 Num fake examples 30757 Num true examples 32443\n",
      "  Batch 31,640  of  44,637.    Elapsed: 0:18:59. Training loss. 0.004630323965102434 Num fake examples 30802 Num true examples 32478\n",
      "  Batch 31,680  of  44,637.    Elapsed: 0:19:00. Training loss. 0.0034257385414093733 Num fake examples 30841 Num true examples 32519\n",
      "  Batch 31,720  of  44,637.    Elapsed: 0:19:02. Training loss. 2.753384590148926 Num fake examples 30883 Num true examples 32557\n",
      "  Batch 31,760  of  44,637.    Elapsed: 0:19:03. Training loss. 0.0049360753037035465 Num fake examples 30931 Num true examples 32589\n",
      "  Batch 31,800  of  44,637.    Elapsed: 0:19:04. Training loss. 0.003723790403455496 Num fake examples 30967 Num true examples 32633\n",
      "  Batch 31,840  of  44,637.    Elapsed: 0:19:06. Training loss. 0.004519561771303415 Num fake examples 31003 Num true examples 32677\n",
      "  Batch 31,880  of  44,637.    Elapsed: 0:19:07. Training loss. 0.004069613292813301 Num fake examples 31055 Num true examples 32705\n",
      "  Batch 31,920  of  44,637.    Elapsed: 0:19:09. Training loss. 0.0027639674954116344 Num fake examples 31095 Num true examples 32745\n",
      "  Batch 31,960  of  44,637.    Elapsed: 0:19:10. Training loss. 0.0019214514177292585 Num fake examples 31136 Num true examples 32784\n",
      "  Batch 32,000  of  44,637.    Elapsed: 0:19:12. Training loss. 0.004489506594836712 Num fake examples 31172 Num true examples 32828\n",
      "  Batch 32,040  of  44,637.    Elapsed: 0:19:13. Training loss. 0.0025521782226860523 Num fake examples 31214 Num true examples 32866\n",
      "  Batch 32,080  of  44,637.    Elapsed: 0:19:15. Training loss. 0.004525964148342609 Num fake examples 31255 Num true examples 32905\n",
      "  Batch 32,120  of  44,637.    Elapsed: 0:19:16. Training loss. 0.0068243700079619884 Num fake examples 31291 Num true examples 32949\n",
      "  Batch 32,160  of  44,637.    Elapsed: 0:19:18. Training loss. 0.006344344932585955 Num fake examples 31336 Num true examples 32984\n",
      "  Batch 32,200  of  44,637.    Elapsed: 0:19:19. Training loss. 0.005012765526771545 Num fake examples 31368 Num true examples 33032\n",
      "  Batch 32,240  of  44,637.    Elapsed: 0:19:21. Training loss. 0.005060384515672922 Num fake examples 31401 Num true examples 33079\n",
      "  Batch 32,280  of  44,637.    Elapsed: 0:19:22. Training loss. 0.003214809577912092 Num fake examples 31443 Num true examples 33117\n",
      "  Batch 32,320  of  44,637.    Elapsed: 0:19:24. Training loss. 0.004125841893255711 Num fake examples 31478 Num true examples 33162\n",
      "  Batch 32,360  of  44,637.    Elapsed: 0:19:25. Training loss. 0.00358671136200428 Num fake examples 31514 Num true examples 33206\n",
      "  Batch 32,400  of  44,637.    Elapsed: 0:19:27. Training loss. 0.00443809200078249 Num fake examples 31551 Num true examples 33249\n",
      "  Batch 32,440  of  44,637.    Elapsed: 0:19:28. Training loss. 0.004015639889985323 Num fake examples 31592 Num true examples 33288\n",
      "  Batch 32,480  of  44,637.    Elapsed: 0:19:30. Training loss. 0.0017891565803438425 Num fake examples 31623 Num true examples 33337\n",
      "  Batch 32,520  of  44,637.    Elapsed: 0:19:31. Training loss. 0.0035077095963060856 Num fake examples 31663 Num true examples 33377\n",
      "  Batch 32,560  of  44,637.    Elapsed: 0:19:33. Training loss. 2.835047960281372 Num fake examples 31693 Num true examples 33427\n",
      "  Batch 32,600  of  44,637.    Elapsed: 0:19:34. Training loss. 0.003181328298524022 Num fake examples 31731 Num true examples 33469\n",
      "  Batch 32,640  of  44,637.    Elapsed: 0:19:36. Training loss. 0.002230283571407199 Num fake examples 31771 Num true examples 33509\n",
      "  Batch 32,680  of  44,637.    Elapsed: 0:19:37. Training loss. 0.0032890194561332464 Num fake examples 31809 Num true examples 33551\n",
      "  Batch 32,720  of  44,637.    Elapsed: 0:19:39. Training loss. 0.003038510913029313 Num fake examples 31858 Num true examples 33582\n",
      "  Batch 32,760  of  44,637.    Elapsed: 0:19:40. Training loss. 0.0033036069944500923 Num fake examples 31901 Num true examples 33619\n",
      "  Batch 32,800  of  44,637.    Elapsed: 0:19:42. Training loss. 0.004032230004668236 Num fake examples 31940 Num true examples 33660\n",
      "  Batch 32,840  of  44,637.    Elapsed: 0:19:43. Training loss. 0.0026460359804332256 Num fake examples 31980 Num true examples 33700\n",
      "  Batch 32,880  of  44,637.    Elapsed: 0:19:45. Training loss. 0.004245959222316742 Num fake examples 32024 Num true examples 33736\n",
      "  Batch 32,920  of  44,637.    Elapsed: 0:19:46. Training loss. 0.004507375881075859 Num fake examples 32058 Num true examples 33782\n",
      "  Batch 32,960  of  44,637.    Elapsed: 0:19:48. Training loss. 0.0028717084787786007 Num fake examples 32097 Num true examples 33823\n",
      "  Batch 33,000  of  44,637.    Elapsed: 0:19:49. Training loss. 0.00307028042152524 Num fake examples 32138 Num true examples 33862\n",
      "  Batch 33,040  of  44,637.    Elapsed: 0:19:51. Training loss. 0.0034343451261520386 Num fake examples 32173 Num true examples 33907\n",
      "  Batch 33,080  of  44,637.    Elapsed: 0:19:52. Training loss. 0.002293679863214493 Num fake examples 32210 Num true examples 33950\n",
      "  Batch 33,120  of  44,637.    Elapsed: 0:19:54. Training loss. 0.002062257844954729 Num fake examples 32251 Num true examples 33989\n",
      "  Batch 33,160  of  44,637.    Elapsed: 0:19:55. Training loss. 0.002925325185060501 Num fake examples 32291 Num true examples 34029\n",
      "  Batch 33,200  of  44,637.    Elapsed: 0:19:57. Training loss. 0.0026443926617503166 Num fake examples 32327 Num true examples 34073\n",
      "  Batch 33,240  of  44,637.    Elapsed: 0:19:58. Training loss. 0.0020920131355524063 Num fake examples 32372 Num true examples 34108\n",
      "  Batch 33,280  of  44,637.    Elapsed: 0:20:00. Training loss. 0.001607248093932867 Num fake examples 32414 Num true examples 34146\n",
      "  Batch 33,320  of  44,637.    Elapsed: 0:20:01. Training loss. 0.0022160722874104977 Num fake examples 32461 Num true examples 34179\n",
      "  Batch 33,360  of  44,637.    Elapsed: 0:20:03. Training loss. 0.0020443443208932877 Num fake examples 32502 Num true examples 34218\n",
      "  Batch 33,400  of  44,637.    Elapsed: 0:20:04. Training loss. 0.002814809326082468 Num fake examples 32541 Num true examples 34259\n",
      "  Batch 33,440  of  44,637.    Elapsed: 0:20:06. Training loss. 0.003043500706553459 Num fake examples 32583 Num true examples 34297\n",
      "  Batch 33,480  of  44,637.    Elapsed: 0:20:07. Training loss. 0.002979851560667157 Num fake examples 32619 Num true examples 34341\n",
      "  Batch 33,520  of  44,637.    Elapsed: 0:20:09. Training loss. 0.002137462142854929 Num fake examples 32669 Num true examples 34371\n",
      "  Batch 33,560  of  44,637.    Elapsed: 0:20:10. Training loss. 0.0018708778079599142 Num fake examples 32701 Num true examples 34419\n",
      "  Batch 33,600  of  44,637.    Elapsed: 0:20:11. Training loss. 0.0024732258170843124 Num fake examples 32739 Num true examples 34461\n",
      "  Batch 33,640  of  44,637.    Elapsed: 0:20:13. Training loss. 0.0014521691482514143 Num fake examples 32772 Num true examples 34508\n",
      "  Batch 33,680  of  44,637.    Elapsed: 0:20:14. Training loss. 0.0022726343013346195 Num fake examples 32813 Num true examples 34547\n",
      "  Batch 33,720  of  44,637.    Elapsed: 0:20:16. Training loss. 0.003132003592327237 Num fake examples 32862 Num true examples 34578\n",
      "  Batch 33,760  of  44,637.    Elapsed: 0:20:17. Training loss. 0.00220725079998374 Num fake examples 32911 Num true examples 34609\n",
      "  Batch 33,800  of  44,637.    Elapsed: 0:20:19. Training loss. 0.0026579545810818672 Num fake examples 32954 Num true examples 34646\n",
      "  Batch 33,840  of  44,637.    Elapsed: 0:20:20. Training loss. 0.0028826971538364887 Num fake examples 32997 Num true examples 34683\n",
      "  Batch 33,880  of  44,637.    Elapsed: 0:20:22. Training loss. 0.002231935039162636 Num fake examples 33037 Num true examples 34723\n",
      "  Batch 33,920  of  44,637.    Elapsed: 0:20:23. Training loss. 0.0019984315149486065 Num fake examples 33073 Num true examples 34767\n",
      "  Batch 33,960  of  44,637.    Elapsed: 0:20:25. Training loss. 0.0025703059509396553 Num fake examples 33120 Num true examples 34800\n",
      "  Batch 34,000  of  44,637.    Elapsed: 0:20:26. Training loss. 0.002254239981994033 Num fake examples 33154 Num true examples 34846\n",
      "  Batch 34,040  of  44,637.    Elapsed: 0:20:28. Training loss. 0.002189136343076825 Num fake examples 33195 Num true examples 34885\n",
      "  Batch 34,080  of  44,637.    Elapsed: 0:20:29. Training loss. 0.002463721204549074 Num fake examples 33238 Num true examples 34922\n",
      "  Batch 34,120  of  44,637.    Elapsed: 0:20:31. Training loss. 0.0030199105385690928 Num fake examples 33279 Num true examples 34961\n",
      "  Batch 34,160  of  44,637.    Elapsed: 0:20:32. Training loss. 0.003359172958880663 Num fake examples 33320 Num true examples 35000\n",
      "  Batch 34,200  of  44,637.    Elapsed: 0:20:34. Training loss. 0.005028326064348221 Num fake examples 33352 Num true examples 35048\n",
      "  Batch 34,240  of  44,637.    Elapsed: 0:20:35. Training loss. 0.004868500400334597 Num fake examples 33383 Num true examples 35097\n",
      "  Batch 34,280  of  44,637.    Elapsed: 0:20:37. Training loss. 0.006036271341145039 Num fake examples 33430 Num true examples 35130\n",
      "  Batch 34,320  of  44,637.    Elapsed: 0:20:38. Training loss. 0.004166828468441963 Num fake examples 33469 Num true examples 35171\n",
      "  Batch 34,360  of  44,637.    Elapsed: 0:20:40. Training loss. 0.002859467873349786 Num fake examples 33498 Num true examples 35222\n",
      "  Batch 34,400  of  44,637.    Elapsed: 0:20:41. Training loss. 0.0022420063614845276 Num fake examples 33526 Num true examples 35274\n",
      "  Batch 34,440  of  44,637.    Elapsed: 0:20:43. Training loss. 0.005196941085159779 Num fake examples 33571 Num true examples 35309\n",
      "  Batch 34,480  of  44,637.    Elapsed: 0:20:44. Training loss. 0.004269835539162159 Num fake examples 33607 Num true examples 35353\n",
      "  Batch 34,520  of  44,637.    Elapsed: 0:20:46. Training loss. 0.0036173223052173853 Num fake examples 33648 Num true examples 35392\n",
      "  Batch 34,560  of  44,637.    Elapsed: 0:20:47. Training loss. 0.002579787280410528 Num fake examples 33686 Num true examples 35434\n",
      "  Batch 34,600  of  44,637.    Elapsed: 0:20:49. Training loss. 0.005979790817946196 Num fake examples 33719 Num true examples 35481\n",
      "  Batch 34,640  of  44,637.    Elapsed: 0:20:50. Training loss. 0.004253900609910488 Num fake examples 33763 Num true examples 35517\n",
      "  Batch 34,680  of  44,637.    Elapsed: 0:20:52. Training loss. 2.801184892654419 Num fake examples 33801 Num true examples 35559\n",
      "  Batch 34,720  of  44,637.    Elapsed: 0:20:53. Training loss. 0.004799882881343365 Num fake examples 33840 Num true examples 35600\n",
      "  Batch 34,760  of  44,637.    Elapsed: 0:20:55. Training loss. 0.006401072256267071 Num fake examples 33877 Num true examples 35643\n",
      "  Batch 34,800  of  44,637.    Elapsed: 0:20:56. Training loss. 0.011234389618039131 Num fake examples 33914 Num true examples 35686\n",
      "  Batch 34,840  of  44,637.    Elapsed: 0:20:58. Training loss. 0.002599411178380251 Num fake examples 33938 Num true examples 35742\n",
      "  Batch 34,880  of  44,637.    Elapsed: 0:20:59. Training loss. 0.004068377427756786 Num fake examples 33973 Num true examples 35787\n",
      "  Batch 34,920  of  44,637.    Elapsed: 0:21:01. Training loss. 0.0031888431403785944 Num fake examples 34021 Num true examples 35819\n",
      "  Batch 34,960  of  44,637.    Elapsed: 0:21:02. Training loss. 0.0028793103992938995 Num fake examples 34056 Num true examples 35864\n",
      "  Batch 35,000  of  44,637.    Elapsed: 0:21:03. Training loss. 0.0032418626360595226 Num fake examples 34095 Num true examples 35905\n",
      "  Batch 35,040  of  44,637.    Elapsed: 0:21:05. Training loss. 0.0030293483287096024 Num fake examples 34135 Num true examples 35945\n",
      "  Batch 35,080  of  44,637.    Elapsed: 0:21:06. Training loss. 0.0029988037422299385 Num fake examples 34176 Num true examples 35984\n",
      "  Batch 35,120  of  44,637.    Elapsed: 0:21:08. Training loss. 0.003676943015307188 Num fake examples 34212 Num true examples 36028\n",
      "  Batch 35,160  of  44,637.    Elapsed: 0:21:09. Training loss. 0.0020319721661508083 Num fake examples 34244 Num true examples 36076\n",
      "  Batch 35,200  of  44,637.    Elapsed: 0:21:11. Training loss. 0.0036299950443208218 Num fake examples 34284 Num true examples 36116\n",
      "  Batch 35,240  of  44,637.    Elapsed: 0:21:12. Training loss. 0.002677755896002054 Num fake examples 34321 Num true examples 36159\n",
      "  Batch 35,280  of  44,637.    Elapsed: 0:21:14. Training loss. 0.00297806435264647 Num fake examples 34350 Num true examples 36210\n",
      "  Batch 35,320  of  44,637.    Elapsed: 0:21:15. Training loss. 0.0038869406562298536 Num fake examples 34381 Num true examples 36259\n",
      "  Batch 35,360  of  44,637.    Elapsed: 0:21:17. Training loss. 0.0015245497925207019 Num fake examples 34419 Num true examples 36301\n",
      "  Batch 35,400  of  44,637.    Elapsed: 0:21:18. Training loss. 0.00227830046787858 Num fake examples 34454 Num true examples 36346\n",
      "  Batch 35,440  of  44,637.    Elapsed: 0:21:20. Training loss. 0.001975833438336849 Num fake examples 34494 Num true examples 36386\n",
      "  Batch 35,480  of  44,637.    Elapsed: 0:21:21. Training loss. 0.002526378259062767 Num fake examples 34528 Num true examples 36432\n",
      "  Batch 35,520  of  44,637.    Elapsed: 0:21:22. Training loss. 0.004266675561666489 Num fake examples 34573 Num true examples 36467\n",
      "  Batch 35,560  of  44,637.    Elapsed: 0:21:24. Training loss. 0.004614879377186298 Num fake examples 34612 Num true examples 36508\n",
      "  Batch 35,600  of  44,637.    Elapsed: 0:21:25. Training loss. 0.0037633671890944242 Num fake examples 34644 Num true examples 36556\n",
      "  Batch 35,640  of  44,637.    Elapsed: 0:21:27. Training loss. 0.0038342312909662724 Num fake examples 34677 Num true examples 36603\n",
      "  Batch 35,680  of  44,637.    Elapsed: 0:21:28. Training loss. 0.0038709547370672226 Num fake examples 34721 Num true examples 36639\n",
      "  Batch 35,720  of  44,637.    Elapsed: 0:21:30. Training loss. 0.002631929237395525 Num fake examples 34753 Num true examples 36687\n",
      "  Batch 35,760  of  44,637.    Elapsed: 0:21:31. Training loss. 0.001970522804185748 Num fake examples 34800 Num true examples 36720\n",
      "  Batch 35,800  of  44,637.    Elapsed: 0:21:33. Training loss. 0.0050574857741594315 Num fake examples 34837 Num true examples 36763\n",
      "  Batch 35,840  of  44,637.    Elapsed: 0:21:34. Training loss. 0.00331155676394701 Num fake examples 34881 Num true examples 36799\n",
      "  Batch 35,880  of  44,637.    Elapsed: 0:21:36. Training loss. 0.003350646235048771 Num fake examples 34925 Num true examples 36835\n",
      "  Batch 35,920  of  44,637.    Elapsed: 0:21:37. Training loss. 2.935610771179199 Num fake examples 34959 Num true examples 36881\n",
      "  Batch 35,960  of  44,637.    Elapsed: 0:21:38. Training loss. 0.003314519766718149 Num fake examples 35002 Num true examples 36918\n",
      "  Batch 36,000  of  44,637.    Elapsed: 0:21:40. Training loss. 0.0029609315097332 Num fake examples 35039 Num true examples 36961\n",
      "  Batch 36,040  of  44,637.    Elapsed: 0:21:41. Training loss. 0.0027459077537059784 Num fake examples 35071 Num true examples 37009\n",
      "  Batch 36,080  of  44,637.    Elapsed: 0:21:43. Training loss. 0.004064809996634722 Num fake examples 35109 Num true examples 37051\n",
      "  Batch 36,120  of  44,637.    Elapsed: 0:21:44. Training loss. 0.004846454598009586 Num fake examples 35154 Num true examples 37086\n",
      "  Batch 36,160  of  44,637.    Elapsed: 0:21:46. Training loss. 0.0034512807615101337 Num fake examples 35197 Num true examples 37123\n",
      "  Batch 36,200  of  44,637.    Elapsed: 0:21:47. Training loss. 0.0027050164062529802 Num fake examples 35236 Num true examples 37164\n",
      "  Batch 36,240  of  44,637.    Elapsed: 0:21:49. Training loss. 0.003214445896446705 Num fake examples 35267 Num true examples 37213\n",
      "  Batch 36,280  of  44,637.    Elapsed: 0:21:50. Training loss. 0.002623053267598152 Num fake examples 35308 Num true examples 37252\n",
      "  Batch 36,320  of  44,637.    Elapsed: 0:21:52. Training loss. 0.001953724306076765 Num fake examples 35345 Num true examples 37295\n",
      "  Batch 36,360  of  44,637.    Elapsed: 0:21:53. Training loss. 0.0037344174925237894 Num fake examples 35378 Num true examples 37342\n",
      "  Batch 36,400  of  44,637.    Elapsed: 0:21:55. Training loss. 0.003925349563360214 Num fake examples 35419 Num true examples 37381\n",
      "  Batch 36,440  of  44,637.    Elapsed: 0:21:56. Training loss. 0.004763220436871052 Num fake examples 35465 Num true examples 37415\n",
      "  Batch 36,480  of  44,637.    Elapsed: 0:21:57. Training loss. 0.005054682493209839 Num fake examples 35504 Num true examples 37456\n",
      "  Batch 36,520  of  44,637.    Elapsed: 0:21:59. Training loss. 0.0051777977496385574 Num fake examples 35551 Num true examples 37489\n",
      "  Batch 36,560  of  44,637.    Elapsed: 0:22:00. Training loss. 0.003018489107489586 Num fake examples 35591 Num true examples 37529\n",
      "  Batch 36,600  of  44,637.    Elapsed: 0:22:02. Training loss. 0.0035903961397707462 Num fake examples 35628 Num true examples 37572\n",
      "  Batch 36,640  of  44,637.    Elapsed: 0:22:03. Training loss. 0.002723120152950287 Num fake examples 35668 Num true examples 37612\n",
      "  Batch 36,680  of  44,637.    Elapsed: 0:22:05. Training loss. 0.0037471014074981213 Num fake examples 35706 Num true examples 37654\n",
      "  Batch 36,720  of  44,637.    Elapsed: 0:22:06. Training loss. 0.004736781120300293 Num fake examples 35742 Num true examples 37698\n",
      "  Batch 36,760  of  44,637.    Elapsed: 0:22:08. Training loss. 0.003284851787611842 Num fake examples 35780 Num true examples 37740\n",
      "  Batch 36,800  of  44,637.    Elapsed: 0:22:09. Training loss. 0.004636557772755623 Num fake examples 35823 Num true examples 37777\n",
      "  Batch 36,840  of  44,637.    Elapsed: 0:22:11. Training loss. 2.8035972118377686 Num fake examples 35867 Num true examples 37813\n",
      "  Batch 36,880  of  44,637.    Elapsed: 0:22:12. Training loss. 2.6996238231658936 Num fake examples 35906 Num true examples 37854\n",
      "  Batch 36,920  of  44,637.    Elapsed: 0:22:14. Training loss. 0.003914210945367813 Num fake examples 35948 Num true examples 37892\n",
      "  Batch 36,960  of  44,637.    Elapsed: 0:22:15. Training loss. 0.0020181748550385237 Num fake examples 35986 Num true examples 37934\n",
      "  Batch 37,000  of  44,637.    Elapsed: 0:22:17. Training loss. 0.002909876871854067 Num fake examples 36028 Num true examples 37972\n",
      "  Batch 37,040  of  44,637.    Elapsed: 0:22:18. Training loss. 0.0021166158840060234 Num fake examples 36064 Num true examples 38016\n",
      "  Batch 37,080  of  44,637.    Elapsed: 0:22:20. Training loss. 0.0010550061706453562 Num fake examples 36108 Num true examples 38052\n",
      "  Batch 37,120  of  44,637.    Elapsed: 0:22:21. Training loss. 0.0022231093607842922 Num fake examples 36150 Num true examples 38090\n",
      "  Batch 37,160  of  44,637.    Elapsed: 0:22:23. Training loss. 0.0019652382470667362 Num fake examples 36185 Num true examples 38135\n",
      "  Batch 37,200  of  44,637.    Elapsed: 0:22:24. Training loss. 0.0015956477727741003 Num fake examples 36226 Num true examples 38174\n",
      "  Batch 37,240  of  44,637.    Elapsed: 0:22:26. Training loss. 0.0015529675874859095 Num fake examples 36264 Num true examples 38216\n",
      "  Batch 37,280  of  44,637.    Elapsed: 0:22:27. Training loss. 0.0038983719423413277 Num fake examples 36309 Num true examples 38251\n",
      "  Batch 37,320  of  44,637.    Elapsed: 0:22:29. Training loss. 0.0028774524107575417 Num fake examples 36343 Num true examples 38297\n",
      "  Batch 37,360  of  44,637.    Elapsed: 0:22:30. Training loss. 0.002398091834038496 Num fake examples 36385 Num true examples 38335\n",
      "  Batch 37,400  of  44,637.    Elapsed: 0:22:32. Training loss. 0.0014647250063717365 Num fake examples 36418 Num true examples 38382\n",
      "  Batch 37,440  of  44,637.    Elapsed: 0:22:33. Training loss. 0.0038385866209864616 Num fake examples 36459 Num true examples 38421\n",
      "  Batch 37,480  of  44,637.    Elapsed: 0:22:35. Training loss. 0.002248481847345829 Num fake examples 36500 Num true examples 38460\n",
      "  Batch 37,520  of  44,637.    Elapsed: 0:22:36. Training loss. 0.00283048115670681 Num fake examples 36545 Num true examples 38495\n",
      "  Batch 37,560  of  44,637.    Elapsed: 0:22:37. Training loss. 0.002865319140255451 Num fake examples 36585 Num true examples 38535\n",
      "  Batch 37,600  of  44,637.    Elapsed: 0:22:39. Training loss. 0.0030281231738626957 Num fake examples 36628 Num true examples 38572\n",
      "  Batch 37,640  of  44,637.    Elapsed: 0:22:40. Training loss. 0.0036882637068629265 Num fake examples 36664 Num true examples 38616\n",
      "  Batch 37,680  of  44,637.    Elapsed: 0:22:42. Training loss. 0.003960001282393932 Num fake examples 36697 Num true examples 38663\n",
      "  Batch 37,720  of  44,637.    Elapsed: 0:22:43. Training loss. 0.003917868714779615 Num fake examples 36735 Num true examples 38705\n",
      "  Batch 37,760  of  44,637.    Elapsed: 0:22:45. Training loss. 0.0026011865120381117 Num fake examples 36764 Num true examples 38756\n",
      "  Batch 37,800  of  44,637.    Elapsed: 0:22:46. Training loss. 0.002466588281095028 Num fake examples 36804 Num true examples 38796\n",
      "  Batch 37,840  of  44,637.    Elapsed: 0:22:48. Training loss. 0.0019910670816898346 Num fake examples 36842 Num true examples 38838\n",
      "  Batch 37,880  of  44,637.    Elapsed: 0:22:49. Training loss. 0.002329787937924266 Num fake examples 36872 Num true examples 38888\n",
      "  Batch 37,920  of  44,637.    Elapsed: 0:22:51. Training loss. 0.0027431054040789604 Num fake examples 36909 Num true examples 38931\n",
      "  Batch 37,960  of  44,637.    Elapsed: 0:22:52. Training loss. 0.004028595983982086 Num fake examples 36949 Num true examples 38971\n",
      "  Batch 38,000  of  44,637.    Elapsed: 0:22:53. Training loss. 0.0024457427207380533 Num fake examples 36989 Num true examples 39011\n",
      "  Batch 38,040  of  44,637.    Elapsed: 0:22:55. Training loss. 0.002098273253068328 Num fake examples 37028 Num true examples 39052\n",
      "  Batch 38,080  of  44,637.    Elapsed: 0:22:56. Training loss. 0.0028849272057414055 Num fake examples 37067 Num true examples 39093\n",
      "  Batch 38,120  of  44,637.    Elapsed: 0:22:58. Training loss. 3.0299408435821533 Num fake examples 37103 Num true examples 39137\n",
      "  Batch 38,160  of  44,637.    Elapsed: 0:22:59. Training loss. 0.003783724969252944 Num fake examples 37152 Num true examples 39168\n",
      "  Batch 38,200  of  44,637.    Elapsed: 0:23:01. Training loss. 0.0035511706955730915 Num fake examples 37187 Num true examples 39213\n",
      "  Batch 38,240  of  44,637.    Elapsed: 0:23:02. Training loss. 0.0037397819105535746 Num fake examples 37222 Num true examples 39258\n",
      "  Batch 38,280  of  44,637.    Elapsed: 0:23:04. Training loss. 0.002054044511169195 Num fake examples 37262 Num true examples 39298\n",
      "  Batch 38,320  of  44,637.    Elapsed: 0:23:05. Training loss. 0.003837892785668373 Num fake examples 37295 Num true examples 39345\n",
      "  Batch 38,360  of  44,637.    Elapsed: 0:23:07. Training loss. 0.004339865408837795 Num fake examples 37327 Num true examples 39393\n",
      "  Batch 38,400  of  44,637.    Elapsed: 0:23:08. Training loss. 0.0024306760169565678 Num fake examples 37364 Num true examples 39436\n",
      "  Batch 38,440  of  44,637.    Elapsed: 0:23:10. Training loss. 0.0015263804234564304 Num fake examples 37396 Num true examples 39484\n",
      "  Batch 38,480  of  44,637.    Elapsed: 0:23:11. Training loss. 0.003120109438896179 Num fake examples 37440 Num true examples 39520\n",
      "  Batch 38,520  of  44,637.    Elapsed: 0:23:12. Training loss. 0.0017312271520495415 Num fake examples 37485 Num true examples 39555\n",
      "  Batch 38,560  of  44,637.    Elapsed: 0:23:14. Training loss. 0.0018969650845974684 Num fake examples 37524 Num true examples 39596\n",
      "  Batch 38,600  of  44,637.    Elapsed: 0:23:15. Training loss. 0.002446659840643406 Num fake examples 37564 Num true examples 39636\n",
      "  Batch 38,640  of  44,637.    Elapsed: 0:23:17. Training loss. 0.002915660385042429 Num fake examples 37608 Num true examples 39672\n",
      "  Batch 38,680  of  44,637.    Elapsed: 0:23:18. Training loss. 0.0030253948643803596 Num fake examples 37648 Num true examples 39712\n",
      "  Batch 38,720  of  44,637.    Elapsed: 0:23:20. Training loss. 3.006098985671997 Num fake examples 37692 Num true examples 39748\n",
      "  Batch 38,760  of  44,637.    Elapsed: 0:23:21. Training loss. 0.0032789036631584167 Num fake examples 37727 Num true examples 39793\n",
      "  Batch 38,800  of  44,637.    Elapsed: 0:23:23. Training loss. 0.0043257251381874084 Num fake examples 37771 Num true examples 39829\n",
      "  Batch 38,840  of  44,637.    Elapsed: 0:23:24. Training loss. 0.0032271540258079767 Num fake examples 37818 Num true examples 39862\n",
      "  Batch 38,880  of  44,637.    Elapsed: 0:23:26. Training loss. 0.003017932642251253 Num fake examples 37852 Num true examples 39908\n",
      "  Batch 38,920  of  44,637.    Elapsed: 0:23:27. Training loss. 0.0020417869091033936 Num fake examples 37886 Num true examples 39954\n",
      "  Batch 38,960  of  44,637.    Elapsed: 0:23:29. Training loss. 0.004525217227637768 Num fake examples 37925 Num true examples 39995\n",
      "  Batch 39,000  of  44,637.    Elapsed: 0:23:30. Training loss. 0.005521299783140421 Num fake examples 37970 Num true examples 40030\n",
      "  Batch 39,040  of  44,637.    Elapsed: 0:23:31. Training loss. 0.003327284473925829 Num fake examples 38013 Num true examples 40067\n",
      "  Batch 39,080  of  44,637.    Elapsed: 0:23:33. Training loss. 0.004137811250984669 Num fake examples 38045 Num true examples 40115\n",
      "  Batch 39,120  of  44,637.    Elapsed: 0:23:34. Training loss. 0.0024301880039274693 Num fake examples 38079 Num true examples 40161\n",
      "  Batch 39,160  of  44,637.    Elapsed: 0:23:36. Training loss. 0.0028780000284314156 Num fake examples 38121 Num true examples 40199\n",
      "  Batch 39,200  of  44,637.    Elapsed: 0:23:37. Training loss. 0.0041894735768437386 Num fake examples 38167 Num true examples 40233\n",
      "  Batch 39,240  of  44,637.    Elapsed: 0:23:39. Training loss. 0.0036786613054573536 Num fake examples 38205 Num true examples 40275\n",
      "  Batch 39,280  of  44,637.    Elapsed: 0:23:40. Training loss. 0.003194791032001376 Num fake examples 38237 Num true examples 40323\n",
      "  Batch 39,320  of  44,637.    Elapsed: 0:23:42. Training loss. 0.0037089595571160316 Num fake examples 38275 Num true examples 40365\n",
      "  Batch 39,360  of  44,637.    Elapsed: 0:23:43. Training loss. 0.004850303754210472 Num fake examples 38310 Num true examples 40410\n",
      "  Batch 39,400  of  44,637.    Elapsed: 0:23:45. Training loss. 0.0035624955780804157 Num fake examples 38339 Num true examples 40461\n",
      "  Batch 39,440  of  44,637.    Elapsed: 0:23:46. Training loss. 0.003395841456949711 Num fake examples 38385 Num true examples 40495\n",
      "  Batch 39,480  of  44,637.    Elapsed: 0:23:48. Training loss. 0.0026443602982908487 Num fake examples 38422 Num true examples 40538\n",
      "  Batch 39,520  of  44,637.    Elapsed: 0:23:49. Training loss. 0.002643986139446497 Num fake examples 38465 Num true examples 40575\n",
      "  Batch 39,560  of  44,637.    Elapsed: 0:23:50. Training loss. 0.0022012577392160892 Num fake examples 38499 Num true examples 40621\n",
      "  Batch 39,600  of  44,637.    Elapsed: 0:23:52. Training loss. 0.0019342624582350254 Num fake examples 38536 Num true examples 40664\n",
      "  Batch 39,640  of  44,637.    Elapsed: 0:23:53. Training loss. 0.0018071980448439717 Num fake examples 38577 Num true examples 40703\n",
      "  Batch 39,680  of  44,637.    Elapsed: 0:23:55. Training loss. 0.002247184980660677 Num fake examples 38616 Num true examples 40744\n",
      "  Batch 39,720  of  44,637.    Elapsed: 0:23:56. Training loss. 0.0026429775170981884 Num fake examples 38665 Num true examples 40775\n",
      "  Batch 39,760  of  44,637.    Elapsed: 0:23:58. Training loss. 0.002566876355558634 Num fake examples 38706 Num true examples 40814\n",
      "  Batch 39,800  of  44,637.    Elapsed: 0:23:59. Training loss. 0.0028680656105279922 Num fake examples 38750 Num true examples 40850\n",
      "  Batch 39,840  of  44,637.    Elapsed: 0:24:01. Training loss. 0.002080455655232072 Num fake examples 38794 Num true examples 40886\n",
      "  Batch 39,880  of  44,637.    Elapsed: 0:24:02. Training loss. 0.0032427257392555475 Num fake examples 38831 Num true examples 40929\n",
      "  Batch 39,920  of  44,637.    Elapsed: 0:24:04. Training loss. 0.0035321032628417015 Num fake examples 38872 Num true examples 40968\n",
      "  Batch 39,960  of  44,637.    Elapsed: 0:24:05. Training loss. 0.002468870719894767 Num fake examples 38909 Num true examples 41011\n",
      "  Batch 40,000  of  44,637.    Elapsed: 0:24:07. Training loss. 0.0038626138120889664 Num fake examples 38947 Num true examples 41053\n",
      "  Batch 40,040  of  44,637.    Elapsed: 0:24:08. Training loss. 0.003488148795440793 Num fake examples 38984 Num true examples 41096\n",
      "  Batch 40,080  of  44,637.    Elapsed: 0:24:09. Training loss. 0.0023127091117203236 Num fake examples 39025 Num true examples 41135\n",
      "  Batch 40,120  of  44,637.    Elapsed: 0:24:11. Training loss. 0.002607803326100111 Num fake examples 39072 Num true examples 41168\n",
      "  Batch 40,160  of  44,637.    Elapsed: 0:24:12. Training loss. 0.0028405897319316864 Num fake examples 39117 Num true examples 41203\n",
      "  Batch 40,200  of  44,637.    Elapsed: 0:24:14. Training loss. 0.0023894095793366432 Num fake examples 39155 Num true examples 41245\n",
      "  Batch 40,240  of  44,637.    Elapsed: 0:24:15. Training loss. 2.8434054851531982 Num fake examples 39197 Num true examples 41283\n",
      "  Batch 40,280  of  44,637.    Elapsed: 0:24:17. Training loss. 0.003081635572016239 Num fake examples 39237 Num true examples 41323\n",
      "  Batch 40,320  of  44,637.    Elapsed: 0:24:18. Training loss. 0.001683293143287301 Num fake examples 39269 Num true examples 41371\n",
      "  Batch 40,360  of  44,637.    Elapsed: 0:24:20. Training loss. 0.005639398470520973 Num fake examples 39310 Num true examples 41410\n",
      "  Batch 40,400  of  44,637.    Elapsed: 0:24:21. Training loss. 0.0033935275860130787 Num fake examples 39361 Num true examples 41439\n",
      "  Batch 40,440  of  44,637.    Elapsed: 0:24:23. Training loss. 3.022498369216919 Num fake examples 39407 Num true examples 41473\n",
      "  Batch 40,480  of  44,637.    Elapsed: 0:24:24. Training loss. 0.004149873740971088 Num fake examples 39444 Num true examples 41516\n",
      "  Batch 40,520  of  44,637.    Elapsed: 0:24:26. Training loss. 0.0029065555427223444 Num fake examples 39488 Num true examples 41552\n",
      "  Batch 40,560  of  44,637.    Elapsed: 0:24:27. Training loss. 0.003029992338269949 Num fake examples 39532 Num true examples 41588\n",
      "  Batch 40,600  of  44,637.    Elapsed: 0:24:28. Training loss. 0.00150150409899652 Num fake examples 39573 Num true examples 41627\n",
      "  Batch 40,640  of  44,637.    Elapsed: 0:24:30. Training loss. 0.002977126743644476 Num fake examples 39615 Num true examples 41665\n",
      "  Batch 40,680  of  44,637.    Elapsed: 0:24:31. Training loss. 0.003472533542662859 Num fake examples 39655 Num true examples 41705\n",
      "  Batch 40,720  of  44,637.    Elapsed: 0:24:33. Training loss. 0.0018765489803627133 Num fake examples 39696 Num true examples 41744\n",
      "  Batch 40,760  of  44,637.    Elapsed: 0:24:34. Training loss. 0.0032850864809006453 Num fake examples 39738 Num true examples 41782\n",
      "  Batch 40,800  of  44,637.    Elapsed: 0:24:36. Training loss. 0.002770355436950922 Num fake examples 39777 Num true examples 41823\n",
      "  Batch 40,840  of  44,637.    Elapsed: 0:24:37. Training loss. 0.0022323066368699074 Num fake examples 39816 Num true examples 41864\n",
      "  Batch 40,880  of  44,637.    Elapsed: 0:24:39. Training loss. 0.0026882896199822426 Num fake examples 39862 Num true examples 41898\n",
      "  Batch 40,920  of  44,637.    Elapsed: 0:24:40. Training loss. 0.003298406954854727 Num fake examples 39909 Num true examples 41931\n",
      "  Batch 40,960  of  44,637.    Elapsed: 0:24:42. Training loss. 0.0029526525177061558 Num fake examples 39948 Num true examples 41972\n",
      "  Batch 41,000  of  44,637.    Elapsed: 0:24:43. Training loss. 0.001703751040622592 Num fake examples 39984 Num true examples 42016\n",
      "  Batch 41,040  of  44,637.    Elapsed: 0:24:44. Training loss. 0.0019428577506914735 Num fake examples 40026 Num true examples 42054\n",
      "  Batch 41,080  of  44,637.    Elapsed: 0:24:46. Training loss. 0.003073049709200859 Num fake examples 40065 Num true examples 42095\n",
      "  Batch 41,120  of  44,637.    Elapsed: 0:24:47. Training loss. 0.0024642639327794313 Num fake examples 40111 Num true examples 42129\n",
      "  Batch 41,160  of  44,637.    Elapsed: 0:24:49. Training loss. 0.003403240814805031 Num fake examples 40157 Num true examples 42163\n",
      "  Batch 41,200  of  44,637.    Elapsed: 0:24:50. Training loss. 0.003835769137367606 Num fake examples 40195 Num true examples 42205\n",
      "  Batch 41,240  of  44,637.    Elapsed: 0:24:52. Training loss. 0.003926826640963554 Num fake examples 40229 Num true examples 42251\n",
      "  Batch 41,280  of  44,637.    Elapsed: 0:24:53. Training loss. 0.0027873036451637745 Num fake examples 40267 Num true examples 42293\n",
      "  Batch 41,320  of  44,637.    Elapsed: 0:24:55. Training loss. 0.0020674285478889942 Num fake examples 40307 Num true examples 42333\n",
      "  Batch 41,360  of  44,637.    Elapsed: 0:24:56. Training loss. 0.0019148518331348896 Num fake examples 40348 Num true examples 42372\n",
      "  Batch 41,400  of  44,637.    Elapsed: 0:24:58. Training loss. 0.002049689181149006 Num fake examples 40386 Num true examples 42414\n",
      "  Batch 41,440  of  44,637.    Elapsed: 0:24:59. Training loss. 0.0027766828425228596 Num fake examples 40421 Num true examples 42459\n",
      "  Batch 41,480  of  44,637.    Elapsed: 0:25:01. Training loss. 0.0021927542984485626 Num fake examples 40457 Num true examples 42503\n",
      "  Batch 41,520  of  44,637.    Elapsed: 0:25:02. Training loss. 0.0024623782373964787 Num fake examples 40487 Num true examples 42553\n",
      "  Batch 41,560  of  44,637.    Elapsed: 0:25:03. Training loss. 2.825193166732788 Num fake examples 40523 Num true examples 42597\n",
      "  Batch 41,600  of  44,637.    Elapsed: 0:25:05. Training loss. 0.0024935666006058455 Num fake examples 40564 Num true examples 42636\n",
      "  Batch 41,640  of  44,637.    Elapsed: 0:25:06. Training loss. 0.0025376416742801666 Num fake examples 40601 Num true examples 42679\n",
      "  Batch 41,680  of  44,637.    Elapsed: 0:25:08. Training loss. 0.003630234394222498 Num fake examples 40633 Num true examples 42727\n",
      "  Batch 41,720  of  44,637.    Elapsed: 0:25:09. Training loss. 0.004404550883919001 Num fake examples 40672 Num true examples 42768\n",
      "  Batch 41,760  of  44,637.    Elapsed: 0:25:11. Training loss. 0.0023374762386083603 Num fake examples 40709 Num true examples 42811\n",
      "  Batch 41,800  of  44,637.    Elapsed: 0:25:12. Training loss. 0.0015699140494689345 Num fake examples 40743 Num true examples 42857\n",
      "  Batch 41,840  of  44,637.    Elapsed: 0:25:14. Training loss. 0.002513066167011857 Num fake examples 40780 Num true examples 42900\n",
      "  Batch 41,880  of  44,637.    Elapsed: 0:25:15. Training loss. 0.002664368599653244 Num fake examples 40816 Num true examples 42944\n",
      "  Batch 41,920  of  44,637.    Elapsed: 0:25:17. Training loss. 0.0013063133228570223 Num fake examples 40860 Num true examples 42980\n",
      "  Batch 41,960  of  44,637.    Elapsed: 0:25:18. Training loss. 0.002707013161852956 Num fake examples 40898 Num true examples 43022\n",
      "  Batch 42,000  of  44,637.    Elapsed: 0:25:20. Training loss. 0.004075929522514343 Num fake examples 40938 Num true examples 43062\n",
      "  Batch 42,040  of  44,637.    Elapsed: 0:25:21. Training loss. 0.0021147222723811865 Num fake examples 40973 Num true examples 43107\n",
      "  Batch 42,080  of  44,637.    Elapsed: 0:25:22. Training loss. 3.0072522163391113 Num fake examples 41013 Num true examples 43147\n",
      "  Batch 42,120  of  44,637.    Elapsed: 0:25:24. Training loss. 0.003143100533634424 Num fake examples 41049 Num true examples 43191\n",
      "  Batch 42,160  of  44,637.    Elapsed: 0:25:25. Training loss. 0.0036507397890090942 Num fake examples 41091 Num true examples 43229\n",
      "  Batch 42,200  of  44,637.    Elapsed: 0:25:27. Training loss. 0.004315138328820467 Num fake examples 41127 Num true examples 43273\n",
      "  Batch 42,240  of  44,637.    Elapsed: 0:25:28. Training loss. 0.002965461928397417 Num fake examples 41162 Num true examples 43318\n",
      "  Batch 42,280  of  44,637.    Elapsed: 0:25:30. Training loss. 0.004525135271251202 Num fake examples 41205 Num true examples 43355\n",
      "  Batch 42,320  of  44,637.    Elapsed: 0:25:31. Training loss. 2.6936392784118652 Num fake examples 41248 Num true examples 43392\n",
      "  Batch 42,360  of  44,637.    Elapsed: 0:25:33. Training loss. 0.002218095352873206 Num fake examples 41289 Num true examples 43431\n",
      "  Batch 42,400  of  44,637.    Elapsed: 0:25:34. Training loss. 0.003639386733993888 Num fake examples 41326 Num true examples 43474\n",
      "  Batch 42,440  of  44,637.    Elapsed: 0:25:36. Training loss. 0.0037185908295214176 Num fake examples 41354 Num true examples 43526\n",
      "  Batch 42,480  of  44,637.    Elapsed: 0:25:37. Training loss. 0.0030375239439308643 Num fake examples 41395 Num true examples 43565\n",
      "  Batch 42,520  of  44,637.    Elapsed: 0:25:39. Training loss. 0.003506622277200222 Num fake examples 41435 Num true examples 43605\n",
      "  Batch 42,560  of  44,637.    Elapsed: 0:25:40. Training loss. 0.0028168708086013794 Num fake examples 41470 Num true examples 43650\n",
      "  Batch 42,600  of  44,637.    Elapsed: 0:25:41. Training loss. 0.0022871233522892 Num fake examples 41507 Num true examples 43693\n",
      "  Batch 42,640  of  44,637.    Elapsed: 0:25:43. Training loss. 0.0021350111346691847 Num fake examples 41548 Num true examples 43732\n",
      "  Batch 42,680  of  44,637.    Elapsed: 0:25:44. Training loss. 0.0021445048041641712 Num fake examples 41591 Num true examples 43769\n",
      "  Batch 42,720  of  44,637.    Elapsed: 0:25:46. Training loss. 0.001683985348790884 Num fake examples 41626 Num true examples 43814\n",
      "  Batch 42,760  of  44,637.    Elapsed: 0:25:47. Training loss. 0.0016994763864204288 Num fake examples 41663 Num true examples 43857\n",
      "  Batch 42,800  of  44,637.    Elapsed: 0:25:49. Training loss. 0.0018047146731987596 Num fake examples 41700 Num true examples 43900\n",
      "  Batch 42,840  of  44,637.    Elapsed: 0:25:50. Training loss. 0.0024862228892743587 Num fake examples 41744 Num true examples 43936\n",
      "  Batch 42,880  of  44,637.    Elapsed: 0:25:52. Training loss. 0.002060576342046261 Num fake examples 41782 Num true examples 43978\n",
      "  Batch 42,920  of  44,637.    Elapsed: 0:25:53. Training loss. 0.002244139090180397 Num fake examples 41825 Num true examples 44015\n",
      "  Batch 42,960  of  44,637.    Elapsed: 0:25:55. Training loss. 0.0030800586100667715 Num fake examples 41861 Num true examples 44059\n",
      "  Batch 43,000  of  44,637.    Elapsed: 0:25:56. Training loss. 0.0035061268135905266 Num fake examples 41900 Num true examples 44100\n",
      "  Batch 43,040  of  44,637.    Elapsed: 0:25:58. Training loss. 0.004437570460140705 Num fake examples 41937 Num true examples 44143\n",
      "  Batch 43,080  of  44,637.    Elapsed: 0:25:59. Training loss. 0.0036315785255283117 Num fake examples 41970 Num true examples 44190\n",
      "  Batch 43,120  of  44,637.    Elapsed: 0:26:01. Training loss. 0.003943982999771833 Num fake examples 42009 Num true examples 44231\n",
      "  Batch 43,160  of  44,637.    Elapsed: 0:26:02. Training loss. 0.0036629640962928534 Num fake examples 42045 Num true examples 44275\n",
      "  Batch 43,200  of  44,637.    Elapsed: 0:26:03. Training loss. 0.0034308384638279676 Num fake examples 42085 Num true examples 44315\n",
      "  Batch 43,240  of  44,637.    Elapsed: 0:26:05. Training loss. 0.0034029255621135235 Num fake examples 42126 Num true examples 44354\n",
      "  Batch 43,280  of  44,637.    Elapsed: 0:26:06. Training loss. 3.0722591876983643 Num fake examples 42165 Num true examples 44395\n",
      "  Batch 43,320  of  44,637.    Elapsed: 0:26:08. Training loss. 0.0033748168498277664 Num fake examples 42200 Num true examples 44440\n",
      "  Batch 43,360  of  44,637.    Elapsed: 0:26:09. Training loss. 3.012667179107666 Num fake examples 42244 Num true examples 44476\n",
      "  Batch 43,400  of  44,637.    Elapsed: 0:26:11. Training loss. 0.003861806821078062 Num fake examples 42282 Num true examples 44518\n",
      "  Batch 43,440  of  44,637.    Elapsed: 0:26:12. Training loss. 0.002916719764471054 Num fake examples 42323 Num true examples 44557\n",
      "  Batch 43,480  of  44,637.    Elapsed: 0:26:14. Training loss. 0.002513027284294367 Num fake examples 42359 Num true examples 44601\n",
      "  Batch 43,520  of  44,637.    Elapsed: 0:26:15. Training loss. 0.0051164813339710236 Num fake examples 42394 Num true examples 44646\n",
      "  Batch 43,560  of  44,637.    Elapsed: 0:26:17. Training loss. 0.00336964288726449 Num fake examples 42430 Num true examples 44690\n",
      "  Batch 43,600  of  44,637.    Elapsed: 0:26:18. Training loss. 0.004155668895691633 Num fake examples 42471 Num true examples 44729\n",
      "  Batch 43,640  of  44,637.    Elapsed: 0:26:20. Training loss. 0.004244400188326836 Num fake examples 42509 Num true examples 44771\n",
      "  Batch 43,680  of  44,637.    Elapsed: 0:26:21. Training loss. 0.004755809903144836 Num fake examples 42549 Num true examples 44811\n",
      "  Batch 43,720  of  44,637.    Elapsed: 0:26:22. Training loss. 0.0037662957329303026 Num fake examples 42587 Num true examples 44853\n",
      "  Batch 43,760  of  44,637.    Elapsed: 0:26:24. Training loss. 0.0049711004830896854 Num fake examples 42631 Num true examples 44889\n",
      "  Batch 43,800  of  44,637.    Elapsed: 0:26:25. Training loss. 0.003259780816733837 Num fake examples 42675 Num true examples 44925\n",
      "  Batch 43,840  of  44,637.    Elapsed: 0:26:27. Training loss. 0.0038891504518687725 Num fake examples 42704 Num true examples 44976\n",
      "  Batch 43,880  of  44,637.    Elapsed: 0:26:28. Training loss. 0.0029003582894802094 Num fake examples 42745 Num true examples 45015\n",
      "  Batch 43,920  of  44,637.    Elapsed: 0:26:30. Training loss. 0.0033327043056488037 Num fake examples 42781 Num true examples 45059\n",
      "  Batch 43,960  of  44,637.    Elapsed: 0:26:31. Training loss. 0.001967160264030099 Num fake examples 42825 Num true examples 45095\n",
      "  Batch 44,000  of  44,637.    Elapsed: 0:26:33. Training loss. 0.0020832368172705173 Num fake examples 42872 Num true examples 45128\n",
      "  Batch 44,040  of  44,637.    Elapsed: 0:26:34. Training loss. 0.0038923120591789484 Num fake examples 42918 Num true examples 45162\n",
      "  Batch 44,080  of  44,637.    Elapsed: 0:26:36. Training loss. 0.0023465193808078766 Num fake examples 42952 Num true examples 45208\n",
      "  Batch 44,120  of  44,637.    Elapsed: 0:26:37. Training loss. 0.004305645357817411 Num fake examples 42990 Num true examples 45250\n",
      "  Batch 44,160  of  44,637.    Elapsed: 0:26:38. Training loss. 0.003065940225496888 Num fake examples 43022 Num true examples 45298\n",
      "  Batch 44,200  of  44,637.    Elapsed: 0:26:40. Training loss. 0.005124768242239952 Num fake examples 43056 Num true examples 45344\n",
      "  Batch 44,240  of  44,637.    Elapsed: 0:26:41. Training loss. 0.002916408237069845 Num fake examples 43092 Num true examples 45388\n",
      "  Batch 44,280  of  44,637.    Elapsed: 0:26:43. Training loss. 0.0031087398529052734 Num fake examples 43129 Num true examples 45431\n",
      "  Batch 44,320  of  44,637.    Elapsed: 0:26:44. Training loss. 0.004176462069153786 Num fake examples 43168 Num true examples 45472\n",
      "  Batch 44,360  of  44,637.    Elapsed: 0:26:46. Training loss. 0.0039020429830998182 Num fake examples 43209 Num true examples 45511\n",
      "  Batch 44,400  of  44,637.    Elapsed: 0:26:47. Training loss. 0.003690286073833704 Num fake examples 43253 Num true examples 45547\n",
      "  Batch 44,440  of  44,637.    Elapsed: 0:26:49. Training loss. 0.003084097057580948 Num fake examples 43291 Num true examples 45589\n",
      "  Batch 44,480  of  44,637.    Elapsed: 0:26:50. Training loss. 0.00603263545781374 Num fake examples 43330 Num true examples 45630\n",
      "  Batch 44,520  of  44,637.    Elapsed: 0:26:52. Training loss. 0.005577540025115013 Num fake examples 43371 Num true examples 45669\n",
      "  Batch 44,560  of  44,637.    Elapsed: 0:26:53. Training loss. 0.0027558510191738605 Num fake examples 43406 Num true examples 45714\n",
      "  Batch 44,600  of  44,637.    Elapsed: 0:26:55. Training loss. 0.0034229608718305826 Num fake examples 43445 Num true examples 45755\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epcoh took: 0:26:56\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation Loss: 0.18\n",
      "  Validation took: 0:01:40\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  44,637.    Elapsed: 0:00:01. Training loss. 0.0022324607707560062 Num fake examples 42 Num true examples 38\n",
      "  Batch    80  of  44,637.    Elapsed: 0:00:03. Training loss. 2.9053361415863037 Num fake examples 88 Num true examples 72\n",
      "  Batch   120  of  44,637.    Elapsed: 0:00:05. Training loss. 0.002867588307708502 Num fake examples 130 Num true examples 110\n",
      "  Batch   160  of  44,637.    Elapsed: 0:00:06. Training loss. 0.004122263286262751 Num fake examples 173 Num true examples 147\n",
      "  Batch   200  of  44,637.    Elapsed: 0:00:08. Training loss. 2.9920432567596436 Num fake examples 208 Num true examples 192\n",
      "  Batch   240  of  44,637.    Elapsed: 0:00:09. Training loss. 0.0029462622478604317 Num fake examples 252 Num true examples 228\n",
      "  Batch   280  of  44,637.    Elapsed: 0:00:11. Training loss. 0.0025126300752162933 Num fake examples 300 Num true examples 260\n",
      "  Batch   320  of  44,637.    Elapsed: 0:00:12. Training loss. 0.002256997162476182 Num fake examples 345 Num true examples 295\n",
      "  Batch   360  of  44,637.    Elapsed: 0:00:14. Training loss. 0.0015924721956253052 Num fake examples 385 Num true examples 335\n",
      "  Batch   400  of  44,637.    Elapsed: 0:00:15. Training loss. 0.0023392231669276953 Num fake examples 411 Num true examples 389\n",
      "  Batch   440  of  44,637.    Elapsed: 0:00:17. Training loss. 0.0023554880172014236 Num fake examples 463 Num true examples 417\n",
      "  Batch   480  of  44,637.    Elapsed: 0:00:18. Training loss. 0.0047309971414506435 Num fake examples 506 Num true examples 454\n",
      "  Batch   520  of  44,637.    Elapsed: 0:00:20. Training loss. 0.0020466730929911137 Num fake examples 548 Num true examples 492\n",
      "  Batch   560  of  44,637.    Elapsed: 0:00:21. Training loss. 0.004975828342139721 Num fake examples 587 Num true examples 533\n",
      "  Batch   600  of  44,637.    Elapsed: 0:00:23. Training loss. 0.005214858800172806 Num fake examples 625 Num true examples 575\n",
      "  Batch   640  of  44,637.    Elapsed: 0:00:24. Training loss. 0.0023526251316070557 Num fake examples 664 Num true examples 616\n",
      "  Batch   680  of  44,637.    Elapsed: 0:00:26. Training loss. 0.003254751907661557 Num fake examples 698 Num true examples 662\n",
      "  Batch   720  of  44,637.    Elapsed: 0:00:27. Training loss. 0.002139997435733676 Num fake examples 735 Num true examples 705\n",
      "  Batch   760  of  44,637.    Elapsed: 0:00:29. Training loss. 0.004191803280264139 Num fake examples 768 Num true examples 752\n",
      "  Batch   800  of  44,637.    Elapsed: 0:00:30. Training loss. 0.005922374315559864 Num fake examples 808 Num true examples 792\n",
      "  Batch   840  of  44,637.    Elapsed: 0:00:32. Training loss. 2.7866687774658203 Num fake examples 846 Num true examples 834\n",
      "  Batch   880  of  44,637.    Elapsed: 0:00:33. Training loss. 0.006874587386846542 Num fake examples 884 Num true examples 876\n",
      "  Batch   920  of  44,637.    Elapsed: 0:00:35. Training loss. 0.005347276106476784 Num fake examples 928 Num true examples 912\n",
      "  Batch   960  of  44,637.    Elapsed: 0:00:36. Training loss. 0.0057761454954743385 Num fake examples 962 Num true examples 958\n",
      "  Batch 1,000  of  44,637.    Elapsed: 0:00:38. Training loss. 0.005947839468717575 Num fake examples 1004 Num true examples 996\n",
      "  Batch 1,040  of  44,637.    Elapsed: 0:00:39. Training loss. 0.0035496316850185394 Num fake examples 1045 Num true examples 1035\n",
      "  Batch 1,080  of  44,637.    Elapsed: 0:00:41. Training loss. 0.00463084876537323 Num fake examples 1079 Num true examples 1081\n",
      "  Batch 1,120  of  44,637.    Elapsed: 0:00:42. Training loss. 0.004681487567722797 Num fake examples 1120 Num true examples 1120\n",
      "  Batch 1,160  of  44,637.    Elapsed: 0:00:44. Training loss. 0.004513233434408903 Num fake examples 1166 Num true examples 1154\n",
      "  Batch 1,200  of  44,637.    Elapsed: 0:00:45. Training loss. 0.0030928172636777163 Num fake examples 1205 Num true examples 1195\n",
      "  Batch 1,240  of  44,637.    Elapsed: 0:00:47. Training loss. 0.002776167821139097 Num fake examples 1239 Num true examples 1241\n",
      "  Batch 1,280  of  44,637.    Elapsed: 0:00:48. Training loss. 0.0024509653449058533 Num fake examples 1282 Num true examples 1278\n",
      "  Batch 1,320  of  44,637.    Elapsed: 0:00:50. Training loss. 0.0021588322706520557 Num fake examples 1315 Num true examples 1325\n",
      "  Batch 1,360  of  44,637.    Elapsed: 0:00:51. Training loss. 0.002976823365315795 Num fake examples 1352 Num true examples 1368\n",
      "  Batch 1,400  of  44,637.    Elapsed: 0:00:53. Training loss. 0.0011835112236440182 Num fake examples 1388 Num true examples 1412\n",
      "  Batch 1,440  of  44,637.    Elapsed: 0:00:54. Training loss. 0.0015756248030811548 Num fake examples 1419 Num true examples 1461\n",
      "  Batch 1,480  of  44,637.    Elapsed: 0:00:56. Training loss. 0.0027348799630999565 Num fake examples 1453 Num true examples 1507\n",
      "  Batch 1,520  of  44,637.    Elapsed: 0:00:57. Training loss. 0.004086350090801716 Num fake examples 1485 Num true examples 1555\n",
      "  Batch 1,560  of  44,637.    Elapsed: 0:00:59. Training loss. 0.0014727226225659251 Num fake examples 1525 Num true examples 1595\n",
      "  Batch 1,600  of  44,637.    Elapsed: 0:01:00. Training loss. 2.63962984085083 Num fake examples 1564 Num true examples 1636\n",
      "  Batch 1,640  of  44,637.    Elapsed: 0:01:02. Training loss. 0.002109672175720334 Num fake examples 1605 Num true examples 1675\n",
      "  Batch 1,680  of  44,637.    Elapsed: 0:01:03. Training loss. 0.0036057666875422 Num fake examples 1648 Num true examples 1712\n",
      "  Batch 1,720  of  44,637.    Elapsed: 0:01:05. Training loss. 0.00526406429708004 Num fake examples 1688 Num true examples 1752\n",
      "  Batch 1,760  of  44,637.    Elapsed: 0:01:06. Training loss. 0.004045388661324978 Num fake examples 1735 Num true examples 1785\n",
      "  Batch 1,800  of  44,637.    Elapsed: 0:01:08. Training loss. 0.006263640243560076 Num fake examples 1771 Num true examples 1829\n",
      "  Batch 1,840  of  44,637.    Elapsed: 0:01:09. Training loss. 0.00428537093102932 Num fake examples 1806 Num true examples 1874\n",
      "  Batch 1,880  of  44,637.    Elapsed: 0:01:11. Training loss. 0.003911970183253288 Num fake examples 1843 Num true examples 1917\n",
      "  Batch 1,920  of  44,637.    Elapsed: 0:01:12. Training loss. 0.004941843915730715 Num fake examples 1883 Num true examples 1957\n",
      "  Batch 1,960  of  44,637.    Elapsed: 0:01:14. Training loss. 0.004201808944344521 Num fake examples 1918 Num true examples 2002\n",
      "  Batch 2,000  of  44,637.    Elapsed: 0:01:15. Training loss. 0.0022132443264126778 Num fake examples 1957 Num true examples 2043\n",
      "  Batch 2,040  of  44,637.    Elapsed: 0:01:17. Training loss. 0.0027467338368296623 Num fake examples 1992 Num true examples 2088\n",
      "  Batch 2,080  of  44,637.    Elapsed: 0:01:18. Training loss. 0.002535853534936905 Num fake examples 2030 Num true examples 2130\n",
      "  Batch 2,120  of  44,637.    Elapsed: 0:01:20. Training loss. 0.002806528937071562 Num fake examples 2073 Num true examples 2167\n",
      "  Batch 2,160  of  44,637.    Elapsed: 0:01:22. Training loss. 0.0027120686136186123 Num fake examples 2118 Num true examples 2202\n",
      "  Batch 2,200  of  44,637.    Elapsed: 0:01:23. Training loss. 0.0025185169652104378 Num fake examples 2153 Num true examples 2247\n",
      "  Batch 2,240  of  44,637.    Elapsed: 0:01:25. Training loss. 0.002840456087142229 Num fake examples 2202 Num true examples 2278\n",
      "  Batch 2,280  of  44,637.    Elapsed: 0:01:26. Training loss. 0.0028545111417770386 Num fake examples 2244 Num true examples 2316\n",
      "  Batch 2,320  of  44,637.    Elapsed: 0:01:28. Training loss. 0.0027138928417116404 Num fake examples 2285 Num true examples 2355\n",
      "  Batch 2,360  of  44,637.    Elapsed: 0:01:29. Training loss. 0.0020106022711843252 Num fake examples 2318 Num true examples 2402\n",
      "  Batch 2,400  of  44,637.    Elapsed: 0:01:31. Training loss. 0.0028434041887521744 Num fake examples 2354 Num true examples 2446\n",
      "  Batch 2,440  of  44,637.    Elapsed: 0:01:32. Training loss. 0.003065485041588545 Num fake examples 2399 Num true examples 2481\n",
      "  Batch 2,480  of  44,637.    Elapsed: 0:01:34. Training loss. 0.00460806442424655 Num fake examples 2442 Num true examples 2518\n",
      "  Batch 2,520  of  44,637.    Elapsed: 0:01:35. Training loss. 0.004745346959680319 Num fake examples 2482 Num true examples 2558\n",
      "  Batch 2,560  of  44,637.    Elapsed: 0:01:37. Training loss. 2.742943525314331 Num fake examples 2520 Num true examples 2600\n",
      "  Batch 2,600  of  44,637.    Elapsed: 0:01:38. Training loss. 0.0030159016605466604 Num fake examples 2563 Num true examples 2637\n",
      "  Batch 2,640  of  44,637.    Elapsed: 0:01:40. Training loss. 0.0035355607978999615 Num fake examples 2594 Num true examples 2686\n",
      "  Batch 2,680  of  44,637.    Elapsed: 0:01:41. Training loss. 0.0026721940375864506 Num fake examples 2637 Num true examples 2723\n",
      "  Batch 2,720  of  44,637.    Elapsed: 0:01:43. Training loss. 0.005796028301119804 Num fake examples 2677 Num true examples 2763\n",
      "  Batch 2,760  of  44,637.    Elapsed: 0:01:44. Training loss. 0.0060529522597789764 Num fake examples 2716 Num true examples 2804\n",
      "  Batch 2,800  of  44,637.    Elapsed: 0:01:46. Training loss. 0.006799993105232716 Num fake examples 2750 Num true examples 2850\n",
      "  Batch 2,840  of  44,637.    Elapsed: 0:01:47. Training loss. 0.005122543312609196 Num fake examples 2793 Num true examples 2887\n",
      "  Batch 2,880  of  44,637.    Elapsed: 0:01:49. Training loss. 0.004100176505744457 Num fake examples 2835 Num true examples 2925\n",
      "  Batch 2,920  of  44,637.    Elapsed: 0:01:50. Training loss. 0.0031398057471960783 Num fake examples 2868 Num true examples 2972\n",
      "  Batch 2,960  of  44,637.    Elapsed: 0:01:52. Training loss. 0.005453730002045631 Num fake examples 2907 Num true examples 3013\n",
      "  Batch 3,000  of  44,637.    Elapsed: 0:01:53. Training loss. 0.003597210394218564 Num fake examples 2945 Num true examples 3055\n",
      "  Batch 3,040  of  44,637.    Elapsed: 0:01:55. Training loss. 0.0015180495101958513 Num fake examples 2979 Num true examples 3101\n",
      "  Batch 3,080  of  44,637.    Elapsed: 0:01:56. Training loss. 0.0014299358008429408 Num fake examples 3016 Num true examples 3144\n",
      "  Batch 3,120  of  44,637.    Elapsed: 0:01:58. Training loss. 0.0023351635318249464 Num fake examples 3056 Num true examples 3184\n",
      "  Batch 3,160  of  44,637.    Elapsed: 0:01:59. Training loss. 0.003095068968832493 Num fake examples 3104 Num true examples 3216\n",
      "  Batch 3,200  of  44,637.    Elapsed: 0:02:01. Training loss. 0.0034306596498936415 Num fake examples 3141 Num true examples 3259\n",
      "  Batch 3,240  of  44,637.    Elapsed: 0:02:02. Training loss. 0.0029862220399081707 Num fake examples 3183 Num true examples 3297\n",
      "  Batch 3,280  of  44,637.    Elapsed: 0:02:04. Training loss. 0.0033630230464041233 Num fake examples 3220 Num true examples 3340\n",
      "  Batch 3,320  of  44,637.    Elapsed: 0:02:05. Training loss. 0.002817534375935793 Num fake examples 3248 Num true examples 3392\n",
      "  Batch 3,360  of  44,637.    Elapsed: 0:02:07. Training loss. 0.002984792459756136 Num fake examples 3286 Num true examples 3434\n",
      "  Batch 3,400  of  44,637.    Elapsed: 0:02:08. Training loss. 0.0030567802023142576 Num fake examples 3328 Num true examples 3472\n",
      "  Batch 3,440  of  44,637.    Elapsed: 0:02:10. Training loss. 0.003178865648806095 Num fake examples 3374 Num true examples 3506\n",
      "  Batch 3,480  of  44,637.    Elapsed: 0:02:11. Training loss. 0.002796964254230261 Num fake examples 3414 Num true examples 3546\n",
      "  Batch 3,520  of  44,637.    Elapsed: 0:02:13. Training loss. 0.0032880697399377823 Num fake examples 3453 Num true examples 3587\n",
      "  Batch 3,560  of  44,637.    Elapsed: 0:02:14. Training loss. 0.003495839424431324 Num fake examples 3501 Num true examples 3619\n",
      "  Batch 3,600  of  44,637.    Elapsed: 0:02:16. Training loss. 0.0033364505507051945 Num fake examples 3538 Num true examples 3662\n",
      "  Batch 3,640  of  44,637.    Elapsed: 0:02:17. Training loss. 0.002846222370862961 Num fake examples 3576 Num true examples 3704\n",
      "  Batch 3,680  of  44,637.    Elapsed: 0:02:19. Training loss. 0.004454670008271933 Num fake examples 3614 Num true examples 3746\n",
      "  Batch 3,720  of  44,637.    Elapsed: 0:02:20. Training loss. 0.004552948288619518 Num fake examples 3655 Num true examples 3785\n",
      "  Batch 3,760  of  44,637.    Elapsed: 0:02:22. Training loss. 0.0028887628577649593 Num fake examples 3689 Num true examples 3831\n",
      "  Batch 3,800  of  44,637.    Elapsed: 0:02:23. Training loss. 0.0036672335118055344 Num fake examples 3725 Num true examples 3875\n",
      "  Batch 3,840  of  44,637.    Elapsed: 0:02:25. Training loss. 0.0033697530161589384 Num fake examples 3767 Num true examples 3913\n",
      "  Batch 3,880  of  44,637.    Elapsed: 0:02:26. Training loss. 0.00252784276381135 Num fake examples 3804 Num true examples 3956\n",
      "  Batch 3,920  of  44,637.    Elapsed: 0:02:28. Training loss. 0.0028150598518550396 Num fake examples 3843 Num true examples 3997\n",
      "  Batch 3,960  of  44,637.    Elapsed: 0:02:29. Training loss. 0.002622075844556093 Num fake examples 3892 Num true examples 4028\n",
      "  Batch 4,000  of  44,637.    Elapsed: 0:02:31. Training loss. 0.0022118170745670795 Num fake examples 3933 Num true examples 4067\n",
      "  Batch 4,040  of  44,637.    Elapsed: 0:02:32. Training loss. 0.0033882884308695793 Num fake examples 3977 Num true examples 4103\n",
      "  Batch 4,080  of  44,637.    Elapsed: 0:02:34. Training loss. 0.006618070881813765 Num fake examples 4018 Num true examples 4142\n",
      "  Batch 4,120  of  44,637.    Elapsed: 0:02:35. Training loss. 0.0012785890139639378 Num fake examples 4048 Num true examples 4192\n",
      "  Batch 4,160  of  44,637.    Elapsed: 0:02:37. Training loss. 0.005220548715442419 Num fake examples 4087 Num true examples 4233\n",
      "  Batch 4,200  of  44,637.    Elapsed: 0:02:38. Training loss. 0.003721340326592326 Num fake examples 4122 Num true examples 4278\n",
      "  Batch 4,240  of  44,637.    Elapsed: 0:02:40. Training loss. 0.004012754186987877 Num fake examples 4165 Num true examples 4315\n",
      "  Batch 4,280  of  44,637.    Elapsed: 0:02:41. Training loss. 0.002873794175684452 Num fake examples 4204 Num true examples 4356\n",
      "  Batch 4,320  of  44,637.    Elapsed: 0:02:43. Training loss. 0.0036075301468372345 Num fake examples 4253 Num true examples 4387\n",
      "  Batch 4,360  of  44,637.    Elapsed: 0:02:44. Training loss. 0.0030013665091246367 Num fake examples 4286 Num true examples 4434\n",
      "  Batch 4,400  of  44,637.    Elapsed: 0:02:45. Training loss. 2.883016347885132 Num fake examples 4326 Num true examples 4474\n",
      "  Batch 4,440  of  44,637.    Elapsed: 0:02:47. Training loss. 0.002979045733809471 Num fake examples 4368 Num true examples 4512\n",
      "  Batch 4,480  of  44,637.    Elapsed: 0:02:48. Training loss. 0.0039045768789947033 Num fake examples 4410 Num true examples 4550\n",
      "  Batch 4,520  of  44,637.    Elapsed: 0:02:50. Training loss. 0.004598364699631929 Num fake examples 4450 Num true examples 4590\n",
      "  Batch 4,560  of  44,637.    Elapsed: 0:02:51. Training loss. 0.0033507870975881815 Num fake examples 4492 Num true examples 4628\n",
      "  Batch 4,600  of  44,637.    Elapsed: 0:02:53. Training loss. 0.002311850432306528 Num fake examples 4525 Num true examples 4675\n",
      "  Batch 4,640  of  44,637.    Elapsed: 0:02:54. Training loss. 0.0037505284417420626 Num fake examples 4568 Num true examples 4712\n",
      "  Batch 4,680  of  44,637.    Elapsed: 0:02:56. Training loss. 0.0036058854311704636 Num fake examples 4603 Num true examples 4757\n",
      "  Batch 4,720  of  44,637.    Elapsed: 0:02:57. Training loss. 0.0034637637436389923 Num fake examples 4640 Num true examples 4800\n",
      "  Batch 4,760  of  44,637.    Elapsed: 0:02:59. Training loss. 0.0046836817637085915 Num fake examples 4682 Num true examples 4838\n",
      "  Batch 4,800  of  44,637.    Elapsed: 0:03:00. Training loss. 2.8315258026123047 Num fake examples 4722 Num true examples 4878\n",
      "  Batch 4,840  of  44,637.    Elapsed: 0:03:02. Training loss. 0.007083545438945293 Num fake examples 4760 Num true examples 4920\n",
      "  Batch 4,880  of  44,637.    Elapsed: 0:03:03. Training loss. 0.0038023407105356455 Num fake examples 4795 Num true examples 4965\n",
      "  Batch 4,920  of  44,637.    Elapsed: 0:03:05. Training loss. 0.005263461731374264 Num fake examples 4837 Num true examples 5003\n",
      "  Batch 4,960  of  44,637.    Elapsed: 0:03:07. Training loss. 0.005000375211238861 Num fake examples 4886 Num true examples 5034\n",
      "  Batch 5,000  of  44,637.    Elapsed: 0:03:08. Training loss. 0.0053300331346690655 Num fake examples 4924 Num true examples 5076\n",
      "  Batch 5,040  of  44,637.    Elapsed: 0:03:10. Training loss. 0.00509512796998024 Num fake examples 4962 Num true examples 5118\n",
      "  Batch 5,080  of  44,637.    Elapsed: 0:03:11. Training loss. 0.002869530115276575 Num fake examples 4997 Num true examples 5163\n",
      "  Batch 5,120  of  44,637.    Elapsed: 0:03:13. Training loss. 0.004159805830568075 Num fake examples 5041 Num true examples 5199\n",
      "  Batch 5,160  of  44,637.    Elapsed: 0:03:15. Training loss. 0.003037676913663745 Num fake examples 5074 Num true examples 5246\n",
      "  Batch 5,200  of  44,637.    Elapsed: 0:03:16. Training loss. 0.003171131480485201 Num fake examples 5112 Num true examples 5288\n",
      "  Batch 5,240  of  44,637.    Elapsed: 0:03:18. Training loss. 2.8716468811035156 Num fake examples 5156 Num true examples 5324\n",
      "  Batch 5,280  of  44,637.    Elapsed: 0:03:19. Training loss. 0.0042532761581242085 Num fake examples 5200 Num true examples 5360\n",
      "  Batch 5,320  of  44,637.    Elapsed: 0:03:21. Training loss. 0.004416122566908598 Num fake examples 5245 Num true examples 5395\n",
      "  Batch 5,360  of  44,637.    Elapsed: 0:03:22. Training loss. 0.004713745787739754 Num fake examples 5282 Num true examples 5438\n",
      "  Batch 5,400  of  44,637.    Elapsed: 0:03:24. Training loss. 0.0064409952610731125 Num fake examples 5321 Num true examples 5479\n",
      "  Batch 5,440  of  44,637.    Elapsed: 0:03:26. Training loss. 0.003952025435864925 Num fake examples 5365 Num true examples 5515\n",
      "  Batch 5,480  of  44,637.    Elapsed: 0:03:27. Training loss. 0.00364904897287488 Num fake examples 5409 Num true examples 5551\n",
      "  Batch 5,520  of  44,637.    Elapsed: 0:03:29. Training loss. 0.0028576492331922054 Num fake examples 5447 Num true examples 5593\n",
      "  Batch 5,560  of  44,637.    Elapsed: 0:03:30. Training loss. 3.181835412979126 Num fake examples 5487 Num true examples 5633\n",
      "  Batch 5,600  of  44,637.    Elapsed: 0:03:32. Training loss. 0.003817631397396326 Num fake examples 5529 Num true examples 5671\n",
      "  Batch 5,640  of  44,637.    Elapsed: 0:03:34. Training loss. 0.004156424663960934 Num fake examples 5565 Num true examples 5715\n",
      "  Batch 5,680  of  44,637.    Elapsed: 0:03:35. Training loss. 0.004533381201326847 Num fake examples 5600 Num true examples 5760\n",
      "  Batch 5,720  of  44,637.    Elapsed: 0:03:37. Training loss. 0.002871089382097125 Num fake examples 5643 Num true examples 5797\n",
      "  Batch 5,760  of  44,637.    Elapsed: 0:03:38. Training loss. 0.003242526901885867 Num fake examples 5686 Num true examples 5834\n",
      "  Batch 5,800  of  44,637.    Elapsed: 0:03:40. Training loss. 0.004501324146986008 Num fake examples 5720 Num true examples 5880\n",
      "  Batch 5,840  of  44,637.    Elapsed: 0:03:41. Training loss. 0.004999889992177486 Num fake examples 5755 Num true examples 5925\n",
      "  Batch 5,880  of  44,637.    Elapsed: 0:03:43. Training loss. 0.0038440395146608353 Num fake examples 5794 Num true examples 5966\n",
      "  Batch 5,920  of  44,637.    Elapsed: 0:03:44. Training loss. 0.004909226670861244 Num fake examples 5834 Num true examples 6006\n",
      "  Batch 5,960  of  44,637.    Elapsed: 0:03:46. Training loss. 0.0026707437355071306 Num fake examples 5876 Num true examples 6044\n",
      "  Batch 6,000  of  44,637.    Elapsed: 0:03:48. Training loss. 0.004097850527614355 Num fake examples 5919 Num true examples 6081\n",
      "  Batch 6,040  of  44,637.    Elapsed: 0:03:49. Training loss. 0.005181737244129181 Num fake examples 5959 Num true examples 6121\n",
      "  Batch 6,080  of  44,637.    Elapsed: 0:03:51. Training loss. 0.0024925218895077705 Num fake examples 5993 Num true examples 6167\n",
      "  Batch 6,120  of  44,637.    Elapsed: 0:03:52. Training loss. 0.00840967521071434 Num fake examples 6030 Num true examples 6210\n",
      "  Batch 6,160  of  44,637.    Elapsed: 0:03:54. Training loss. 0.005246448330581188 Num fake examples 6071 Num true examples 6249\n",
      "  Batch 6,200  of  44,637.    Elapsed: 0:03:55. Training loss. 0.0034959581680595875 Num fake examples 6111 Num true examples 6289\n",
      "  Batch 6,240  of  44,637.    Elapsed: 0:03:57. Training loss. 0.002361750230193138 Num fake examples 6150 Num true examples 6330\n",
      "  Batch 6,280  of  44,637.    Elapsed: 0:03:59. Training loss. 2.7174203395843506 Num fake examples 6189 Num true examples 6371\n",
      "  Batch 6,320  of  44,637.    Elapsed: 0:04:00. Training loss. 0.007375221699476242 Num fake examples 6230 Num true examples 6410\n",
      "  Batch 6,360  of  44,637.    Elapsed: 0:04:02. Training loss. 0.005728032905608416 Num fake examples 6266 Num true examples 6454\n",
      "  Batch 6,400  of  44,637.    Elapsed: 0:04:03. Training loss. 0.006995462812483311 Num fake examples 6309 Num true examples 6491\n",
      "  Batch 6,440  of  44,637.    Elapsed: 0:04:05. Training loss. 0.0022356631234288216 Num fake examples 6349 Num true examples 6531\n",
      "  Batch 6,480  of  44,637.    Elapsed: 0:04:06. Training loss. 0.004666103050112724 Num fake examples 6381 Num true examples 6579\n",
      "  Batch 6,520  of  44,637.    Elapsed: 0:04:08. Training loss. 0.002906553680077195 Num fake examples 6417 Num true examples 6623\n",
      "  Batch 6,560  of  44,637.    Elapsed: 0:04:10. Training loss. 0.003178361803293228 Num fake examples 6453 Num true examples 6667\n",
      "  Batch 6,600  of  44,637.    Elapsed: 0:04:11. Training loss. 0.002899631392210722 Num fake examples 6490 Num true examples 6710\n",
      "  Batch 6,640  of  44,637.    Elapsed: 0:04:13. Training loss. 0.0019172560423612595 Num fake examples 6525 Num true examples 6755\n",
      "  Batch 6,680  of  44,637.    Elapsed: 0:04:14. Training loss. 0.0014749279944226146 Num fake examples 6557 Num true examples 6803\n",
      "  Batch 6,720  of  44,637.    Elapsed: 0:04:16. Training loss. 0.0018655889434739947 Num fake examples 6595 Num true examples 6845\n",
      "  Batch 6,760  of  44,637.    Elapsed: 0:04:17. Training loss. 0.0033125339541584253 Num fake examples 6634 Num true examples 6886\n",
      "  Batch 6,800  of  44,637.    Elapsed: 0:04:19. Training loss. 0.0026439279317855835 Num fake examples 6683 Num true examples 6917\n",
      "  Batch 6,840  of  44,637.    Elapsed: 0:04:21. Training loss. 0.002297554397955537 Num fake examples 6724 Num true examples 6956\n",
      "  Batch 6,880  of  44,637.    Elapsed: 0:04:22. Training loss. 0.00320252263918519 Num fake examples 6764 Num true examples 6996\n",
      "  Batch 6,920  of  44,637.    Elapsed: 0:04:24. Training loss. 0.004408146254718304 Num fake examples 6801 Num true examples 7039\n",
      "  Batch 6,960  of  44,637.    Elapsed: 0:04:25. Training loss. 0.0022314763627946377 Num fake examples 6836 Num true examples 7084\n",
      "  Batch 7,000  of  44,637.    Elapsed: 0:04:27. Training loss. 0.003487269626930356 Num fake examples 6883 Num true examples 7117\n",
      "  Batch 7,040  of  44,637.    Elapsed: 0:04:29. Training loss. 0.0036874646320939064 Num fake examples 6924 Num true examples 7156\n",
      "  Batch 7,080  of  44,637.    Elapsed: 0:04:30. Training loss. 0.005308616906404495 Num fake examples 6961 Num true examples 7199\n",
      "  Batch 7,120  of  44,637.    Elapsed: 0:04:32. Training loss. 0.004088452085852623 Num fake examples 7003 Num true examples 7237\n",
      "  Batch 7,160  of  44,637.    Elapsed: 0:04:33. Training loss. 0.0041242619045078754 Num fake examples 7042 Num true examples 7278\n",
      "  Batch 7,200  of  44,637.    Elapsed: 0:04:35. Training loss. 0.0049300529062747955 Num fake examples 7082 Num true examples 7318\n",
      "  Batch 7,240  of  44,637.    Elapsed: 0:04:36. Training loss. 0.0030454532243311405 Num fake examples 7122 Num true examples 7358\n",
      "  Batch 7,280  of  44,637.    Elapsed: 0:04:38. Training loss. 0.003748007118701935 Num fake examples 7159 Num true examples 7401\n",
      "  Batch 7,320  of  44,637.    Elapsed: 0:04:40. Training loss. 0.0031140903010964394 Num fake examples 7200 Num true examples 7440\n",
      "  Batch 7,360  of  44,637.    Elapsed: 0:04:41. Training loss. 0.0030356645584106445 Num fake examples 7235 Num true examples 7485\n",
      "  Batch 7,400  of  44,637.    Elapsed: 0:04:43. Training loss. 3.0638015270233154 Num fake examples 7278 Num true examples 7522\n",
      "  Batch 7,440  of  44,637.    Elapsed: 0:04:44. Training loss. 0.004423019476234913 Num fake examples 7312 Num true examples 7568\n",
      "  Batch 7,480  of  44,637.    Elapsed: 0:04:46. Training loss. 0.003935632295906544 Num fake examples 7348 Num true examples 7612\n",
      "  Batch 7,520  of  44,637.    Elapsed: 0:04:47. Training loss. 0.003214037511497736 Num fake examples 7396 Num true examples 7644\n",
      "  Batch 7,560  of  44,637.    Elapsed: 0:04:49. Training loss. 0.0032020306680351496 Num fake examples 7430 Num true examples 7690\n",
      "  Batch 7,600  of  44,637.    Elapsed: 0:04:50. Training loss. 0.0034191207960247993 Num fake examples 7474 Num true examples 7726\n",
      "  Batch 7,640  of  44,637.    Elapsed: 0:04:52. Training loss. 0.004194040782749653 Num fake examples 7515 Num true examples 7765\n",
      "  Batch 7,680  of  44,637.    Elapsed: 0:04:54. Training loss. 0.0030503105372190475 Num fake examples 7567 Num true examples 7793\n",
      "  Batch 7,720  of  44,637.    Elapsed: 0:04:55. Training loss. 2.656639575958252 Num fake examples 7609 Num true examples 7831\n",
      "  Batch 7,760  of  44,637.    Elapsed: 0:04:57. Training loss. 0.004280564840883017 Num fake examples 7641 Num true examples 7879\n",
      "  Batch 7,800  of  44,637.    Elapsed: 0:04:58. Training loss. 0.0032253828831017017 Num fake examples 7682 Num true examples 7918\n",
      "  Batch 7,840  of  44,637.    Elapsed: 0:05:00. Training loss. 0.0025157248601317406 Num fake examples 7713 Num true examples 7967\n",
      "  Batch 7,880  of  44,637.    Elapsed: 0:05:01. Training loss. 0.003848241176456213 Num fake examples 7747 Num true examples 8013\n",
      "  Batch 7,920  of  44,637.    Elapsed: 0:05:03. Training loss. 0.005204242654144764 Num fake examples 7790 Num true examples 8050\n",
      "  Batch 7,960  of  44,637.    Elapsed: 0:05:04. Training loss. 0.006134668365120888 Num fake examples 7822 Num true examples 8098\n",
      "  Batch 8,000  of  44,637.    Elapsed: 0:05:06. Training loss. 0.005897923372685909 Num fake examples 7856 Num true examples 8144\n",
      "  Batch 8,040  of  44,637.    Elapsed: 0:05:08. Training loss. 0.005683178082108498 Num fake examples 7898 Num true examples 8182\n",
      "  Batch 8,080  of  44,637.    Elapsed: 0:05:09. Training loss. 0.005125155206769705 Num fake examples 7937 Num true examples 8223\n",
      "  Batch 8,120  of  44,637.    Elapsed: 0:05:11. Training loss. 0.004842645488679409 Num fake examples 7979 Num true examples 8261\n",
      "  Batch 8,160  of  44,637.    Elapsed: 0:05:12. Training loss. 5.428670883178711 Num fake examples 8015 Num true examples 8305\n",
      "  Batch 8,200  of  44,637.    Elapsed: 0:05:14. Training loss. 0.003330395556986332 Num fake examples 8058 Num true examples 8342\n",
      "  Batch 8,240  of  44,637.    Elapsed: 0:05:16. Training loss. 0.003068632446229458 Num fake examples 8091 Num true examples 8389\n",
      "  Batch 8,280  of  44,637.    Elapsed: 0:05:17. Training loss. 0.003429786302149296 Num fake examples 8131 Num true examples 8429\n",
      "  Batch 8,320  of  44,637.    Elapsed: 0:05:18. Training loss. 0.00488231610506773 Num fake examples 8169 Num true examples 8471\n",
      "  Batch 8,360  of  44,637.    Elapsed: 0:05:20. Training loss. 0.0055299000814557076 Num fake examples 8212 Num true examples 8508\n",
      "  Batch 8,400  of  44,637.    Elapsed: 0:05:22. Training loss. 0.0050347656942903996 Num fake examples 8256 Num true examples 8544\n",
      "  Batch 8,440  of  44,637.    Elapsed: 0:05:23. Training loss. 0.0025645741261541843 Num fake examples 8292 Num true examples 8588\n",
      "  Batch 8,480  of  44,637.    Elapsed: 0:05:25. Training loss. 0.002881284337490797 Num fake examples 8339 Num true examples 8621\n",
      "  Batch 8,520  of  44,637.    Elapsed: 0:05:26. Training loss. 0.0030181307811290026 Num fake examples 8371 Num true examples 8669\n",
      "  Batch 8,560  of  44,637.    Elapsed: 0:05:28. Training loss. 0.0025436775758862495 Num fake examples 8414 Num true examples 8706\n",
      "  Batch 8,600  of  44,637.    Elapsed: 0:05:29. Training loss. 0.002823531860485673 Num fake examples 8453 Num true examples 8747\n",
      "  Batch 8,640  of  44,637.    Elapsed: 0:05:31. Training loss. 0.003403326030820608 Num fake examples 8492 Num true examples 8788\n",
      "  Batch 8,680  of  44,637.    Elapsed: 0:05:33. Training loss. 0.0030418597161769867 Num fake examples 8525 Num true examples 8835\n",
      "  Batch 8,720  of  44,637.    Elapsed: 0:05:34. Training loss. 0.005273257382214069 Num fake examples 8567 Num true examples 8873\n",
      "  Batch 8,760  of  44,637.    Elapsed: 0:05:36. Training loss. 0.003428366733714938 Num fake examples 8602 Num true examples 8918\n",
      "  Batch 8,800  of  44,637.    Elapsed: 0:05:37. Training loss. 2.81988263130188 Num fake examples 8645 Num true examples 8955\n",
      "  Batch 8,840  of  44,637.    Elapsed: 0:05:39. Training loss. 0.00241109449416399 Num fake examples 8688 Num true examples 8992\n",
      "  Batch 8,880  of  44,637.    Elapsed: 0:05:40. Training loss. 0.0027095158584415913 Num fake examples 8718 Num true examples 9042\n",
      "  Batch 8,920  of  44,637.    Elapsed: 0:05:42. Training loss. 2.87502384185791 Num fake examples 8756 Num true examples 9084\n",
      "  Batch 8,960  of  44,637.    Elapsed: 0:05:43. Training loss. 0.004171968437731266 Num fake examples 8794 Num true examples 9126\n",
      "  Batch 9,000  of  44,637.    Elapsed: 0:05:45. Training loss. 0.0058903684839606285 Num fake examples 8837 Num true examples 9163\n",
      "  Batch 9,040  of  44,637.    Elapsed: 0:05:47. Training loss. 0.0032990104518830776 Num fake examples 8880 Num true examples 9200\n",
      "  Batch 9,080  of  44,637.    Elapsed: 0:05:48. Training loss. 0.003526213113218546 Num fake examples 8920 Num true examples 9240\n",
      "  Batch 9,120  of  44,637.    Elapsed: 0:05:50. Training loss. 0.0037260977551341057 Num fake examples 8956 Num true examples 9284\n",
      "  Batch 9,160  of  44,637.    Elapsed: 0:05:51. Training loss. 0.0025519076734781265 Num fake examples 8996 Num true examples 9324\n",
      "  Batch 9,200  of  44,637.    Elapsed: 0:05:53. Training loss. 0.003322412259876728 Num fake examples 9040 Num true examples 9360\n",
      "  Batch 9,240  of  44,637.    Elapsed: 0:05:54. Training loss. 0.0028193150646984577 Num fake examples 9080 Num true examples 9400\n",
      "  Batch 9,280  of  44,637.    Elapsed: 0:05:56. Training loss. 0.0030351479072123766 Num fake examples 9121 Num true examples 9439\n",
      "  Batch 9,320  of  44,637.    Elapsed: 0:05:58. Training loss. 0.0038406532257795334 Num fake examples 9167 Num true examples 9473\n",
      "  Batch 9,360  of  44,637.    Elapsed: 0:05:59. Training loss. 0.004828672856092453 Num fake examples 9207 Num true examples 9513\n",
      "  Batch 9,400  of  44,637.    Elapsed: 0:06:01. Training loss. 0.0029522732365876436 Num fake examples 9245 Num true examples 9555\n",
      "  Batch 9,440  of  44,637.    Elapsed: 0:06:02. Training loss. 0.00216299039311707 Num fake examples 9288 Num true examples 9592\n",
      "  Batch 9,480  of  44,637.    Elapsed: 0:06:04. Training loss. 0.0023407787084579468 Num fake examples 9332 Num true examples 9628\n",
      "  Batch 9,520  of  44,637.    Elapsed: 0:06:06. Training loss. 0.002071815077215433 Num fake examples 9366 Num true examples 9674\n",
      "  Batch 9,560  of  44,637.    Elapsed: 0:06:07. Training loss. 0.002078367630019784 Num fake examples 9401 Num true examples 9719\n",
      "  Batch 9,600  of  44,637.    Elapsed: 0:06:09. Training loss. 0.0023176393005996943 Num fake examples 9436 Num true examples 9764\n",
      "  Batch 9,640  of  44,637.    Elapsed: 0:06:10. Training loss. 0.0018655600724741817 Num fake examples 9475 Num true examples 9805\n",
      "  Batch 9,680  of  44,637.    Elapsed: 0:06:12. Training loss. 0.0019333092495799065 Num fake examples 9518 Num true examples 9842\n",
      "  Batch 9,720  of  44,637.    Elapsed: 0:06:14. Training loss. 0.0015013683587312698 Num fake examples 9560 Num true examples 9880\n",
      "  Batch 9,760  of  44,637.    Elapsed: 0:06:15. Training loss. 0.0028336811810731888 Num fake examples 9593 Num true examples 9927\n",
      "  Batch 9,800  of  44,637.    Elapsed: 0:06:17. Training loss. 0.0025025303475558758 Num fake examples 9642 Num true examples 9958\n",
      "  Batch 9,840  of  44,637.    Elapsed: 0:06:18. Training loss. 0.0033469856716692448 Num fake examples 9676 Num true examples 10004\n",
      "  Batch 9,880  of  44,637.    Elapsed: 0:06:20. Training loss. 0.0024063349701464176 Num fake examples 9711 Num true examples 10049\n",
      "  Batch 9,920  of  44,637.    Elapsed: 0:06:21. Training loss. 0.0028312732465565205 Num fake examples 9759 Num true examples 10081\n",
      "  Batch 9,960  of  44,637.    Elapsed: 0:06:23. Training loss. 0.005128146149218082 Num fake examples 9795 Num true examples 10125\n",
      "  Batch 10,000  of  44,637.    Elapsed: 0:06:25. Training loss. 0.0028934264555573463 Num fake examples 9840 Num true examples 10160\n",
      "  Batch 10,040  of  44,637.    Elapsed: 0:06:26. Training loss. 0.0022771358489990234 Num fake examples 9881 Num true examples 10199\n",
      "  Batch 10,080  of  44,637.    Elapsed: 0:06:28. Training loss. 0.0022260230034589767 Num fake examples 9930 Num true examples 10230\n",
      "  Batch 10,120  of  44,637.    Elapsed: 0:06:29. Training loss. 0.003567168489098549 Num fake examples 9967 Num true examples 10273\n",
      "  Batch 10,160  of  44,637.    Elapsed: 0:06:31. Training loss. 0.0027116069104522467 Num fake examples 10007 Num true examples 10313\n",
      "  Batch 10,200  of  44,637.    Elapsed: 0:06:32. Training loss. 0.0028004248160868883 Num fake examples 10042 Num true examples 10358\n",
      "  Batch 10,240  of  44,637.    Elapsed: 0:06:34. Training loss. 0.0022525624372065067 Num fake examples 10084 Num true examples 10396\n",
      "  Batch 10,280  of  44,637.    Elapsed: 0:06:36. Training loss. 0.00474636210128665 Num fake examples 10125 Num true examples 10435\n",
      "  Batch 10,320  of  44,637.    Elapsed: 0:06:37. Training loss. 0.0037684855051338673 Num fake examples 10163 Num true examples 10477\n",
      "  Batch 10,360  of  44,637.    Elapsed: 0:06:39. Training loss. 0.0027197774033993483 Num fake examples 10203 Num true examples 10517\n",
      "  Batch 10,400  of  44,637.    Elapsed: 0:06:40. Training loss. 0.003265809267759323 Num fake examples 10234 Num true examples 10566\n",
      "  Batch 10,440  of  44,637.    Elapsed: 0:06:42. Training loss. 2.3930556774139404 Num fake examples 10267 Num true examples 10613\n",
      "  Batch 10,480  of  44,637.    Elapsed: 0:06:44. Training loss. 0.005106527358293533 Num fake examples 10300 Num true examples 10660\n",
      "  Batch 10,520  of  44,637.    Elapsed: 0:06:45. Training loss. 0.00239711906760931 Num fake examples 10344 Num true examples 10696\n",
      "  Batch 10,560  of  44,637.    Elapsed: 0:06:47. Training loss. 0.0032616062089800835 Num fake examples 10380 Num true examples 10740\n",
      "  Batch 10,600  of  44,637.    Elapsed: 0:06:48. Training loss. 0.003232306567952037 Num fake examples 10414 Num true examples 10786\n",
      "  Batch 10,640  of  44,637.    Elapsed: 0:06:50. Training loss. 0.00452957209199667 Num fake examples 10458 Num true examples 10822\n",
      "  Batch 10,680  of  44,637.    Elapsed: 0:06:51. Training loss. 0.0068236482329666615 Num fake examples 10498 Num true examples 10862\n",
      "  Batch 10,720  of  44,637.    Elapsed: 0:06:53. Training loss. 0.007182177156209946 Num fake examples 10536 Num true examples 10904\n",
      "  Batch 10,760  of  44,637.    Elapsed: 0:06:55. Training loss. 0.006149813532829285 Num fake examples 10574 Num true examples 10946\n",
      "  Batch 10,800  of  44,637.    Elapsed: 0:06:56. Training loss. 0.004371770657598972 Num fake examples 10624 Num true examples 10976\n",
      "  Batch 10,840  of  44,637.    Elapsed: 0:06:58. Training loss. 0.003559570526704192 Num fake examples 10664 Num true examples 11016\n",
      "  Batch 10,880  of  44,637.    Elapsed: 0:06:59. Training loss. 0.0037568090483546257 Num fake examples 10707 Num true examples 11053\n",
      "  Batch 10,920  of  44,637.    Elapsed: 0:07:01. Training loss. 0.003391362028196454 Num fake examples 10750 Num true examples 11090\n",
      "  Batch 10,960  of  44,637.    Elapsed: 0:07:02. Training loss. 0.003526333486661315 Num fake examples 10782 Num true examples 11138\n",
      "  Batch 11,000  of  44,637.    Elapsed: 0:07:04. Training loss. 0.0035414898302406073 Num fake examples 10823 Num true examples 11177\n",
      "  Batch 11,040  of  44,637.    Elapsed: 0:07:06. Training loss. 0.003363856580108404 Num fake examples 10853 Num true examples 11227\n",
      "  Batch 11,080  of  44,637.    Elapsed: 0:07:07. Training loss. 2.8774685859680176 Num fake examples 10885 Num true examples 11275\n",
      "  Batch 11,120  of  44,637.    Elapsed: 0:07:09. Training loss. 0.003909695893526077 Num fake examples 10923 Num true examples 11317\n",
      "  Batch 11,160  of  44,637.    Elapsed: 0:07:10. Training loss. 0.004897616803646088 Num fake examples 10963 Num true examples 11357\n",
      "  Batch 11,200  of  44,637.    Elapsed: 0:07:12. Training loss. 0.007249557413160801 Num fake examples 11004 Num true examples 11396\n",
      "  Batch 11,240  of  44,637.    Elapsed: 0:07:13. Training loss. 0.003982510417699814 Num fake examples 11042 Num true examples 11438\n",
      "  Batch 11,280  of  44,637.    Elapsed: 0:07:15. Training loss. 0.005351036787033081 Num fake examples 11085 Num true examples 11475\n",
      "  Batch 11,320  of  44,637.    Elapsed: 0:07:16. Training loss. 0.0045052156783640385 Num fake examples 11122 Num true examples 11518\n",
      "  Batch 11,360  of  44,637.    Elapsed: 0:07:18. Training loss. 0.0032709799706935883 Num fake examples 11167 Num true examples 11553\n",
      "  Batch 11,400  of  44,637.    Elapsed: 0:07:19. Training loss. 0.004225480370223522 Num fake examples 11200 Num true examples 11600\n",
      "  Batch 11,440  of  44,637.    Elapsed: 0:07:21. Training loss. 0.0038883103989064693 Num fake examples 11239 Num true examples 11641\n",
      "  Batch 11,480  of  44,637.    Elapsed: 0:07:22. Training loss. 0.005720571614801884 Num fake examples 11282 Num true examples 11678\n",
      "  Batch 11,520  of  44,637.    Elapsed: 0:07:24. Training loss. 0.008039137348532677 Num fake examples 11318 Num true examples 11722\n",
      "  Batch 11,560  of  44,637.    Elapsed: 0:07:25. Training loss. 0.004502960480749607 Num fake examples 11357 Num true examples 11763\n",
      "  Batch 11,600  of  44,637.    Elapsed: 0:07:27. Training loss. 0.004122566897422075 Num fake examples 11395 Num true examples 11805\n",
      "  Batch 11,640  of  44,637.    Elapsed: 0:07:28. Training loss. 0.004097651690244675 Num fake examples 11436 Num true examples 11844\n",
      "  Batch 11,680  of  44,637.    Elapsed: 0:07:30. Training loss. 0.003956042230129242 Num fake examples 11467 Num true examples 11893\n",
      "  Batch 11,720  of  44,637.    Elapsed: 0:07:31. Training loss. 0.0027518421411514282 Num fake examples 11501 Num true examples 11939\n",
      "  Batch 11,760  of  44,637.    Elapsed: 0:07:32. Training loss. 0.003570551984012127 Num fake examples 11541 Num true examples 11979\n",
      "  Batch 11,800  of  44,637.    Elapsed: 0:07:34. Training loss. 0.002316164318472147 Num fake examples 11576 Num true examples 12024\n",
      "  Batch 11,840  of  44,637.    Elapsed: 0:07:35. Training loss. 0.003207040950655937 Num fake examples 11618 Num true examples 12062\n",
      "  Batch 11,880  of  44,637.    Elapsed: 0:07:37. Training loss. 0.0035873863380402327 Num fake examples 11663 Num true examples 12097\n",
      "  Batch 11,920  of  44,637.    Elapsed: 0:07:38. Training loss. 0.002458135597407818 Num fake examples 11698 Num true examples 12142\n",
      "  Batch 11,960  of  44,637.    Elapsed: 0:07:40. Training loss. 0.003110315650701523 Num fake examples 11738 Num true examples 12182\n",
      "  Batch 12,000  of  44,637.    Elapsed: 0:07:41. Training loss. 0.003853648668155074 Num fake examples 11775 Num true examples 12225\n",
      "  Batch 12,040  of  44,637.    Elapsed: 0:07:43. Training loss. 0.002882571890950203 Num fake examples 11821 Num true examples 12259\n",
      "  Batch 12,080  of  44,637.    Elapsed: 0:07:44. Training loss. 0.0021651550196111202 Num fake examples 11864 Num true examples 12296\n",
      "  Batch 12,120  of  44,637.    Elapsed: 0:07:46. Training loss. 0.0021130270324647427 Num fake examples 11905 Num true examples 12335\n",
      "  Batch 12,160  of  44,637.    Elapsed: 0:07:47. Training loss. 0.003510416019707918 Num fake examples 11939 Num true examples 12381\n",
      "  Batch 12,200  of  44,637.    Elapsed: 0:07:49. Training loss. 0.0024545190390199423 Num fake examples 11976 Num true examples 12424\n",
      "  Batch 12,240  of  44,637.    Elapsed: 0:07:50. Training loss. 0.0038496439810842276 Num fake examples 12015 Num true examples 12465\n",
      "  Batch 12,280  of  44,637.    Elapsed: 0:07:52. Training loss. 0.0024970087688416243 Num fake examples 12048 Num true examples 12512\n",
      "  Batch 12,320  of  44,637.    Elapsed: 0:07:53. Training loss. 0.002089965157210827 Num fake examples 12091 Num true examples 12549\n",
      "  Batch 12,360  of  44,637.    Elapsed: 0:07:55. Training loss. 0.002666961634531617 Num fake examples 12132 Num true examples 12588\n",
      "  Batch 12,400  of  44,637.    Elapsed: 0:07:56. Training loss. 0.005787015426903963 Num fake examples 12171 Num true examples 12629\n",
      "  Batch 12,440  of  44,637.    Elapsed: 0:07:58. Training loss. 0.004423872567713261 Num fake examples 12213 Num true examples 12667\n",
      "  Batch 12,480  of  44,637.    Elapsed: 0:07:59. Training loss. 0.003991236910223961 Num fake examples 12254 Num true examples 12706\n",
      "  Batch 12,520  of  44,637.    Elapsed: 0:08:01. Training loss. 0.004266759380698204 Num fake examples 12284 Num true examples 12756\n",
      "  Batch 12,560  of  44,637.    Elapsed: 0:08:02. Training loss. 0.004748684354126453 Num fake examples 12326 Num true examples 12794\n",
      "  Batch 12,600  of  44,637.    Elapsed: 0:08:04. Training loss. 0.0036727534607052803 Num fake examples 12370 Num true examples 12830\n",
      "  Batch 12,640  of  44,637.    Elapsed: 0:08:05. Training loss. 0.0038203306030482054 Num fake examples 12413 Num true examples 12867\n",
      "  Batch 12,680  of  44,637.    Elapsed: 0:08:07. Training loss. 0.003923807293176651 Num fake examples 12453 Num true examples 12907\n",
      "  Batch 12,720  of  44,637.    Elapsed: 0:08:08. Training loss. 0.003474713070318103 Num fake examples 12490 Num true examples 12950\n",
      "  Batch 12,760  of  44,637.    Elapsed: 0:08:09. Training loss. 0.003915723413228989 Num fake examples 12523 Num true examples 12997\n",
      "  Batch 12,800  of  44,637.    Elapsed: 0:08:11. Training loss. 2.912172555923462 Num fake examples 12560 Num true examples 13040\n",
      "  Batch 12,840  of  44,637.    Elapsed: 0:08:12. Training loss. 0.003212061244994402 Num fake examples 12589 Num true examples 13091\n",
      "  Batch 12,880  of  44,637.    Elapsed: 0:08:14. Training loss. 0.002945561893284321 Num fake examples 12625 Num true examples 13135\n",
      "  Batch 12,920  of  44,637.    Elapsed: 0:08:15. Training loss. 0.0023925499990582466 Num fake examples 12666 Num true examples 13174\n",
      "  Batch 12,960  of  44,637.    Elapsed: 0:08:17. Training loss. 0.003284208010882139 Num fake examples 12703 Num true examples 13217\n",
      "  Batch 13,000  of  44,637.    Elapsed: 0:08:18. Training loss. 0.0037484802305698395 Num fake examples 12742 Num true examples 13258\n",
      "  Batch 13,040  of  44,637.    Elapsed: 0:08:20. Training loss. 0.003523749765008688 Num fake examples 12782 Num true examples 13298\n",
      "  Batch 13,080  of  44,637.    Elapsed: 0:08:21. Training loss. 0.0024509644135832787 Num fake examples 12820 Num true examples 13340\n",
      "  Batch 13,120  of  44,637.    Elapsed: 0:08:23. Training loss. 0.0029548723250627518 Num fake examples 12859 Num true examples 13381\n",
      "  Batch 13,160  of  44,637.    Elapsed: 0:08:24. Training loss. 0.0028037403244525194 Num fake examples 12893 Num true examples 13427\n",
      "  Batch 13,200  of  44,637.    Elapsed: 0:08:26. Training loss. 0.004406002815812826 Num fake examples 12938 Num true examples 13462\n",
      "  Batch 13,240  of  44,637.    Elapsed: 0:08:27. Training loss. 0.0014752360293641686 Num fake examples 12979 Num true examples 13501\n",
      "  Batch 13,280  of  44,637.    Elapsed: 0:08:29. Training loss. 0.002922292798757553 Num fake examples 13021 Num true examples 13539\n",
      "  Batch 13,320  of  44,637.    Elapsed: 0:08:30. Training loss. 0.0031582051888108253 Num fake examples 13058 Num true examples 13582\n",
      "  Batch 13,360  of  44,637.    Elapsed: 0:08:32. Training loss. 0.0023072841577231884 Num fake examples 13091 Num true examples 13629\n",
      "  Batch 13,400  of  44,637.    Elapsed: 0:08:34. Training loss. 0.005651029292494059 Num fake examples 13133 Num true examples 13667\n",
      "  Batch 13,440  of  44,637.    Elapsed: 0:08:35. Training loss. 0.002970718313008547 Num fake examples 13174 Num true examples 13706\n",
      "  Batch 13,480  of  44,637.    Elapsed: 0:08:37. Training loss. 0.00373641774058342 Num fake examples 13211 Num true examples 13749\n",
      "  Batch 13,520  of  44,637.    Elapsed: 0:08:38. Training loss. 0.003583964193239808 Num fake examples 13259 Num true examples 13781\n",
      "  Batch 13,560  of  44,637.    Elapsed: 0:08:40. Training loss. 0.0027144465129822493 Num fake examples 13300 Num true examples 13820\n",
      "  Batch 13,600  of  44,637.    Elapsed: 0:08:41. Training loss. 0.003467933274805546 Num fake examples 13340 Num true examples 13860\n",
      "  Batch 13,640  of  44,637.    Elapsed: 0:08:43. Training loss. 0.0025888823438435793 Num fake examples 13373 Num true examples 13907\n",
      "  Batch 13,680  of  44,637.    Elapsed: 0:08:44. Training loss. 0.002155893947929144 Num fake examples 13407 Num true examples 13953\n",
      "  Batch 13,720  of  44,637.    Elapsed: 0:08:46. Training loss. 0.0023355085868388414 Num fake examples 13448 Num true examples 13992\n",
      "  Batch 13,760  of  44,637.    Elapsed: 0:08:47. Training loss. 0.0025011266116052866 Num fake examples 13483 Num true examples 14037\n",
      "  Batch 13,800  of  44,637.    Elapsed: 0:08:49. Training loss. 0.0021923689637333155 Num fake examples 13524 Num true examples 14076\n",
      "  Batch 13,840  of  44,637.    Elapsed: 0:08:50. Training loss. 0.0029792343266308308 Num fake examples 13564 Num true examples 14116\n",
      "  Batch 13,880  of  44,637.    Elapsed: 0:08:52. Training loss. 0.0023341646883636713 Num fake examples 13605 Num true examples 14155\n",
      "  Batch 13,920  of  44,637.    Elapsed: 0:08:54. Training loss. 0.002547272713854909 Num fake examples 13640 Num true examples 14200\n",
      "  Batch 13,960  of  44,637.    Elapsed: 0:08:55. Training loss. 0.0057082390412688255 Num fake examples 13676 Num true examples 14244\n",
      "  Batch 14,000  of  44,637.    Elapsed: 0:08:57. Training loss. 0.004430110566318035 Num fake examples 13719 Num true examples 14281\n",
      "  Batch 14,040  of  44,637.    Elapsed: 0:08:58. Training loss. 0.004205131903290749 Num fake examples 13752 Num true examples 14328\n",
      "  Batch 14,080  of  44,637.    Elapsed: 0:09:00. Training loss. 0.003147765062749386 Num fake examples 13790 Num true examples 14370\n",
      "  Batch 14,120  of  44,637.    Elapsed: 0:09:02. Training loss. 0.002353969495743513 Num fake examples 13837 Num true examples 14403\n",
      "  Batch 14,160  of  44,637.    Elapsed: 0:09:03. Training loss. 0.003931156359612942 Num fake examples 13872 Num true examples 14448\n",
      "  Batch 14,200  of  44,637.    Elapsed: 0:09:05. Training loss. 0.00276136863976717 Num fake examples 13911 Num true examples 14489\n",
      "  Batch 14,240  of  44,637.    Elapsed: 0:09:06. Training loss. 0.0027397696394473314 Num fake examples 13947 Num true examples 14533\n",
      "  Batch 14,280  of  44,637.    Elapsed: 0:09:07. Training loss. 0.0024911141954362392 Num fake examples 13984 Num true examples 14576\n",
      "  Batch 14,320  of  44,637.    Elapsed: 0:09:09. Training loss. 0.0025976626202464104 Num fake examples 14030 Num true examples 14610\n",
      "  Batch 14,360  of  44,637.    Elapsed: 0:09:10. Training loss. 0.0027499618008732796 Num fake examples 14067 Num true examples 14653\n",
      "  Batch 14,400  of  44,637.    Elapsed: 0:09:12. Training loss. 0.0029863445088267326 Num fake examples 14105 Num true examples 14695\n",
      "  Batch 14,440  of  44,637.    Elapsed: 0:09:13. Training loss. 0.0022861999459564686 Num fake examples 14146 Num true examples 14734\n",
      "  Batch 14,480  of  44,637.    Elapsed: 0:09:15. Training loss. 0.002771107479929924 Num fake examples 14186 Num true examples 14774\n",
      "  Batch 14,520  of  44,637.    Elapsed: 0:09:16. Training loss. 0.0022671674378216267 Num fake examples 14220 Num true examples 14820\n",
      "  Batch 14,560  of  44,637.    Elapsed: 0:09:18. Training loss. 0.0037807421758770943 Num fake examples 14255 Num true examples 14865\n",
      "  Batch 14,600  of  44,637.    Elapsed: 0:09:19. Training loss. 0.0033360603265464306 Num fake examples 14298 Num true examples 14902\n",
      "  Batch 14,640  of  44,637.    Elapsed: 0:09:21. Training loss. 0.002697309944778681 Num fake examples 14333 Num true examples 14947\n",
      "  Batch 14,680  of  44,637.    Elapsed: 0:09:22. Training loss. 0.002218660432845354 Num fake examples 14376 Num true examples 14984\n",
      "  Batch 14,720  of  44,637.    Elapsed: 0:09:24. Training loss. 0.0035431880969554186 Num fake examples 14417 Num true examples 15023\n",
      "  Batch 14,760  of  44,637.    Elapsed: 0:09:25. Training loss. 0.0035644778981804848 Num fake examples 14459 Num true examples 15061\n",
      "  Batch 14,800  of  44,637.    Elapsed: 0:09:27. Training loss. 0.003383326344192028 Num fake examples 14498 Num true examples 15102\n",
      "  Batch 14,840  of  44,637.    Elapsed: 0:09:28. Training loss. 0.002680988749489188 Num fake examples 14528 Num true examples 15152\n",
      "  Batch 14,880  of  44,637.    Elapsed: 0:09:29. Training loss. 0.002860694658011198 Num fake examples 14573 Num true examples 15187\n",
      "  Batch 14,920  of  44,637.    Elapsed: 0:09:31. Training loss. 0.0034309877082705498 Num fake examples 14619 Num true examples 15221\n",
      "  Batch 14,960  of  44,637.    Elapsed: 0:09:32. Training loss. 0.002496801083907485 Num fake examples 14657 Num true examples 15263\n",
      "  Batch 15,000  of  44,637.    Elapsed: 0:09:34. Training loss. 0.0029442503582686186 Num fake examples 14699 Num true examples 15301\n",
      "  Batch 15,040  of  44,637.    Elapsed: 0:09:35. Training loss. 0.003639093367382884 Num fake examples 14738 Num true examples 15342\n",
      "  Batch 15,080  of  44,637.    Elapsed: 0:09:37. Training loss. 0.0027836840599775314 Num fake examples 14773 Num true examples 15387\n",
      "  Batch 15,120  of  44,637.    Elapsed: 0:09:38. Training loss. 0.001999450847506523 Num fake examples 14800 Num true examples 15440\n",
      "  Batch 15,160  of  44,637.    Elapsed: 0:09:40. Training loss. 0.0019821475725620985 Num fake examples 14834 Num true examples 15486\n",
      "  Batch 15,200  of  44,637.    Elapsed: 0:09:41. Training loss. 0.0029789165128022432 Num fake examples 14869 Num true examples 15531\n",
      "  Batch 15,240  of  44,637.    Elapsed: 0:09:43. Training loss. 0.006269357167184353 Num fake examples 14904 Num true examples 15576\n",
      "  Batch 15,280  of  44,637.    Elapsed: 0:09:44. Training loss. 0.0033430918119847775 Num fake examples 14945 Num true examples 15615\n",
      "  Batch 15,320  of  44,637.    Elapsed: 0:09:46. Training loss. 0.0035037361085414886 Num fake examples 14988 Num true examples 15652\n",
      "  Batch 15,360  of  44,637.    Elapsed: 0:09:47. Training loss. 0.0030631572008132935 Num fake examples 15034 Num true examples 15686\n",
      "  Batch 15,400  of  44,637.    Elapsed: 0:09:49. Training loss. 0.002862165914848447 Num fake examples 15070 Num true examples 15730\n",
      "  Batch 15,440  of  44,637.    Elapsed: 0:09:50. Training loss. 0.002581249689683318 Num fake examples 15111 Num true examples 15769\n",
      "  Batch 15,480  of  44,637.    Elapsed: 0:09:52. Training loss. 0.0023324417416006327 Num fake examples 15157 Num true examples 15803\n",
      "  Batch 15,520  of  44,637.    Elapsed: 0:09:53. Training loss. 3.013576030731201 Num fake examples 15200 Num true examples 15840\n",
      "  Batch 15,560  of  44,637.    Elapsed: 0:09:54. Training loss. 0.003464780282229185 Num fake examples 15243 Num true examples 15877\n",
      "  Batch 15,600  of  44,637.    Elapsed: 0:09:56. Training loss. 2.7045116424560547 Num fake examples 15281 Num true examples 15919\n",
      "  Batch 15,640  of  44,637.    Elapsed: 0:09:57. Training loss. 0.003185840090736747 Num fake examples 15319 Num true examples 15961\n",
      "  Batch 15,680  of  44,637.    Elapsed: 0:09:59. Training loss. 0.002026872243732214 Num fake examples 15351 Num true examples 16009\n",
      "  Batch 15,720  of  44,637.    Elapsed: 0:10:00. Training loss. 0.00213230331428349 Num fake examples 15387 Num true examples 16053\n",
      "  Batch 15,760  of  44,637.    Elapsed: 0:10:02. Training loss. 0.0021690805442631245 Num fake examples 15421 Num true examples 16099\n",
      "  Batch 15,800  of  44,637.    Elapsed: 0:10:04. Training loss. 0.003091302467510104 Num fake examples 15460 Num true examples 16140\n",
      "  Batch 15,840  of  44,637.    Elapsed: 0:10:05. Training loss. 0.0024815243668854237 Num fake examples 15501 Num true examples 16179\n",
      "  Batch 15,880  of  44,637.    Elapsed: 0:10:07. Training loss. 0.0026000316720455885 Num fake examples 15535 Num true examples 16225\n",
      "  Batch 15,920  of  44,637.    Elapsed: 0:10:08. Training loss. 0.0016623244155198336 Num fake examples 15571 Num true examples 16269\n",
      "  Batch 15,960  of  44,637.    Elapsed: 0:10:10. Training loss. 0.002301824279129505 Num fake examples 15618 Num true examples 16302\n",
      "  Batch 16,000  of  44,637.    Elapsed: 0:10:11. Training loss. 0.002069555688649416 Num fake examples 15651 Num true examples 16349\n",
      "  Batch 16,040  of  44,637.    Elapsed: 0:10:12. Training loss. 0.002438531257212162 Num fake examples 15692 Num true examples 16388\n",
      "  Batch 16,080  of  44,637.    Elapsed: 0:10:14. Training loss. 0.0023286668583750725 Num fake examples 15728 Num true examples 16432\n",
      "  Batch 16,120  of  44,637.    Elapsed: 0:10:15. Training loss. 0.0028264126740396023 Num fake examples 15768 Num true examples 16472\n",
      "  Batch 16,160  of  44,637.    Elapsed: 0:10:17. Training loss. 0.0020945684518665075 Num fake examples 15808 Num true examples 16512\n",
      "  Batch 16,200  of  44,637.    Elapsed: 0:10:18. Training loss. 0.0015394467627629638 Num fake examples 15852 Num true examples 16548\n",
      "  Batch 16,240  of  44,637.    Elapsed: 0:10:20. Training loss. 0.0024891726206988096 Num fake examples 15889 Num true examples 16591\n",
      "  Batch 16,280  of  44,637.    Elapsed: 0:10:21. Training loss. 0.002438519150018692 Num fake examples 15930 Num true examples 16630\n",
      "  Batch 16,320  of  44,637.    Elapsed: 0:10:23. Training loss. 0.002767854603007436 Num fake examples 15969 Num true examples 16671\n",
      "  Batch 16,360  of  44,637.    Elapsed: 0:10:24. Training loss. 0.003798318561166525 Num fake examples 16004 Num true examples 16716\n",
      "  Batch 16,400  of  44,637.    Elapsed: 0:10:26. Training loss. 0.0021008141338825226 Num fake examples 16041 Num true examples 16759\n",
      "  Batch 16,440  of  44,637.    Elapsed: 0:10:27. Training loss. 0.0034489131066948175 Num fake examples 16078 Num true examples 16802\n",
      "  Batch 16,480  of  44,637.    Elapsed: 0:10:29. Training loss. 0.0029780054464936256 Num fake examples 16122 Num true examples 16838\n",
      "  Batch 16,520  of  44,637.    Elapsed: 0:10:30. Training loss. 0.00201340951025486 Num fake examples 16157 Num true examples 16883\n",
      "  Batch 16,560  of  44,637.    Elapsed: 0:10:32. Training loss. 0.0029015662148594856 Num fake examples 16202 Num true examples 16918\n",
      "  Batch 16,600  of  44,637.    Elapsed: 0:10:33. Training loss. 0.0035317076835781336 Num fake examples 16243 Num true examples 16957\n",
      "  Batch 16,640  of  44,637.    Elapsed: 0:10:34. Training loss. 0.003699189517647028 Num fake examples 16284 Num true examples 16996\n",
      "  Batch 16,680  of  44,637.    Elapsed: 0:10:36. Training loss. 0.0033736072946339846 Num fake examples 16323 Num true examples 17037\n",
      "  Batch 16,720  of  44,637.    Elapsed: 0:10:37. Training loss. 0.0037359334528446198 Num fake examples 16357 Num true examples 17083\n",
      "  Batch 16,760  of  44,637.    Elapsed: 0:10:39. Training loss. 0.0031179250217974186 Num fake examples 16394 Num true examples 17126\n",
      "  Batch 16,800  of  44,637.    Elapsed: 0:10:40. Training loss. 0.0027707370463758707 Num fake examples 16436 Num true examples 17164\n",
      "  Batch 16,840  of  44,637.    Elapsed: 0:10:42. Training loss. 0.0029074393678456545 Num fake examples 16480 Num true examples 17200\n",
      "  Batch 16,880  of  44,637.    Elapsed: 0:10:43. Training loss. 0.0024364544078707695 Num fake examples 16520 Num true examples 17240\n",
      "  Batch 16,920  of  44,637.    Elapsed: 0:10:45. Training loss. 0.002520096255466342 Num fake examples 16555 Num true examples 17285\n",
      "  Batch 16,960  of  44,637.    Elapsed: 0:10:46. Training loss. 0.002477428410202265 Num fake examples 16593 Num true examples 17327\n",
      "  Batch 17,000  of  44,637.    Elapsed: 0:10:48. Training loss. 2.9200901985168457 Num fake examples 16635 Num true examples 17365\n",
      "  Batch 17,040  of  44,637.    Elapsed: 0:10:49. Training loss. 0.00471422728151083 Num fake examples 16670 Num true examples 17410\n",
      "  Batch 17,080  of  44,637.    Elapsed: 0:10:51. Training loss. 0.004892669152468443 Num fake examples 16706 Num true examples 17454\n",
      "  Batch 17,120  of  44,637.    Elapsed: 0:10:52. Training loss. 0.004819272551685572 Num fake examples 16746 Num true examples 17494\n",
      "  Batch 17,160  of  44,637.    Elapsed: 0:10:54. Training loss. 0.003337546018883586 Num fake examples 16785 Num true examples 17535\n",
      "  Batch 17,200  of  44,637.    Elapsed: 0:10:55. Training loss. 0.003312407759949565 Num fake examples 16822 Num true examples 17578\n",
      "  Batch 17,240  of  44,637.    Elapsed: 0:10:57. Training loss. 0.003225641790777445 Num fake examples 16861 Num true examples 17619\n",
      "  Batch 17,280  of  44,637.    Elapsed: 0:10:58. Training loss. 0.0037426732014864683 Num fake examples 16901 Num true examples 17659\n",
      "  Batch 17,320  of  44,637.    Elapsed: 0:11:00. Training loss. 0.0026544188149273396 Num fake examples 16953 Num true examples 17687\n",
      "  Batch 17,360  of  44,637.    Elapsed: 0:11:01. Training loss. 0.0026702985633164644 Num fake examples 16994 Num true examples 17726\n",
      "  Batch 17,400  of  44,637.    Elapsed: 0:11:03. Training loss. 0.0032754745334386826 Num fake examples 17031 Num true examples 17769\n",
      "  Batch 17,440  of  44,637.    Elapsed: 0:11:04. Training loss. 0.0026303534395992756 Num fake examples 17072 Num true examples 17808\n",
      "  Batch 17,480  of  44,637.    Elapsed: 0:11:05. Training loss. 0.003092641942203045 Num fake examples 17118 Num true examples 17842\n",
      "  Batch 17,520  of  44,637.    Elapsed: 0:11:07. Training loss. 0.002559281187132001 Num fake examples 17153 Num true examples 17887\n",
      "  Batch 17,560  of  44,637.    Elapsed: 0:11:08. Training loss. 0.002793140709400177 Num fake examples 17191 Num true examples 17929\n",
      "  Batch 17,600  of  44,637.    Elapsed: 0:11:10. Training loss. 0.005895846523344517 Num fake examples 17231 Num true examples 17969\n",
      "  Batch 17,640  of  44,637.    Elapsed: 0:11:11. Training loss. 3.2680745124816895 Num fake examples 17270 Num true examples 18010\n",
      "  Batch 17,680  of  44,637.    Elapsed: 0:11:13. Training loss. 0.0026042433455586433 Num fake examples 17315 Num true examples 18045\n",
      "  Batch 17,720  of  44,637.    Elapsed: 0:11:14. Training loss. 0.0025354507379233837 Num fake examples 17353 Num true examples 18087\n",
      "  Batch 17,760  of  44,637.    Elapsed: 0:11:16. Training loss. 0.0022304565645754337 Num fake examples 17385 Num true examples 18135\n",
      "  Batch 17,800  of  44,637.    Elapsed: 0:11:17. Training loss. 0.0019690010230988264 Num fake examples 17422 Num true examples 18178\n",
      "  Batch 17,840  of  44,637.    Elapsed: 0:11:19. Training loss. 0.0029563228599727154 Num fake examples 17453 Num true examples 18227\n",
      "  Batch 17,880  of  44,637.    Elapsed: 0:11:20. Training loss. 0.0026981933042407036 Num fake examples 17494 Num true examples 18266\n",
      "  Batch 17,920  of  44,637.    Elapsed: 0:11:22. Training loss. 0.0021350712049752474 Num fake examples 17533 Num true examples 18307\n",
      "  Batch 17,960  of  44,637.    Elapsed: 0:11:23. Training loss. 0.0021822326816618443 Num fake examples 17570 Num true examples 18350\n",
      "  Batch 18,000  of  44,637.    Elapsed: 0:11:25. Training loss. 0.0038297437131404877 Num fake examples 17610 Num true examples 18390\n",
      "  Batch 18,040  of  44,637.    Elapsed: 0:11:26. Training loss. 0.002648446476086974 Num fake examples 17649 Num true examples 18431\n",
      "  Batch 18,080  of  44,637.    Elapsed: 0:11:28. Training loss. 0.004368060268461704 Num fake examples 17693 Num true examples 18467\n",
      "  Batch 18,120  of  44,637.    Elapsed: 0:11:29. Training loss. 0.005490316078066826 Num fake examples 17732 Num true examples 18508\n",
      "  Batch 18,160  of  44,637.    Elapsed: 0:11:30. Training loss. 0.003723489586263895 Num fake examples 17768 Num true examples 18552\n",
      "  Batch 18,200  of  44,637.    Elapsed: 0:11:32. Training loss. 0.004575405269861221 Num fake examples 17800 Num true examples 18600\n",
      "  Batch 18,240  of  44,637.    Elapsed: 0:11:33. Training loss. 0.005147763527929783 Num fake examples 17840 Num true examples 18640\n",
      "  Batch 18,280  of  44,637.    Elapsed: 0:11:35. Training loss. 0.003555335570126772 Num fake examples 17880 Num true examples 18680\n",
      "  Batch 18,320  of  44,637.    Elapsed: 0:11:36. Training loss. 0.0020902014803141356 Num fake examples 17913 Num true examples 18727\n",
      "  Batch 18,360  of  44,637.    Elapsed: 0:11:38. Training loss. 0.002677113050594926 Num fake examples 17953 Num true examples 18767\n",
      "  Batch 18,400  of  44,637.    Elapsed: 0:11:39. Training loss. 0.002232013735920191 Num fake examples 17993 Num true examples 18807\n",
      "  Batch 18,440  of  44,637.    Elapsed: 0:11:41. Training loss. 0.0022388892248272896 Num fake examples 18036 Num true examples 18844\n",
      "  Batch 18,480  of  44,637.    Elapsed: 0:11:42. Training loss. 0.002539017703384161 Num fake examples 18083 Num true examples 18877\n",
      "  Batch 18,520  of  44,637.    Elapsed: 0:11:44. Training loss. 0.004452008754014969 Num fake examples 18114 Num true examples 18926\n",
      "  Batch 18,560  of  44,637.    Elapsed: 0:11:45. Training loss. 0.006720959208905697 Num fake examples 18159 Num true examples 18961\n",
      "  Batch 18,600  of  44,637.    Elapsed: 0:11:47. Training loss. 0.0050283619202673435 Num fake examples 18202 Num true examples 18998\n",
      "  Batch 18,640  of  44,637.    Elapsed: 0:11:48. Training loss. 0.0033607296645641327 Num fake examples 18244 Num true examples 19036\n",
      "  Batch 18,680  of  44,637.    Elapsed: 0:11:50. Training loss. 0.005248663946986198 Num fake examples 18282 Num true examples 19078\n",
      "  Batch 18,720  of  44,637.    Elapsed: 0:11:51. Training loss. 0.004397316835820675 Num fake examples 18321 Num true examples 19119\n",
      "  Batch 18,760  of  44,637.    Elapsed: 0:11:53. Training loss. 0.003945562522858381 Num fake examples 18362 Num true examples 19158\n",
      "  Batch 18,800  of  44,637.    Elapsed: 0:11:54. Training loss. 0.004563580732792616 Num fake examples 18399 Num true examples 19201\n",
      "  Batch 18,840  of  44,637.    Elapsed: 0:11:56. Training loss. 0.005552147980779409 Num fake examples 18439 Num true examples 19241\n",
      "  Batch 18,880  of  44,637.    Elapsed: 0:11:57. Training loss. 0.0041865031234920025 Num fake examples 18467 Num true examples 19293\n",
      "  Batch 18,920  of  44,637.    Elapsed: 0:11:59. Training loss. 0.00416158139705658 Num fake examples 18504 Num true examples 19336\n",
      "  Batch 18,960  of  44,637.    Elapsed: 0:12:00. Training loss. 0.003723704721778631 Num fake examples 18542 Num true examples 19378\n",
      "  Batch 19,000  of  44,637.    Elapsed: 0:12:02. Training loss. 0.002740659285336733 Num fake examples 18584 Num true examples 19416\n",
      "  Batch 19,040  of  44,637.    Elapsed: 0:12:03. Training loss. 3.0919203758239746 Num fake examples 18627 Num true examples 19453\n",
      "  Batch 19,080  of  44,637.    Elapsed: 0:12:05. Training loss. 0.0031920026522129774 Num fake examples 18662 Num true examples 19498\n",
      "  Batch 19,120  of  44,637.    Elapsed: 0:12:06. Training loss. 0.0031539283227175474 Num fake examples 18705 Num true examples 19535\n",
      "  Batch 19,160  of  44,637.    Elapsed: 0:12:08. Training loss. 0.003000859636813402 Num fake examples 18744 Num true examples 19576\n",
      "  Batch 19,200  of  44,637.    Elapsed: 0:12:09. Training loss. 0.0045424774289131165 Num fake examples 18784 Num true examples 19616\n",
      "  Batch 19,240  of  44,637.    Elapsed: 0:12:11. Training loss. 0.003589420346543193 Num fake examples 18826 Num true examples 19654\n",
      "  Batch 19,280  of  44,637.    Elapsed: 0:12:12. Training loss. 0.0035063298419117928 Num fake examples 18862 Num true examples 19698\n",
      "  Batch 19,320  of  44,637.    Elapsed: 0:12:14. Training loss. 0.0038588903844356537 Num fake examples 18905 Num true examples 19735\n",
      "  Batch 19,360  of  44,637.    Elapsed: 0:12:15. Training loss. 0.002841516863554716 Num fake examples 18936 Num true examples 19784\n",
      "  Batch 19,400  of  44,637.    Elapsed: 0:12:17. Training loss. 0.0033157672733068466 Num fake examples 18975 Num true examples 19825\n",
      "  Batch 19,440  of  44,637.    Elapsed: 0:12:18. Training loss. 0.0027309837751090527 Num fake examples 19010 Num true examples 19870\n",
      "  Batch 19,480  of  44,637.    Elapsed: 0:12:20. Training loss. 0.004816994536668062 Num fake examples 19052 Num true examples 19908\n",
      "  Batch 19,520  of  44,637.    Elapsed: 0:12:21. Training loss. 0.004596630576997995 Num fake examples 19093 Num true examples 19947\n",
      "  Batch 19,560  of  44,637.    Elapsed: 0:12:23. Training loss. 0.004123008344322443 Num fake examples 19134 Num true examples 19986\n",
      "  Batch 19,600  of  44,637.    Elapsed: 0:12:24. Training loss. 0.0038817974273115396 Num fake examples 19166 Num true examples 20034\n",
      "  Batch 19,640  of  44,637.    Elapsed: 0:12:26. Training loss. 0.003114469116553664 Num fake examples 19205 Num true examples 20075\n",
      "  Batch 19,680  of  44,637.    Elapsed: 0:12:27. Training loss. 0.003902746131643653 Num fake examples 19243 Num true examples 20117\n",
      "  Batch 19,720  of  44,637.    Elapsed: 0:12:29. Training loss. 0.003379839239642024 Num fake examples 19280 Num true examples 20160\n",
      "  Batch 19,760  of  44,637.    Elapsed: 0:12:30. Training loss. 0.004138241522014141 Num fake examples 19321 Num true examples 20199\n",
      "  Batch 19,800  of  44,637.    Elapsed: 0:12:31. Training loss. 0.004174884408712387 Num fake examples 19362 Num true examples 20238\n",
      "  Batch 19,840  of  44,637.    Elapsed: 0:12:33. Training loss. 0.0028535486198961735 Num fake examples 19389 Num true examples 20291\n",
      "  Batch 19,880  of  44,637.    Elapsed: 0:12:34. Training loss. 0.002931843977421522 Num fake examples 19423 Num true examples 20337\n",
      "  Batch 19,920  of  44,637.    Elapsed: 0:12:36. Training loss. 0.004808480851352215 Num fake examples 19456 Num true examples 20384\n",
      "  Batch 19,960  of  44,637.    Elapsed: 0:12:37. Training loss. 0.0069111427292227745 Num fake examples 19497 Num true examples 20423\n",
      "  Batch 20,000  of  44,637.    Elapsed: 0:12:39. Training loss. 0.002235087566077709 Num fake examples 19527 Num true examples 20473\n",
      "  Batch 20,040  of  44,637.    Elapsed: 0:12:40. Training loss. 0.0038528055883944035 Num fake examples 19565 Num true examples 20515\n",
      "  Batch 20,080  of  44,637.    Elapsed: 0:12:42. Training loss. 0.004194386303424835 Num fake examples 19600 Num true examples 20560\n",
      "  Batch 20,120  of  44,637.    Elapsed: 0:12:43. Training loss. 0.0024908059276640415 Num fake examples 19644 Num true examples 20596\n",
      "  Batch 20,160  of  44,637.    Elapsed: 0:12:45. Training loss. 0.003977570682764053 Num fake examples 19683 Num true examples 20637\n",
      "  Batch 20,200  of  44,637.    Elapsed: 0:12:46. Training loss. 0.004568160045892 Num fake examples 19720 Num true examples 20680\n",
      "  Batch 20,240  of  44,637.    Elapsed: 0:12:48. Training loss. 0.0019401488825678825 Num fake examples 19763 Num true examples 20717\n",
      "  Batch 20,280  of  44,637.    Elapsed: 0:12:49. Training loss. 0.0027582645416259766 Num fake examples 19802 Num true examples 20758\n",
      "  Batch 20,320  of  44,637.    Elapsed: 0:12:51. Training loss. 0.0024917207192629576 Num fake examples 19848 Num true examples 20792\n",
      "  Batch 20,360  of  44,637.    Elapsed: 0:12:52. Training loss. 0.002426585415378213 Num fake examples 19890 Num true examples 20830\n",
      "  Batch 20,400  of  44,637.    Elapsed: 0:12:53. Training loss. 0.0032067177817225456 Num fake examples 19927 Num true examples 20873\n",
      "  Batch 20,440  of  44,637.    Elapsed: 0:12:55. Training loss. 0.004034820012748241 Num fake examples 19967 Num true examples 20913\n",
      "  Batch 20,480  of  44,637.    Elapsed: 0:12:56. Training loss. 2.872127056121826 Num fake examples 20009 Num true examples 20951\n",
      "  Batch 20,520  of  44,637.    Elapsed: 0:12:58. Training loss. 0.0041216337122023106 Num fake examples 20046 Num true examples 20994\n",
      "  Batch 20,560  of  44,637.    Elapsed: 0:12:59. Training loss. 0.004502683877944946 Num fake examples 20077 Num true examples 21043\n",
      "  Batch 20,600  of  44,637.    Elapsed: 0:13:01. Training loss. 0.0022476771846413612 Num fake examples 20111 Num true examples 21089\n",
      "  Batch 20,640  of  44,637.    Elapsed: 0:13:02. Training loss. 0.002467920770868659 Num fake examples 20152 Num true examples 21128\n",
      "  Batch 20,680  of  44,637.    Elapsed: 0:13:04. Training loss. 3.0120882987976074 Num fake examples 20193 Num true examples 21167\n",
      "  Batch 20,720  of  44,637.    Elapsed: 0:13:05. Training loss. 0.002856744220480323 Num fake examples 20225 Num true examples 21215\n",
      "  Batch 20,760  of  44,637.    Elapsed: 0:13:07. Training loss. 0.002646570559591055 Num fake examples 20264 Num true examples 21256\n",
      "  Batch 20,800  of  44,637.    Elapsed: 0:13:08. Training loss. 2.8459839820861816 Num fake examples 20301 Num true examples 21299\n",
      "  Batch 20,840  of  44,637.    Elapsed: 0:13:10. Training loss. 0.0030448974575847387 Num fake examples 20336 Num true examples 21344\n",
      "  Batch 20,880  of  44,637.    Elapsed: 0:13:11. Training loss. 0.00373368663713336 Num fake examples 20373 Num true examples 21387\n",
      "  Batch 20,920  of  44,637.    Elapsed: 0:13:13. Training loss. 0.0027426339220255613 Num fake examples 20410 Num true examples 21430\n",
      "  Batch 20,960  of  44,637.    Elapsed: 0:13:14. Training loss. 0.0039402879774570465 Num fake examples 20453 Num true examples 21467\n",
      "  Batch 21,000  of  44,637.    Elapsed: 0:13:16. Training loss. 0.0034091854467988014 Num fake examples 20494 Num true examples 21506\n",
      "  Batch 21,040  of  44,637.    Elapsed: 0:13:17. Training loss. 0.00264287693426013 Num fake examples 20530 Num true examples 21550\n",
      "  Batch 21,080  of  44,637.    Elapsed: 0:13:18. Training loss. 0.00298476405441761 Num fake examples 20568 Num true examples 21592\n",
      "  Batch 21,120  of  44,637.    Elapsed: 0:13:20. Training loss. 0.004308751318603754 Num fake examples 20604 Num true examples 21636\n",
      "  Batch 21,160  of  44,637.    Elapsed: 0:13:21. Training loss. 0.0031456039287149906 Num fake examples 20646 Num true examples 21674\n",
      "  Batch 21,200  of  44,637.    Elapsed: 0:13:23. Training loss. 0.0030144660267978907 Num fake examples 20688 Num true examples 21712\n",
      "  Batch 21,240  of  44,637.    Elapsed: 0:13:24. Training loss. 0.002386125735938549 Num fake examples 20720 Num true examples 21760\n",
      "  Batch 21,280  of  44,637.    Elapsed: 0:13:26. Training loss. 0.0028875726275146008 Num fake examples 20757 Num true examples 21803\n",
      "  Batch 21,320  of  44,637.    Elapsed: 0:13:27. Training loss. 0.004154345020651817 Num fake examples 20797 Num true examples 21843\n",
      "  Batch 21,360  of  44,637.    Elapsed: 0:13:29. Training loss. 0.0031142383813858032 Num fake examples 20836 Num true examples 21884\n",
      "  Batch 21,400  of  44,637.    Elapsed: 0:13:30. Training loss. 0.0026825186796486378 Num fake examples 20878 Num true examples 21922\n",
      "  Batch 21,440  of  44,637.    Elapsed: 0:13:32. Training loss. 0.002373920753598213 Num fake examples 20928 Num true examples 21952\n",
      "  Batch 21,480  of  44,637.    Elapsed: 0:13:33. Training loss. 0.001980123110115528 Num fake examples 20970 Num true examples 21990\n",
      "  Batch 21,520  of  44,637.    Elapsed: 0:13:35. Training loss. 0.0021279549691826105 Num fake examples 21007 Num true examples 22033\n",
      "  Batch 21,560  of  44,637.    Elapsed: 0:13:36. Training loss. 0.0022252071648836136 Num fake examples 21044 Num true examples 22076\n",
      "  Batch 21,600  of  44,637.    Elapsed: 0:13:38. Training loss. 0.0024699349887669086 Num fake examples 21077 Num true examples 22123\n",
      "  Batch 21,640  of  44,637.    Elapsed: 0:13:39. Training loss. 0.0022476576268672943 Num fake examples 21120 Num true examples 22160\n",
      "  Batch 21,680  of  44,637.    Elapsed: 0:13:41. Training loss. 0.002522579859942198 Num fake examples 21160 Num true examples 22200\n",
      "  Batch 21,720  of  44,637.    Elapsed: 0:13:42. Training loss. 0.002006431110203266 Num fake examples 21209 Num true examples 22231\n",
      "  Batch 21,760  of  44,637.    Elapsed: 0:13:43. Training loss. 0.00308482488617301 Num fake examples 21253 Num true examples 22267\n",
      "  Batch 21,800  of  44,637.    Elapsed: 0:13:45. Training loss. 0.003639357164502144 Num fake examples 21291 Num true examples 22309\n",
      "  Batch 21,840  of  44,637.    Elapsed: 0:13:46. Training loss. 0.0029835435561835766 Num fake examples 21333 Num true examples 22347\n",
      "  Batch 21,880  of  44,637.    Elapsed: 0:13:48. Training loss. 0.004474678076803684 Num fake examples 21367 Num true examples 22393\n",
      "  Batch 21,920  of  44,637.    Elapsed: 0:13:49. Training loss. 0.0030320819932967424 Num fake examples 21411 Num true examples 22429\n",
      "  Batch 21,960  of  44,637.    Elapsed: 0:13:51. Training loss. 0.0025978186167776585 Num fake examples 21450 Num true examples 22470\n",
      "  Batch 22,000  of  44,637.    Elapsed: 0:13:52. Training loss. 0.0028020357713103294 Num fake examples 21487 Num true examples 22513\n",
      "  Batch 22,040  of  44,637.    Elapsed: 0:13:54. Training loss. 0.002754693850874901 Num fake examples 21524 Num true examples 22556\n",
      "  Batch 22,080  of  44,637.    Elapsed: 0:13:55. Training loss. 0.002930547809228301 Num fake examples 21559 Num true examples 22601\n",
      "  Batch 22,120  of  44,637.    Elapsed: 0:13:57. Training loss. 0.007572627626359463 Num fake examples 21597 Num true examples 22643\n",
      "  Batch 22,160  of  44,637.    Elapsed: 0:13:58. Training loss. 0.004751352593302727 Num fake examples 21639 Num true examples 22681\n",
      "  Batch 22,200  of  44,637.    Elapsed: 0:14:00. Training loss. 0.005766821559518576 Num fake examples 21681 Num true examples 22719\n",
      "  Batch 22,240  of  44,637.    Elapsed: 0:14:01. Training loss. 0.006256457883864641 Num fake examples 21710 Num true examples 22770\n",
      "  Batch 22,280  of  44,637.    Elapsed: 0:14:03. Training loss. 0.0047256662510335445 Num fake examples 21750 Num true examples 22810\n",
      "  Batch 22,320  of  44,637.    Elapsed: 0:14:04. Training loss. 0.00640262383967638 Num fake examples 21784 Num true examples 22856\n",
      "  Batch 22,360  of  44,637.    Elapsed: 0:14:05. Training loss. 0.0043580131605267525 Num fake examples 21817 Num true examples 22903\n",
      "  Batch 22,400  of  44,637.    Elapsed: 0:14:07. Training loss. 0.0029149367474019527 Num fake examples 21854 Num true examples 22946\n",
      "  Batch 22,440  of  44,637.    Elapsed: 0:14:08. Training loss. 0.0037776404060423374 Num fake examples 21893 Num true examples 22987\n",
      "  Batch 22,480  of  44,637.    Elapsed: 0:14:10. Training loss. 0.0043527791276574135 Num fake examples 21928 Num true examples 23032\n",
      "  Batch 22,520  of  44,637.    Elapsed: 0:14:11. Training loss. 0.007765540853142738 Num fake examples 21962 Num true examples 23078\n",
      "  Batch 22,560  of  44,637.    Elapsed: 0:14:13. Training loss. 0.0028509623371064663 Num fake examples 21997 Num true examples 23123\n",
      "  Batch 22,600  of  44,637.    Elapsed: 0:14:14. Training loss. 0.002968955785036087 Num fake examples 22032 Num true examples 23168\n",
      "  Batch 22,640  of  44,637.    Elapsed: 0:14:16. Training loss. 0.00358730461448431 Num fake examples 22078 Num true examples 23202\n",
      "  Batch 22,680  of  44,637.    Elapsed: 0:14:17. Training loss. 0.002811596030369401 Num fake examples 22109 Num true examples 23251\n",
      "  Batch 22,720  of  44,637.    Elapsed: 0:14:19. Training loss. 0.0027362010441720486 Num fake examples 22151 Num true examples 23289\n",
      "  Batch 22,760  of  44,637.    Elapsed: 0:14:20. Training loss. 0.0019150669686496258 Num fake examples 22190 Num true examples 23330\n",
      "  Batch 22,800  of  44,637.    Elapsed: 0:14:22. Training loss. 0.002855801023542881 Num fake examples 22228 Num true examples 23372\n",
      "  Batch 22,840  of  44,637.    Elapsed: 0:14:23. Training loss. 0.002485301112756133 Num fake examples 22274 Num true examples 23406\n",
      "  Batch 22,880  of  44,637.    Elapsed: 0:14:25. Training loss. 0.0019156859489157796 Num fake examples 22315 Num true examples 23445\n",
      "  Batch 22,920  of  44,637.    Elapsed: 0:14:26. Training loss. 0.0014779000775888562 Num fake examples 22356 Num true examples 23484\n",
      "  Batch 22,960  of  44,637.    Elapsed: 0:14:28. Training loss. 0.002044857945293188 Num fake examples 22393 Num true examples 23527\n",
      "  Batch 23,000  of  44,637.    Elapsed: 0:14:29. Training loss. 0.0039896597154438496 Num fake examples 22440 Num true examples 23560\n",
      "  Batch 23,040  of  44,637.    Elapsed: 0:14:30. Training loss. 0.003698822110891342 Num fake examples 22475 Num true examples 23605\n",
      "  Batch 23,080  of  44,637.    Elapsed: 0:14:32. Training loss. 0.004961056634783745 Num fake examples 22512 Num true examples 23648\n",
      "  Batch 23,120  of  44,637.    Elapsed: 0:14:33. Training loss. 0.0048663429915905 Num fake examples 22545 Num true examples 23695\n",
      "  Batch 23,160  of  44,637.    Elapsed: 0:14:35. Training loss. 0.003438531653955579 Num fake examples 22579 Num true examples 23741\n",
      "  Batch 23,200  of  44,637.    Elapsed: 0:14:36. Training loss. 0.002586127258837223 Num fake examples 22608 Num true examples 23792\n",
      "  Batch 23,240  of  44,637.    Elapsed: 0:14:38. Training loss. 0.0025478212628513575 Num fake examples 22637 Num true examples 23843\n",
      "  Batch 23,280  of  44,637.    Elapsed: 0:14:39. Training loss. 0.0021629719994962215 Num fake examples 22679 Num true examples 23881\n",
      "  Batch 23,320  of  44,637.    Elapsed: 0:14:41. Training loss. 0.002255448140203953 Num fake examples 22726 Num true examples 23914\n",
      "  Batch 23,360  of  44,637.    Elapsed: 0:14:42. Training loss. 0.0027435016818344593 Num fake examples 22763 Num true examples 23957\n",
      "  Batch 23,400  of  44,637.    Elapsed: 0:14:44. Training loss. 0.0024968183133751154 Num fake examples 22799 Num true examples 24001\n",
      "  Batch 23,440  of  44,637.    Elapsed: 0:14:45. Training loss. 0.0021968986839056015 Num fake examples 22839 Num true examples 24041\n",
      "  Batch 23,480  of  44,637.    Elapsed: 0:14:47. Training loss. 2.9673383235931396 Num fake examples 22876 Num true examples 24084\n",
      "  Batch 23,520  of  44,637.    Elapsed: 0:14:48. Training loss. 0.003341532777994871 Num fake examples 22918 Num true examples 24122\n",
      "  Batch 23,560  of  44,637.    Elapsed: 0:14:50. Training loss. 0.0026619776617735624 Num fake examples 22954 Num true examples 24166\n",
      "  Batch 23,600  of  44,637.    Elapsed: 0:14:51. Training loss. 0.0020112127531319857 Num fake examples 22993 Num true examples 24207\n",
      "  Batch 23,640  of  44,637.    Elapsed: 0:14:53. Training loss. 0.002697254531085491 Num fake examples 23030 Num true examples 24250\n",
      "  Batch 23,680  of  44,637.    Elapsed: 0:14:54. Training loss. 0.0021960935555398464 Num fake examples 23065 Num true examples 24295\n",
      "  Batch 23,720  of  44,637.    Elapsed: 0:14:56. Training loss. 0.0024484184104949236 Num fake examples 23103 Num true examples 24337\n",
      "  Batch 23,760  of  44,637.    Elapsed: 0:14:57. Training loss. 0.0023939330130815506 Num fake examples 23144 Num true examples 24376\n",
      "  Batch 23,800  of  44,637.    Elapsed: 0:14:59. Training loss. 0.002161568496376276 Num fake examples 23193 Num true examples 24407\n",
      "  Batch 23,840  of  44,637.    Elapsed: 0:15:00. Training loss. 0.002655474469065666 Num fake examples 23232 Num true examples 24448\n",
      "  Batch 23,880  of  44,637.    Elapsed: 0:15:02. Training loss. 0.0031504875514656305 Num fake examples 23276 Num true examples 24484\n",
      "  Batch 23,920  of  44,637.    Elapsed: 0:15:03. Training loss. 0.004461769014596939 Num fake examples 23317 Num true examples 24523\n",
      "  Batch 23,960  of  44,637.    Elapsed: 0:15:05. Training loss. 0.0029575680382549763 Num fake examples 23358 Num true examples 24562\n",
      "  Batch 24,000  of  44,637.    Elapsed: 0:15:06. Training loss. 0.004027280490845442 Num fake examples 23393 Num true examples 24607\n",
      "  Batch 24,040  of  44,637.    Elapsed: 0:15:07. Training loss. 0.004329496994614601 Num fake examples 23431 Num true examples 24649\n",
      "  Batch 24,080  of  44,637.    Elapsed: 0:15:09. Training loss. 0.0034734467044472694 Num fake examples 23471 Num true examples 24689\n",
      "  Batch 24,120  of  44,637.    Elapsed: 0:15:10. Training loss. 0.0035951281897723675 Num fake examples 23511 Num true examples 24729\n",
      "  Batch 24,160  of  44,637.    Elapsed: 0:15:12. Training loss. 0.0031003940384835005 Num fake examples 23544 Num true examples 24776\n",
      "  Batch 24,200  of  44,637.    Elapsed: 0:15:13. Training loss. 0.005412141792476177 Num fake examples 23586 Num true examples 24814\n",
      "  Batch 24,240  of  44,637.    Elapsed: 0:15:15. Training loss. 2.9152121543884277 Num fake examples 23623 Num true examples 24857\n",
      "  Batch 24,280  of  44,637.    Elapsed: 0:15:16. Training loss. 0.004097898490726948 Num fake examples 23656 Num true examples 24904\n",
      "  Batch 24,320  of  44,637.    Elapsed: 0:15:18. Training loss. 0.00370253948494792 Num fake examples 23694 Num true examples 24946\n",
      "  Batch 24,360  of  44,637.    Elapsed: 0:15:19. Training loss. 0.0025065159425139427 Num fake examples 23734 Num true examples 24986\n",
      "  Batch 24,400  of  44,637.    Elapsed: 0:15:21. Training loss. 2.9036810398101807 Num fake examples 23768 Num true examples 25032\n",
      "  Batch 24,440  of  44,637.    Elapsed: 0:15:22. Training loss. 0.0027845141012221575 Num fake examples 23814 Num true examples 25066\n",
      "  Batch 24,480  of  44,637.    Elapsed: 0:15:24. Training loss. 0.003136175684630871 Num fake examples 23854 Num true examples 25106\n",
      "  Batch 24,520  of  44,637.    Elapsed: 0:15:26. Training loss. 0.003173230681568384 Num fake examples 23897 Num true examples 25143\n",
      "  Batch 24,560  of  44,637.    Elapsed: 0:15:27. Training loss. 0.0027379754465073347 Num fake examples 23933 Num true examples 25187\n",
      "  Batch 24,600  of  44,637.    Elapsed: 0:15:29. Training loss. 0.0036682267673313618 Num fake examples 23960 Num true examples 25240\n",
      "  Batch 24,640  of  44,637.    Elapsed: 0:15:31. Training loss. 0.0040165213868021965 Num fake examples 23992 Num true examples 25288\n",
      "  Batch 24,680  of  44,637.    Elapsed: 0:15:32. Training loss. 0.0039534843526780605 Num fake examples 24031 Num true examples 25329\n",
      "  Batch 24,720  of  44,637.    Elapsed: 0:15:34. Training loss. 0.0037079902831465006 Num fake examples 24069 Num true examples 25371\n",
      "  Batch 24,760  of  44,637.    Elapsed: 0:15:35. Training loss. 0.0028351987712085247 Num fake examples 24101 Num true examples 25419\n",
      "  Batch 24,800  of  44,637.    Elapsed: 0:15:37. Training loss. 0.003299043281003833 Num fake examples 24141 Num true examples 25459\n",
      "  Batch 24,840  of  44,637.    Elapsed: 0:15:39. Training loss. 0.002594747580587864 Num fake examples 24177 Num true examples 25503\n",
      "  Batch 24,880  of  44,637.    Elapsed: 0:15:40. Training loss. 0.0041274018585681915 Num fake examples 24222 Num true examples 25538\n",
      "  Batch 24,920  of  44,637.    Elapsed: 0:15:42. Training loss. 0.004451248794794083 Num fake examples 24257 Num true examples 25583\n",
      "  Batch 24,960  of  44,637.    Elapsed: 0:15:44. Training loss. 0.0043961722403764725 Num fake examples 24288 Num true examples 25632\n",
      "  Batch 25,000  of  44,637.    Elapsed: 0:15:45. Training loss. 0.003515295684337616 Num fake examples 24327 Num true examples 25673\n",
      "  Batch 25,040  of  44,637.    Elapsed: 0:15:47. Training loss. 0.0035898620262742043 Num fake examples 24376 Num true examples 25704\n",
      "  Batch 25,080  of  44,637.    Elapsed: 0:15:48. Training loss. 0.0034726797603070736 Num fake examples 24416 Num true examples 25744\n",
      "  Batch 25,120  of  44,637.    Elapsed: 0:15:50. Training loss. 0.004598097875714302 Num fake examples 24454 Num true examples 25786\n",
      "  Batch 25,160  of  44,637.    Elapsed: 0:15:51. Training loss. 0.0031427708454430103 Num fake examples 24480 Num true examples 25840\n",
      "  Batch 25,200  of  44,637.    Elapsed: 0:15:53. Training loss. 0.00341775082051754 Num fake examples 24517 Num true examples 25883\n",
      "  Batch 25,240  of  44,637.    Elapsed: 0:15:54. Training loss. 0.0028625805862247944 Num fake examples 24552 Num true examples 25928\n",
      "  Batch 25,280  of  44,637.    Elapsed: 0:15:56. Training loss. 0.0028236894868314266 Num fake examples 24591 Num true examples 25969\n",
      "  Batch 25,320  of  44,637.    Elapsed: 0:15:58. Training loss. 0.003228115849196911 Num fake examples 24627 Num true examples 26013\n",
      "  Batch 25,360  of  44,637.    Elapsed: 0:15:59. Training loss. 0.0028625656850636005 Num fake examples 24667 Num true examples 26053\n",
      "  Batch 25,400  of  44,637.    Elapsed: 0:16:00. Training loss. 0.0026411726139485836 Num fake examples 24707 Num true examples 26093\n",
      "  Batch 25,440  of  44,637.    Elapsed: 0:16:02. Training loss. 0.0029387695249170065 Num fake examples 24734 Num true examples 26146\n",
      "  Batch 25,480  of  44,637.    Elapsed: 0:16:03. Training loss. 0.003957442007958889 Num fake examples 24763 Num true examples 26197\n",
      "  Batch 25,520  of  44,637.    Elapsed: 0:16:05. Training loss. 0.0028780228458344936 Num fake examples 24798 Num true examples 26242\n",
      "  Batch 25,560  of  44,637.    Elapsed: 0:16:06. Training loss. 0.002437958028167486 Num fake examples 24837 Num true examples 26283\n",
      "  Batch 25,600  of  44,637.    Elapsed: 0:16:08. Training loss. 0.00222982931882143 Num fake examples 24883 Num true examples 26317\n",
      "  Batch 25,640  of  44,637.    Elapsed: 0:16:09. Training loss. 0.004104306921362877 Num fake examples 24923 Num true examples 26357\n",
      "  Batch 25,680  of  44,637.    Elapsed: 0:16:11. Training loss. 0.002869575284421444 Num fake examples 24954 Num true examples 26406\n",
      "  Batch 25,720  of  44,637.    Elapsed: 0:16:12. Training loss. 0.002657859120517969 Num fake examples 24993 Num true examples 26447\n",
      "  Batch 25,760  of  44,637.    Elapsed: 0:16:14. Training loss. 0.002270693890750408 Num fake examples 25036 Num true examples 26484\n",
      "  Batch 25,800  of  44,637.    Elapsed: 0:16:15. Training loss. 0.002260108944028616 Num fake examples 25072 Num true examples 26528\n",
      "  Batch 25,840  of  44,637.    Elapsed: 0:16:17. Training loss. 0.0019411541288718581 Num fake examples 25113 Num true examples 26567\n",
      "  Batch 25,880  of  44,637.    Elapsed: 0:16:18. Training loss. 0.0034408876672387123 Num fake examples 25145 Num true examples 26615\n",
      "  Batch 25,920  of  44,637.    Elapsed: 0:16:20. Training loss. 0.0025405725464224815 Num fake examples 25181 Num true examples 26659\n",
      "  Batch 25,960  of  44,637.    Elapsed: 0:16:21. Training loss. 3.151334285736084 Num fake examples 25225 Num true examples 26695\n",
      "  Batch 26,000  of  44,637.    Elapsed: 0:16:22. Training loss. 0.002946537220850587 Num fake examples 25268 Num true examples 26732\n",
      "  Batch 26,040  of  44,637.    Elapsed: 0:16:24. Training loss. 0.002323143882676959 Num fake examples 25303 Num true examples 26777\n",
      "  Batch 26,080  of  44,637.    Elapsed: 0:16:26. Training loss. 0.002159006427973509 Num fake examples 25352 Num true examples 26808\n",
      "  Batch 26,120  of  44,637.    Elapsed: 0:16:27. Training loss. 0.003526626154780388 Num fake examples 25393 Num true examples 26847\n",
      "  Batch 26,160  of  44,637.    Elapsed: 0:16:29. Training loss. 0.002596670761704445 Num fake examples 25435 Num true examples 26885\n",
      "  Batch 26,200  of  44,637.    Elapsed: 0:16:30. Training loss. 0.002247251570224762 Num fake examples 25468 Num true examples 26932\n",
      "  Batch 26,240  of  44,637.    Elapsed: 0:16:32. Training loss. 0.0024422891438007355 Num fake examples 25511 Num true examples 26969\n",
      "  Batch 26,280  of  44,637.    Elapsed: 0:16:34. Training loss. 0.00256545003503561 Num fake examples 25547 Num true examples 27013\n",
      "  Batch 26,320  of  44,637.    Elapsed: 0:16:35. Training loss. 0.0028583058156073093 Num fake examples 25592 Num true examples 27048\n",
      "  Batch 26,360  of  44,637.    Elapsed: 0:16:37. Training loss. 0.00301623553968966 Num fake examples 25632 Num true examples 27088\n",
      "  Batch 26,400  of  44,637.    Elapsed: 0:16:38. Training loss. 0.002766014076769352 Num fake examples 25673 Num true examples 27127\n",
      "  Batch 26,440  of  44,637.    Elapsed: 0:16:40. Training loss. 0.003377871587872505 Num fake examples 25714 Num true examples 27166\n",
      "  Batch 26,480  of  44,637.    Elapsed: 0:16:41. Training loss. 0.004714675713330507 Num fake examples 25758 Num true examples 27202\n",
      "  Batch 26,520  of  44,637.    Elapsed: 0:16:43. Training loss. 0.003174885641783476 Num fake examples 25795 Num true examples 27245\n",
      "  Batch 26,560  of  44,637.    Elapsed: 0:16:44. Training loss. 0.004879684187471867 Num fake examples 25831 Num true examples 27289\n",
      "  Batch 26,600  of  44,637.    Elapsed: 0:16:46. Training loss. 2.81717586517334 Num fake examples 25872 Num true examples 27328\n",
      "  Batch 26,640  of  44,637.    Elapsed: 0:16:48. Training loss. 0.0026694361586123705 Num fake examples 25920 Num true examples 27360\n",
      "  Batch 26,680  of  44,637.    Elapsed: 0:16:49. Training loss. 0.0028788787312805653 Num fake examples 25962 Num true examples 27398\n",
      "  Batch 26,720  of  44,637.    Elapsed: 0:16:51. Training loss. 0.004132529255002737 Num fake examples 25999 Num true examples 27441\n",
      "  Batch 26,760  of  44,637.    Elapsed: 0:16:53. Training loss. 0.0027520060539245605 Num fake examples 26040 Num true examples 27480\n",
      "  Batch 26,800  of  44,637.    Elapsed: 0:16:54. Training loss. 0.0017641090089455247 Num fake examples 26078 Num true examples 27522\n",
      "  Batch 26,840  of  44,637.    Elapsed: 0:16:56. Training loss. 0.002901385072618723 Num fake examples 26118 Num true examples 27562\n",
      "  Batch 26,880  of  44,637.    Elapsed: 0:16:57. Training loss. 0.00245444243773818 Num fake examples 26154 Num true examples 27606\n",
      "  Batch 26,920  of  44,637.    Elapsed: 0:16:59. Training loss. 0.003703382797539234 Num fake examples 26190 Num true examples 27650\n",
      "  Batch 26,960  of  44,637.    Elapsed: 0:17:00. Training loss. 0.004225488286465406 Num fake examples 26226 Num true examples 27694\n",
      "  Batch 27,000  of  44,637.    Elapsed: 0:17:02. Training loss. 0.004736254923045635 Num fake examples 26264 Num true examples 27736\n",
      "  Batch 27,040  of  44,637.    Elapsed: 0:17:03. Training loss. 0.0034805499017238617 Num fake examples 26295 Num true examples 27785\n",
      "  Batch 27,080  of  44,637.    Elapsed: 0:17:05. Training loss. 0.003939881920814514 Num fake examples 26337 Num true examples 27823\n",
      "  Batch 27,120  of  44,637.    Elapsed: 0:17:06. Training loss. 2.5737133026123047 Num fake examples 26376 Num true examples 27864\n",
      "  Batch 27,160  of  44,637.    Elapsed: 0:17:08. Training loss. 0.006752317305654287 Num fake examples 26409 Num true examples 27911\n",
      "  Batch 27,200  of  44,637.    Elapsed: 0:17:09. Training loss. 2.9928059577941895 Num fake examples 26450 Num true examples 27950\n",
      "  Batch 27,240  of  44,637.    Elapsed: 0:17:11. Training loss. 0.005213805008679628 Num fake examples 26487 Num true examples 27993\n",
      "  Batch 27,280  of  44,637.    Elapsed: 0:17:12. Training loss. 0.004795044660568237 Num fake examples 26522 Num true examples 28038\n",
      "  Batch 27,320  of  44,637.    Elapsed: 0:17:14. Training loss. 0.003849187633022666 Num fake examples 26556 Num true examples 28084\n",
      "  Batch 27,360  of  44,637.    Elapsed: 0:17:15. Training loss. 0.002961505204439163 Num fake examples 26591 Num true examples 28129\n",
      "  Batch 27,400  of  44,637.    Elapsed: 0:17:16. Training loss. 0.0033343927934765816 Num fake examples 26634 Num true examples 28166\n",
      "  Batch 27,440  of  44,637.    Elapsed: 0:17:18. Training loss. 0.003243805607780814 Num fake examples 26672 Num true examples 28208\n",
      "  Batch 27,480  of  44,637.    Elapsed: 0:17:19. Training loss. 0.002869531512260437 Num fake examples 26714 Num true examples 28246\n",
      "  Batch 27,520  of  44,637.    Elapsed: 0:17:21. Training loss. 0.002982883481308818 Num fake examples 26752 Num true examples 28288\n",
      "  Batch 27,560  of  44,637.    Elapsed: 0:17:23. Training loss. 0.0021082062739878893 Num fake examples 26786 Num true examples 28334\n",
      "  Batch 27,600  of  44,637.    Elapsed: 0:17:24. Training loss. 0.0019999814685434103 Num fake examples 26825 Num true examples 28375\n",
      "  Batch 27,640  of  44,637.    Elapsed: 0:17:26. Training loss. 0.0019191665342077613 Num fake examples 26866 Num true examples 28414\n",
      "  Batch 27,680  of  44,637.    Elapsed: 0:17:27. Training loss. 0.0026601513382047415 Num fake examples 26905 Num true examples 28455\n",
      "  Batch 27,720  of  44,637.    Elapsed: 0:17:29. Training loss. 0.0029163036961108446 Num fake examples 26944 Num true examples 28496\n",
      "  Batch 27,760  of  44,637.    Elapsed: 0:17:30. Training loss. 0.0032552224583923817 Num fake examples 26972 Num true examples 28548\n",
      "  Batch 27,800  of  44,637.    Elapsed: 0:17:32. Training loss. 0.0035395051818341017 Num fake examples 27013 Num true examples 28587\n",
      "  Batch 27,840  of  44,637.    Elapsed: 0:17:33. Training loss. 0.0031019297894090414 Num fake examples 27065 Num true examples 28615\n",
      "  Batch 27,880  of  44,637.    Elapsed: 0:17:35. Training loss. 0.003912008833140135 Num fake examples 27097 Num true examples 28663\n",
      "  Batch 27,920  of  44,637.    Elapsed: 0:17:36. Training loss. 0.005326511804014444 Num fake examples 27135 Num true examples 28705\n",
      "  Batch 27,960  of  44,637.    Elapsed: 0:17:38. Training loss. 0.002785456134006381 Num fake examples 27177 Num true examples 28743\n",
      "  Batch 28,000  of  44,637.    Elapsed: 0:17:39. Training loss. 0.002401496982201934 Num fake examples 27215 Num true examples 28785\n",
      "  Batch 28,040  of  44,637.    Elapsed: 0:17:41. Training loss. 0.003054647706449032 Num fake examples 27259 Num true examples 28821\n",
      "  Batch 28,080  of  44,637.    Elapsed: 0:17:42. Training loss. 0.003899041097611189 Num fake examples 27292 Num true examples 28868\n",
      "  Batch 28,120  of  44,637.    Elapsed: 0:17:44. Training loss. 0.0034733754582703114 Num fake examples 27331 Num true examples 28909\n",
      "  Batch 28,160  of  44,637.    Elapsed: 0:17:45. Training loss. 0.0028075638692826033 Num fake examples 27374 Num true examples 28946\n",
      "  Batch 28,200  of  44,637.    Elapsed: 0:17:47. Training loss. 0.0018208605470135808 Num fake examples 27409 Num true examples 28991\n",
      "  Batch 28,240  of  44,637.    Elapsed: 0:17:48. Training loss. 0.0024081291630864143 Num fake examples 27449 Num true examples 29031\n",
      "  Batch 28,280  of  44,637.    Elapsed: 0:17:49. Training loss. 0.0038673526141792536 Num fake examples 27483 Num true examples 29077\n",
      "  Batch 28,320  of  44,637.    Elapsed: 0:17:51. Training loss. 0.001902509480714798 Num fake examples 27521 Num true examples 29119\n",
      "  Batch 28,360  of  44,637.    Elapsed: 0:17:52. Training loss. 0.002454507863149047 Num fake examples 27563 Num true examples 29157\n",
      "  Batch 28,400  of  44,637.    Elapsed: 0:17:54. Training loss. 0.0037153824232518673 Num fake examples 27601 Num true examples 29199\n",
      "  Batch 28,440  of  44,637.    Elapsed: 0:17:55. Training loss. 0.004327256698161364 Num fake examples 27636 Num true examples 29244\n",
      "  Batch 28,480  of  44,637.    Elapsed: 0:17:57. Training loss. 0.002238525077700615 Num fake examples 27678 Num true examples 29282\n",
      "  Batch 28,520  of  44,637.    Elapsed: 0:17:58. Training loss. 0.002092908136546612 Num fake examples 27715 Num true examples 29325\n",
      "  Batch 28,560  of  44,637.    Elapsed: 0:18:00. Training loss. 0.002443540608510375 Num fake examples 27753 Num true examples 29367\n",
      "  Batch 28,600  of  44,637.    Elapsed: 0:18:01. Training loss. 0.0020018063951283693 Num fake examples 27800 Num true examples 29400\n",
      "  Batch 28,640  of  44,637.    Elapsed: 0:18:03. Training loss. 0.00252439989708364 Num fake examples 27840 Num true examples 29440\n",
      "  Batch 28,680  of  44,637.    Elapsed: 0:18:04. Training loss. 0.0029895673505961895 Num fake examples 27873 Num true examples 29487\n",
      "  Batch 28,720  of  44,637.    Elapsed: 0:18:06. Training loss. 0.0027418979443609715 Num fake examples 27911 Num true examples 29529\n",
      "  Batch 28,760  of  44,637.    Elapsed: 0:18:07. Training loss. 0.0025056302547454834 Num fake examples 27952 Num true examples 29568\n",
      "  Batch 28,800  of  44,637.    Elapsed: 0:18:08. Training loss. 0.0027981188613921404 Num fake examples 27989 Num true examples 29611\n",
      "  Batch 28,840  of  44,637.    Elapsed: 0:18:10. Training loss. 0.0018630410777404904 Num fake examples 28015 Num true examples 29665\n",
      "  Batch 28,880  of  44,637.    Elapsed: 0:18:11. Training loss. 0.002408272121101618 Num fake examples 28064 Num true examples 29696\n",
      "  Batch 28,920  of  44,637.    Elapsed: 0:18:13. Training loss. 0.00221461895853281 Num fake examples 28108 Num true examples 29732\n",
      "  Batch 28,960  of  44,637.    Elapsed: 0:18:14. Training loss. 0.0026373842265456915 Num fake examples 28146 Num true examples 29774\n",
      "  Batch 29,000  of  44,637.    Elapsed: 0:18:16. Training loss. 0.002441281685605645 Num fake examples 28185 Num true examples 29815\n",
      "  Batch 29,040  of  44,637.    Elapsed: 0:18:17. Training loss. 0.00221479800529778 Num fake examples 28227 Num true examples 29853\n",
      "  Batch 29,080  of  44,637.    Elapsed: 0:18:19. Training loss. 0.002208968624472618 Num fake examples 28260 Num true examples 29900\n",
      "  Batch 29,120  of  44,637.    Elapsed: 0:18:20. Training loss. 0.0023329833056777716 Num fake examples 28294 Num true examples 29946\n",
      "  Batch 29,160  of  44,637.    Elapsed: 0:18:22. Training loss. 0.0023300410248339176 Num fake examples 28334 Num true examples 29986\n",
      "  Batch 29,200  of  44,637.    Elapsed: 0:18:23. Training loss. 0.0016884524375200272 Num fake examples 28375 Num true examples 30025\n",
      "  Batch 29,240  of  44,637.    Elapsed: 0:18:25. Training loss. 0.002096803393214941 Num fake examples 28415 Num true examples 30065\n",
      "  Batch 29,280  of  44,637.    Elapsed: 0:18:26. Training loss. 0.0033819281961768866 Num fake examples 28450 Num true examples 30110\n",
      "  Batch 29,320  of  44,637.    Elapsed: 0:18:28. Training loss. 0.002536424668505788 Num fake examples 28499 Num true examples 30141\n",
      "  Batch 29,360  of  44,637.    Elapsed: 0:18:29. Training loss. 0.0034597930498421192 Num fake examples 28537 Num true examples 30183\n",
      "  Batch 29,400  of  44,637.    Elapsed: 0:18:30. Training loss. 0.002926426474004984 Num fake examples 28578 Num true examples 30222\n",
      "  Batch 29,440  of  44,637.    Elapsed: 0:18:32. Training loss. 3.0932366847991943 Num fake examples 28619 Num true examples 30261\n",
      "  Batch 29,480  of  44,637.    Elapsed: 0:18:33. Training loss. 0.0031788507476449013 Num fake examples 28652 Num true examples 30308\n",
      "  Batch 29,520  of  44,637.    Elapsed: 0:18:35. Training loss. 0.003292966401204467 Num fake examples 28689 Num true examples 30351\n",
      "  Batch 29,560  of  44,637.    Elapsed: 0:18:36. Training loss. 0.0020481611136347055 Num fake examples 28728 Num true examples 30392\n",
      "  Batch 29,600  of  44,637.    Elapsed: 0:18:38. Training loss. 3.234104871749878 Num fake examples 28760 Num true examples 30440\n",
      "  Batch 29,640  of  44,637.    Elapsed: 0:18:39. Training loss. 0.0030607087537646294 Num fake examples 28799 Num true examples 30481\n",
      "  Batch 29,680  of  44,637.    Elapsed: 0:18:41. Training loss. 0.004233694635331631 Num fake examples 28840 Num true examples 30520\n",
      "  Batch 29,720  of  44,637.    Elapsed: 0:18:42. Training loss. 0.0035090430174022913 Num fake examples 28881 Num true examples 30559\n",
      "  Batch 29,760  of  44,637.    Elapsed: 0:18:44. Training loss. 0.004220008850097656 Num fake examples 28922 Num true examples 30598\n",
      "  Batch 29,800  of  44,637.    Elapsed: 0:18:45. Training loss. 0.003783460007980466 Num fake examples 28965 Num true examples 30635\n",
      "  Batch 29,840  of  44,637.    Elapsed: 0:18:47. Training loss. 0.004850631579756737 Num fake examples 29009 Num true examples 30671\n",
      "  Batch 29,880  of  44,637.    Elapsed: 0:18:48. Training loss. 0.003867893014103174 Num fake examples 29047 Num true examples 30713\n",
      "  Batch 29,920  of  44,637.    Elapsed: 0:18:49. Training loss. 0.002872733399271965 Num fake examples 29092 Num true examples 30748\n",
      "  Batch 29,960  of  44,637.    Elapsed: 0:18:51. Training loss. 2.93123722076416 Num fake examples 29130 Num true examples 30790\n",
      "  Batch 30,000  of  44,637.    Elapsed: 0:18:52. Training loss. 0.0050894347950816154 Num fake examples 29176 Num true examples 30824\n",
      "  Batch 30,040  of  44,637.    Elapsed: 0:18:54. Training loss. 0.003803593572229147 Num fake examples 29221 Num true examples 30859\n",
      "  Batch 30,080  of  44,637.    Elapsed: 0:18:55. Training loss. 0.0035354793071746826 Num fake examples 29261 Num true examples 30899\n",
      "  Batch 30,120  of  44,637.    Elapsed: 0:18:57. Training loss. 0.0025747139006853104 Num fake examples 29300 Num true examples 30940\n",
      "  Batch 30,160  of  44,637.    Elapsed: 0:18:58. Training loss. 0.004410823341459036 Num fake examples 29345 Num true examples 30975\n",
      "  Batch 30,200  of  44,637.    Elapsed: 0:19:00. Training loss. 0.003516050986945629 Num fake examples 29377 Num true examples 31023\n",
      "  Batch 30,240  of  44,637.    Elapsed: 0:19:01. Training loss. 0.0024231779389083385 Num fake examples 29415 Num true examples 31065\n",
      "  Batch 30,280  of  44,637.    Elapsed: 0:19:03. Training loss. 3.062863826751709 Num fake examples 29449 Num true examples 31111\n",
      "  Batch 30,320  of  44,637.    Elapsed: 0:19:04. Training loss. 0.004084516316652298 Num fake examples 29490 Num true examples 31150\n",
      "  Batch 30,360  of  44,637.    Elapsed: 0:19:06. Training loss. 0.00463795755058527 Num fake examples 29526 Num true examples 31194\n",
      "  Batch 30,400  of  44,637.    Elapsed: 0:19:07. Training loss. 0.004446453880518675 Num fake examples 29569 Num true examples 31231\n",
      "  Batch 30,440  of  44,637.    Elapsed: 0:19:09. Training loss. 0.0026487899012863636 Num fake examples 29602 Num true examples 31278\n",
      "  Batch 30,480  of  44,637.    Elapsed: 0:19:10. Training loss. 0.002544871298596263 Num fake examples 29643 Num true examples 31317\n",
      "  Batch 30,520  of  44,637.    Elapsed: 0:19:12. Training loss. 0.003972960636019707 Num fake examples 29679 Num true examples 31361\n",
      "  Batch 30,560  of  44,637.    Elapsed: 0:19:13. Training loss. 0.0030309290159493685 Num fake examples 29724 Num true examples 31396\n",
      "  Batch 30,600  of  44,637.    Elapsed: 0:19:14. Training loss. 0.0037245596759021282 Num fake examples 29769 Num true examples 31431\n",
      "  Batch 30,640  of  44,637.    Elapsed: 0:19:16. Training loss. 0.003961355425417423 Num fake examples 29807 Num true examples 31473\n",
      "  Batch 30,680  of  44,637.    Elapsed: 0:19:17. Training loss. 0.0020753443241119385 Num fake examples 29847 Num true examples 31513\n",
      "  Batch 30,720  of  44,637.    Elapsed: 0:19:19. Training loss. 0.004014449659734964 Num fake examples 29883 Num true examples 31557\n",
      "  Batch 30,760  of  44,637.    Elapsed: 0:19:20. Training loss. 0.0028555169701576233 Num fake examples 29929 Num true examples 31591\n",
      "  Batch 30,800  of  44,637.    Elapsed: 0:19:22. Training loss. 0.005429564975202084 Num fake examples 29966 Num true examples 31634\n",
      "  Batch 30,840  of  44,637.    Elapsed: 0:19:23. Training loss. 0.0027981700841337442 Num fake examples 30001 Num true examples 31679\n",
      "  Batch 30,880  of  44,637.    Elapsed: 0:19:25. Training loss. 0.004113723058253527 Num fake examples 30039 Num true examples 31721\n",
      "  Batch 30,920  of  44,637.    Elapsed: 0:19:26. Training loss. 0.005722814239561558 Num fake examples 30077 Num true examples 31763\n",
      "  Batch 30,960  of  44,637.    Elapsed: 0:19:28. Training loss. 0.0033048982731997967 Num fake examples 30120 Num true examples 31800\n",
      "  Batch 31,000  of  44,637.    Elapsed: 0:19:29. Training loss. 0.005453482270240784 Num fake examples 30165 Num true examples 31835\n",
      "  Batch 31,040  of  44,637.    Elapsed: 0:19:31. Training loss. 0.006599827203899622 Num fake examples 30212 Num true examples 31868\n",
      "  Batch 31,080  of  44,637.    Elapsed: 0:19:32. Training loss. 0.005190486554056406 Num fake examples 30254 Num true examples 31906\n",
      "  Batch 31,120  of  44,637.    Elapsed: 0:19:34. Training loss. 0.0034791866783052683 Num fake examples 30298 Num true examples 31942\n",
      "  Batch 31,160  of  44,637.    Elapsed: 0:19:35. Training loss. 0.003123324364423752 Num fake examples 30340 Num true examples 31980\n",
      "  Batch 31,200  of  44,637.    Elapsed: 0:19:37. Training loss. 0.004395453259348869 Num fake examples 30375 Num true examples 32025\n",
      "  Batch 31,240  of  44,637.    Elapsed: 0:19:38. Training loss. 0.003468980547040701 Num fake examples 30407 Num true examples 32073\n",
      "  Batch 31,280  of  44,637.    Elapsed: 0:19:40. Training loss. 0.0033393593039363623 Num fake examples 30445 Num true examples 32115\n",
      "  Batch 31,320  of  44,637.    Elapsed: 0:19:41. Training loss. 0.0031959228217601776 Num fake examples 30491 Num true examples 32149\n",
      "  Batch 31,360  of  44,637.    Elapsed: 0:19:43. Training loss. 0.003187594236806035 Num fake examples 30533 Num true examples 32187\n",
      "  Batch 31,400  of  44,637.    Elapsed: 0:19:44. Training loss. 0.0023308941163122654 Num fake examples 30580 Num true examples 32220\n",
      "  Batch 31,440  of  44,637.    Elapsed: 0:19:45. Training loss. 0.002083694562315941 Num fake examples 30620 Num true examples 32260\n",
      "  Batch 31,480  of  44,637.    Elapsed: 0:19:47. Training loss. 0.0024734316393733025 Num fake examples 30654 Num true examples 32306\n",
      "  Batch 31,520  of  44,637.    Elapsed: 0:19:48. Training loss. 0.0017962409183382988 Num fake examples 30691 Num true examples 32349\n",
      "  Batch 31,560  of  44,637.    Elapsed: 0:19:50. Training loss. 0.002422372344881296 Num fake examples 30726 Num true examples 32394\n",
      "  Batch 31,600  of  44,637.    Elapsed: 0:19:51. Training loss. 0.0035831851419061422 Num fake examples 30766 Num true examples 32434\n",
      "  Batch 31,640  of  44,637.    Elapsed: 0:19:53. Training loss. 0.0014950090553611517 Num fake examples 30807 Num true examples 32473\n",
      "  Batch 31,680  of  44,637.    Elapsed: 0:19:54. Training loss. 0.0033020968548953533 Num fake examples 30848 Num true examples 32512\n",
      "  Batch 31,720  of  44,637.    Elapsed: 0:19:56. Training loss. 0.003121779765933752 Num fake examples 30883 Num true examples 32557\n",
      "  Batch 31,760  of  44,637.    Elapsed: 0:19:57. Training loss. 0.003526981920003891 Num fake examples 30923 Num true examples 32597\n",
      "  Batch 31,800  of  44,637.    Elapsed: 0:19:59. Training loss. 0.0038319984450936317 Num fake examples 30964 Num true examples 32636\n",
      "  Batch 31,840  of  44,637.    Elapsed: 0:20:00. Training loss. 0.0025636274367570877 Num fake examples 31007 Num true examples 32673\n",
      "  Batch 31,880  of  44,637.    Elapsed: 0:20:02. Training loss. 0.0036556629929691553 Num fake examples 31048 Num true examples 32712\n",
      "  Batch 31,920  of  44,637.    Elapsed: 0:20:03. Training loss. 0.005032012704759836 Num fake examples 31087 Num true examples 32753\n",
      "  Batch 31,960  of  44,637.    Elapsed: 0:20:05. Training loss. 2.841416358947754 Num fake examples 31129 Num true examples 32791\n",
      "  Batch 32,000  of  44,637.    Elapsed: 0:20:06. Training loss. 0.004357821773737669 Num fake examples 31163 Num true examples 32837\n",
      "  Batch 32,040  of  44,637.    Elapsed: 0:20:08. Training loss. 0.0026272423565387726 Num fake examples 31196 Num true examples 32884\n",
      "  Batch 32,080  of  44,637.    Elapsed: 0:20:09. Training loss. 0.0034899672027677298 Num fake examples 31239 Num true examples 32921\n",
      "  Batch 32,120  of  44,637.    Elapsed: 0:20:11. Training loss. 0.003636065172031522 Num fake examples 31274 Num true examples 32966\n",
      "  Batch 32,160  of  44,637.    Elapsed: 0:20:12. Training loss. 0.003708240808919072 Num fake examples 31307 Num true examples 33013\n",
      "  Batch 32,200  of  44,637.    Elapsed: 0:20:14. Training loss. 0.003696002531796694 Num fake examples 31346 Num true examples 33054\n",
      "  Batch 32,240  of  44,637.    Elapsed: 0:20:15. Training loss. 0.00480553787201643 Num fake examples 31386 Num true examples 33094\n",
      "  Batch 32,280  of  44,637.    Elapsed: 0:20:17. Training loss. 0.004537754692137241 Num fake examples 31427 Num true examples 33133\n",
      "  Batch 32,320  of  44,637.    Elapsed: 0:20:18. Training loss. 0.002060904633253813 Num fake examples 31467 Num true examples 33173\n",
      "  Batch 32,360  of  44,637.    Elapsed: 0:20:19. Training loss. 0.004246101249009371 Num fake examples 31505 Num true examples 33215\n",
      "  Batch 32,400  of  44,637.    Elapsed: 0:20:21. Training loss. 0.003453410929068923 Num fake examples 31550 Num true examples 33250\n",
      "  Batch 32,440  of  44,637.    Elapsed: 0:20:22. Training loss. 0.0036147930659353733 Num fake examples 31588 Num true examples 33292\n",
      "  Batch 32,480  of  44,637.    Elapsed: 0:20:24. Training loss. 0.0036643899511545897 Num fake examples 31628 Num true examples 33332\n",
      "  Batch 32,520  of  44,637.    Elapsed: 0:20:25. Training loss. 0.00537969172000885 Num fake examples 31666 Num true examples 33374\n",
      "  Batch 32,560  of  44,637.    Elapsed: 0:20:27. Training loss. 0.004580107517540455 Num fake examples 31706 Num true examples 33414\n",
      "  Batch 32,600  of  44,637.    Elapsed: 0:20:28. Training loss. 0.004404576029628515 Num fake examples 31743 Num true examples 33457\n",
      "  Batch 32,640  of  44,637.    Elapsed: 0:20:30. Training loss. 0.004443762823939323 Num fake examples 31781 Num true examples 33499\n",
      "  Batch 32,680  of  44,637.    Elapsed: 0:20:31. Training loss. 0.001851684064604342 Num fake examples 31825 Num true examples 33535\n",
      "  Batch 32,720  of  44,637.    Elapsed: 0:20:33. Training loss. 0.006425944156944752 Num fake examples 31863 Num true examples 33577\n",
      "  Batch 32,760  of  44,637.    Elapsed: 0:20:34. Training loss. 0.007646492216736078 Num fake examples 31899 Num true examples 33621\n",
      "  Batch 32,800  of  44,637.    Elapsed: 0:20:36. Training loss. 0.006680703721940517 Num fake examples 31933 Num true examples 33667\n",
      "  Batch 32,840  of  44,637.    Elapsed: 0:20:37. Training loss. 0.005312864203006029 Num fake examples 31972 Num true examples 33708\n",
      "  Batch 32,880  of  44,637.    Elapsed: 0:20:39. Training loss. 0.0036407089792191982 Num fake examples 32019 Num true examples 33741\n",
      "  Batch 32,920  of  44,637.    Elapsed: 0:20:40. Training loss. 3.0158469676971436 Num fake examples 32063 Num true examples 33777\n",
      "  Batch 32,960  of  44,637.    Elapsed: 0:20:42. Training loss. 0.004033133387565613 Num fake examples 32091 Num true examples 33829\n",
      "  Batch 33,000  of  44,637.    Elapsed: 0:20:43. Training loss. 0.003006403800100088 Num fake examples 32125 Num true examples 33875\n",
      "  Batch 33,040  of  44,637.    Elapsed: 0:20:44. Training loss. 0.004002336412668228 Num fake examples 32162 Num true examples 33918\n",
      "  Batch 33,080  of  44,637.    Elapsed: 0:20:46. Training loss. 0.0034417780116200447 Num fake examples 32200 Num true examples 33960\n",
      "  Batch 33,120  of  44,637.    Elapsed: 0:20:47. Training loss. 0.004094976000487804 Num fake examples 32237 Num true examples 34003\n",
      "  Batch 33,160  of  44,637.    Elapsed: 0:20:49. Training loss. 0.0025410749949514866 Num fake examples 32279 Num true examples 34041\n",
      "  Batch 33,200  of  44,637.    Elapsed: 0:20:50. Training loss. 0.003277470823377371 Num fake examples 32319 Num true examples 34081\n",
      "  Batch 33,240  of  44,637.    Elapsed: 0:20:52. Training loss. 0.002536076121032238 Num fake examples 32359 Num true examples 34121\n",
      "  Batch 33,280  of  44,637.    Elapsed: 0:20:53. Training loss. 0.0030199328903108835 Num fake examples 32402 Num true examples 34158\n",
      "  Batch 33,320  of  44,637.    Elapsed: 0:20:55. Training loss. 0.002466881647706032 Num fake examples 32437 Num true examples 34203\n",
      "  Batch 33,360  of  44,637.    Elapsed: 0:20:56. Training loss. 0.002641863189637661 Num fake examples 32471 Num true examples 34249\n",
      "  Batch 33,400  of  44,637.    Elapsed: 0:20:58. Training loss. 0.0028110495768487453 Num fake examples 32511 Num true examples 34289\n",
      "  Batch 33,440  of  44,637.    Elapsed: 0:20:59. Training loss. 0.0024250720161944628 Num fake examples 32552 Num true examples 34328\n",
      "  Batch 33,480  of  44,637.    Elapsed: 0:21:01. Training loss. 0.0029135369695723057 Num fake examples 32592 Num true examples 34368\n",
      "  Batch 33,520  of  44,637.    Elapsed: 0:21:02. Training loss. 0.002189752645790577 Num fake examples 32635 Num true examples 34405\n",
      "  Batch 33,560  of  44,637.    Elapsed: 0:21:04. Training loss. 0.0025836110580712557 Num fake examples 32672 Num true examples 34448\n",
      "  Batch 33,600  of  44,637.    Elapsed: 0:21:05. Training loss. 0.0018646684475243092 Num fake examples 32711 Num true examples 34489\n",
      "  Batch 33,640  of  44,637.    Elapsed: 0:21:06. Training loss. 0.0024351917672902346 Num fake examples 32751 Num true examples 34529\n",
      "  Batch 33,680  of  44,637.    Elapsed: 0:21:08. Training loss. 0.0022338591516017914 Num fake examples 32799 Num true examples 34561\n",
      "  Batch 33,720  of  44,637.    Elapsed: 0:21:09. Training loss. 0.0020800414495170116 Num fake examples 32837 Num true examples 34603\n",
      "  Batch 33,760  of  44,637.    Elapsed: 0:21:11. Training loss. 0.001849232241511345 Num fake examples 32888 Num true examples 34632\n",
      "  Batch 33,800  of  44,637.    Elapsed: 0:21:12. Training loss. 0.0025404610205441713 Num fake examples 32927 Num true examples 34673\n",
      "  Batch 33,840  of  44,637.    Elapsed: 0:21:14. Training loss. 0.002208143472671509 Num fake examples 32972 Num true examples 34708\n",
      "  Batch 33,880  of  44,637.    Elapsed: 0:21:15. Training loss. 0.0018321319948881865 Num fake examples 33016 Num true examples 34744\n",
      "  Batch 33,920  of  44,637.    Elapsed: 0:21:17. Training loss. 0.00257354648783803 Num fake examples 33059 Num true examples 34781\n",
      "  Batch 33,960  of  44,637.    Elapsed: 0:21:18. Training loss. 0.002474220236763358 Num fake examples 33092 Num true examples 34828\n",
      "  Batch 34,000  of  44,637.    Elapsed: 0:21:20. Training loss. 0.0033931415528059006 Num fake examples 33126 Num true examples 34874\n",
      "  Batch 34,040  of  44,637.    Elapsed: 0:21:21. Training loss. 0.0024003279395401478 Num fake examples 33162 Num true examples 34918\n",
      "  Batch 34,080  of  44,637.    Elapsed: 0:21:23. Training loss. 0.0017263100016862154 Num fake examples 33212 Num true examples 34948\n",
      "  Batch 34,120  of  44,637.    Elapsed: 0:21:24. Training loss. 0.0025511253625154495 Num fake examples 33253 Num true examples 34987\n",
      "  Batch 34,160  of  44,637.    Elapsed: 0:21:25. Training loss. 0.002917585661634803 Num fake examples 33292 Num true examples 35028\n",
      "  Batch 34,200  of  44,637.    Elapsed: 0:21:27. Training loss. 0.0026617245748639107 Num fake examples 33327 Num true examples 35073\n",
      "  Batch 34,240  of  44,637.    Elapsed: 0:21:28. Training loss. 0.0022450354881584644 Num fake examples 33368 Num true examples 35112\n",
      "  Batch 34,280  of  44,637.    Elapsed: 0:21:30. Training loss. 0.002148004248738289 Num fake examples 33405 Num true examples 35155\n",
      "  Batch 34,320  of  44,637.    Elapsed: 0:21:31. Training loss. 2.7822072505950928 Num fake examples 33441 Num true examples 35199\n",
      "  Batch 34,360  of  44,637.    Elapsed: 0:21:33. Training loss. 0.005220580846071243 Num fake examples 33481 Num true examples 35239\n",
      "  Batch 34,400  of  44,637.    Elapsed: 0:21:34. Training loss. 0.0050038485787808895 Num fake examples 33517 Num true examples 35283\n",
      "  Batch 34,440  of  44,637.    Elapsed: 0:21:36. Training loss. 0.0025512278079986572 Num fake examples 33553 Num true examples 35327\n",
      "  Batch 34,480  of  44,637.    Elapsed: 0:21:37. Training loss. 0.0031562531366944313 Num fake examples 33597 Num true examples 35363\n",
      "  Batch 34,520  of  44,637.    Elapsed: 0:21:39. Training loss. 0.003160858293995261 Num fake examples 33638 Num true examples 35402\n",
      "  Batch 34,560  of  44,637.    Elapsed: 0:21:40. Training loss. 0.002931654918938875 Num fake examples 33669 Num true examples 35451\n",
      "  Batch 34,600  of  44,637.    Elapsed: 0:21:42. Training loss. 0.0038501869421452284 Num fake examples 33707 Num true examples 35493\n",
      "  Batch 34,640  of  44,637.    Elapsed: 0:21:43. Training loss. 0.00463827233761549 Num fake examples 33742 Num true examples 35538\n",
      "  Batch 34,680  of  44,637.    Elapsed: 0:21:45. Training loss. 0.005186913069337606 Num fake examples 33780 Num true examples 35580\n",
      "  Batch 34,720  of  44,637.    Elapsed: 0:21:46. Training loss. 0.005443851463496685 Num fake examples 33819 Num true examples 35621\n",
      "  Batch 34,760  of  44,637.    Elapsed: 0:21:47. Training loss. 0.004217659123241901 Num fake examples 33853 Num true examples 35667\n",
      "  Batch 34,800  of  44,637.    Elapsed: 0:21:49. Training loss. 0.005455458536744118 Num fake examples 33894 Num true examples 35706\n",
      "  Batch 34,840  of  44,637.    Elapsed: 0:21:50. Training loss. 0.0033920351415872574 Num fake examples 33936 Num true examples 35744\n",
      "  Batch 34,880  of  44,637.    Elapsed: 0:21:52. Training loss. 0.002163361757993698 Num fake examples 33977 Num true examples 35783\n",
      "  Batch 34,920  of  44,637.    Elapsed: 0:21:53. Training loss. 0.002426778431981802 Num fake examples 34013 Num true examples 35827\n",
      "  Batch 34,960  of  44,637.    Elapsed: 0:21:55. Training loss. 2.9292240142822266 Num fake examples 34050 Num true examples 35870\n",
      "  Batch 35,000  of  44,637.    Elapsed: 0:21:56. Training loss. 0.0018260410288348794 Num fake examples 34089 Num true examples 35911\n",
      "  Batch 35,040  of  44,637.    Elapsed: 0:21:58. Training loss. 0.001950921374373138 Num fake examples 34125 Num true examples 35955\n",
      "  Batch 35,080  of  44,637.    Elapsed: 0:21:59. Training loss. 0.002313947770744562 Num fake examples 34163 Num true examples 35997\n",
      "  Batch 35,120  of  44,637.    Elapsed: 0:22:01. Training loss. 0.0021310136653482914 Num fake examples 34199 Num true examples 36041\n",
      "  Batch 35,160  of  44,637.    Elapsed: 0:22:02. Training loss. 0.0018130799289792776 Num fake examples 34245 Num true examples 36075\n",
      "  Batch 35,200  of  44,637.    Elapsed: 0:22:04. Training loss. 0.0019622701220214367 Num fake examples 34278 Num true examples 36122\n",
      "  Batch 35,240  of  44,637.    Elapsed: 0:22:05. Training loss. 0.001703815534710884 Num fake examples 34314 Num true examples 36166\n",
      "  Batch 35,280  of  44,637.    Elapsed: 0:22:06. Training loss. 0.0018190721748396754 Num fake examples 34351 Num true examples 36209\n",
      "  Batch 35,320  of  44,637.    Elapsed: 0:22:08. Training loss. 0.00154875370208174 Num fake examples 34389 Num true examples 36251\n",
      "  Batch 35,360  of  44,637.    Elapsed: 0:22:09. Training loss. 0.0022621327079832554 Num fake examples 34426 Num true examples 36294\n",
      "  Batch 35,400  of  44,637.    Elapsed: 0:22:11. Training loss. 0.001636234112083912 Num fake examples 34465 Num true examples 36335\n",
      "  Batch 35,440  of  44,637.    Elapsed: 0:22:12. Training loss. 0.0013508705887943506 Num fake examples 34501 Num true examples 36379\n",
      "  Batch 35,480  of  44,637.    Elapsed: 0:22:14. Training loss. 0.0019842321053147316 Num fake examples 34543 Num true examples 36417\n",
      "  Batch 35,520  of  44,637.    Elapsed: 0:22:15. Training loss. 0.003194092307239771 Num fake examples 34585 Num true examples 36455\n",
      "  Batch 35,560  of  44,637.    Elapsed: 0:22:17. Training loss. 0.0033917762339115143 Num fake examples 34628 Num true examples 36492\n",
      "  Batch 35,600  of  44,637.    Elapsed: 0:22:18. Training loss. 0.0027189096435904503 Num fake examples 34665 Num true examples 36535\n",
      "  Batch 35,640  of  44,637.    Elapsed: 0:22:20. Training loss. 0.002956694457679987 Num fake examples 34702 Num true examples 36578\n",
      "  Batch 35,680  of  44,637.    Elapsed: 0:22:21. Training loss. 0.0033730973955243826 Num fake examples 34739 Num true examples 36621\n",
      "  Batch 35,720  of  44,637.    Elapsed: 0:22:23. Training loss. 0.003457574173808098 Num fake examples 34769 Num true examples 36671\n",
      "  Batch 35,760  of  44,637.    Elapsed: 0:22:24. Training loss. 0.0026874898467212915 Num fake examples 34810 Num true examples 36710\n",
      "  Batch 35,800  of  44,637.    Elapsed: 0:22:26. Training loss. 0.0031426099594682455 Num fake examples 34846 Num true examples 36754\n",
      "  Batch 35,840  of  44,637.    Elapsed: 0:22:27. Training loss. 0.0025930777192115784 Num fake examples 34892 Num true examples 36788\n",
      "  Batch 35,880  of  44,637.    Elapsed: 0:22:29. Training loss. 0.0038751880638301373 Num fake examples 34933 Num true examples 36827\n",
      "  Batch 35,920  of  44,637.    Elapsed: 0:22:30. Training loss. 0.001953404862433672 Num fake examples 34980 Num true examples 36860\n",
      "  Batch 35,960  of  44,637.    Elapsed: 0:22:32. Training loss. 2.4262428283691406 Num fake examples 35029 Num true examples 36891\n",
      "  Batch 36,000  of  44,637.    Elapsed: 0:22:33. Training loss. 0.004572512581944466 Num fake examples 35070 Num true examples 36930\n",
      "  Batch 36,040  of  44,637.    Elapsed: 0:22:35. Training loss. 0.0027457578107714653 Num fake examples 35102 Num true examples 36978\n",
      "  Batch 36,080  of  44,637.    Elapsed: 0:22:36. Training loss. 0.002701242920011282 Num fake examples 35135 Num true examples 37025\n",
      "  Batch 36,120  of  44,637.    Elapsed: 0:22:38. Training loss. 0.002720510121434927 Num fake examples 35167 Num true examples 37073\n",
      "  Batch 36,160  of  44,637.    Elapsed: 0:22:39. Training loss. 0.002128962893038988 Num fake examples 35205 Num true examples 37115\n",
      "  Batch 36,200  of  44,637.    Elapsed: 0:22:40. Training loss. 0.002762873424217105 Num fake examples 35247 Num true examples 37153\n",
      "  Batch 36,240  of  44,637.    Elapsed: 0:22:42. Training loss. 0.0035038013011217117 Num fake examples 35293 Num true examples 37187\n",
      "  Batch 36,280  of  44,637.    Elapsed: 0:22:43. Training loss. 0.002430437598377466 Num fake examples 35331 Num true examples 37229\n",
      "  Batch 36,320  of  44,637.    Elapsed: 0:22:45. Training loss. 0.0016880123876035213 Num fake examples 35364 Num true examples 37276\n",
      "  Batch 36,360  of  44,637.    Elapsed: 0:22:46. Training loss. 2.990511417388916 Num fake examples 35395 Num true examples 37325\n",
      "  Batch 36,400  of  44,637.    Elapsed: 0:22:48. Training loss. 0.002930243732407689 Num fake examples 35438 Num true examples 37362\n",
      "  Batch 36,440  of  44,637.    Elapsed: 0:22:49. Training loss. 0.002358481287956238 Num fake examples 35470 Num true examples 37410\n",
      "  Batch 36,480  of  44,637.    Elapsed: 0:22:51. Training loss. 0.0032248632051050663 Num fake examples 35515 Num true examples 37445\n",
      "  Batch 36,520  of  44,637.    Elapsed: 0:22:52. Training loss. 0.0018414546502754092 Num fake examples 35550 Num true examples 37490\n",
      "  Batch 36,560  of  44,637.    Elapsed: 0:22:54. Training loss. 0.0023056939244270325 Num fake examples 35593 Num true examples 37527\n",
      "  Batch 36,600  of  44,637.    Elapsed: 0:22:55. Training loss. 0.0030840751715004444 Num fake examples 35634 Num true examples 37566\n",
      "  Batch 36,640  of  44,637.    Elapsed: 0:22:57. Training loss. 0.005124756135046482 Num fake examples 35671 Num true examples 37609\n",
      "  Batch 36,680  of  44,637.    Elapsed: 0:22:58. Training loss. 0.004297881852835417 Num fake examples 35703 Num true examples 37657\n",
      "  Batch 36,720  of  44,637.    Elapsed: 0:23:00. Training loss. 0.004986420273780823 Num fake examples 35741 Num true examples 37699\n",
      "  Batch 36,760  of  44,637.    Elapsed: 0:23:01. Training loss. 0.004347885027527809 Num fake examples 35792 Num true examples 37728\n",
      "  Batch 36,800  of  44,637.    Elapsed: 0:23:03. Training loss. 0.0033815607894212008 Num fake examples 35830 Num true examples 37770\n",
      "  Batch 36,840  of  44,637.    Elapsed: 0:23:04. Training loss. 0.0027759699150919914 Num fake examples 35870 Num true examples 37810\n",
      "  Batch 36,880  of  44,637.    Elapsed: 0:23:06. Training loss. 0.0035844002850353718 Num fake examples 35911 Num true examples 37849\n",
      "  Batch 36,920  of  44,637.    Elapsed: 0:23:07. Training loss. 0.0034199608489871025 Num fake examples 35952 Num true examples 37888\n",
      "  Batch 36,960  of  44,637.    Elapsed: 0:23:09. Training loss. 0.003195861354470253 Num fake examples 35992 Num true examples 37928\n",
      "  Batch 37,000  of  44,637.    Elapsed: 0:23:10. Training loss. 0.0026947776786983013 Num fake examples 36039 Num true examples 37961\n",
      "  Batch 37,040  of  44,637.    Elapsed: 0:23:12. Training loss. 0.0023894361220300198 Num fake examples 36077 Num true examples 38003\n",
      "  Batch 37,080  of  44,637.    Elapsed: 0:23:13. Training loss. 3.171434164047241 Num fake examples 36114 Num true examples 38046\n",
      "  Batch 37,120  of  44,637.    Elapsed: 0:23:15. Training loss. 0.004521629773080349 Num fake examples 36153 Num true examples 38087\n",
      "  Batch 37,160  of  44,637.    Elapsed: 0:23:17. Training loss. 0.00402492843568325 Num fake examples 36198 Num true examples 38122\n",
      "  Batch 37,200  of  44,637.    Elapsed: 0:23:18. Training loss. 0.0025361720472574234 Num fake examples 36248 Num true examples 38152\n",
      "  Batch 37,240  of  44,637.    Elapsed: 0:23:20. Training loss. 0.002802628092467785 Num fake examples 36290 Num true examples 38190\n",
      "  Batch 37,280  of  44,637.    Elapsed: 0:23:21. Training loss. 0.004248392302542925 Num fake examples 36330 Num true examples 38230\n",
      "  Batch 37,320  of  44,637.    Elapsed: 0:23:23. Training loss. 0.004287762567400932 Num fake examples 36370 Num true examples 38270\n",
      "  Batch 37,360  of  44,637.    Elapsed: 0:23:25. Training loss. 0.0016679817344993353 Num fake examples 36418 Num true examples 38302\n",
      "  Batch 37,400  of  44,637.    Elapsed: 0:23:26. Training loss. 3.051957368850708 Num fake examples 36457 Num true examples 38343\n",
      "  Batch 37,440  of  44,637.    Elapsed: 0:23:28. Training loss. 0.0027371300384402275 Num fake examples 36490 Num true examples 38390\n",
      "  Batch 37,480  of  44,637.    Elapsed: 0:23:29. Training loss. 0.0026421318762004375 Num fake examples 36520 Num true examples 38440\n",
      "  Batch 37,520  of  44,637.    Elapsed: 0:23:31. Training loss. 0.0025829216465353966 Num fake examples 36557 Num true examples 38483\n",
      "  Batch 37,560  of  44,637.    Elapsed: 0:23:32. Training loss. 0.0020557972602546215 Num fake examples 36594 Num true examples 38526\n",
      "  Batch 37,600  of  44,637.    Elapsed: 0:23:34. Training loss. 0.0040273480117321014 Num fake examples 36634 Num true examples 38566\n",
      "  Batch 37,640  of  44,637.    Elapsed: 0:23:36. Training loss. 0.003810425288975239 Num fake examples 36676 Num true examples 38604\n",
      "  Batch 37,680  of  44,637.    Elapsed: 0:23:37. Training loss. 0.0037754392251372337 Num fake examples 36713 Num true examples 38647\n",
      "  Batch 37,720  of  44,637.    Elapsed: 0:23:39. Training loss. 2.5520408153533936 Num fake examples 36745 Num true examples 38695\n",
      "  Batch 37,760  of  44,637.    Elapsed: 0:23:40. Training loss. 0.007001211866736412 Num fake examples 36782 Num true examples 38738\n",
      "  Batch 37,800  of  44,637.    Elapsed: 0:23:42. Training loss. 0.003995399922132492 Num fake examples 36826 Num true examples 38774\n",
      "  Batch 37,840  of  44,637.    Elapsed: 0:23:43. Training loss. 2.8749868869781494 Num fake examples 36863 Num true examples 38817\n",
      "  Batch 37,880  of  44,637.    Elapsed: 0:23:45. Training loss. 0.0024910487700253725 Num fake examples 36894 Num true examples 38866\n",
      "  Batch 37,920  of  44,637.    Elapsed: 0:23:46. Training loss. 0.0016724170418456197 Num fake examples 36933 Num true examples 38907\n",
      "  Batch 37,960  of  44,637.    Elapsed: 0:23:48. Training loss. 0.004521855618804693 Num fake examples 36971 Num true examples 38949\n",
      "  Batch 38,000  of  44,637.    Elapsed: 0:23:50. Training loss. 0.005061579868197441 Num fake examples 37013 Num true examples 38987\n",
      "  Batch 38,040  of  44,637.    Elapsed: 0:23:51. Training loss. 0.0047693271189928055 Num fake examples 37050 Num true examples 39030\n",
      "  Batch 38,080  of  44,637.    Elapsed: 0:23:52. Training loss. 0.004701762460172176 Num fake examples 37089 Num true examples 39071\n",
      "  Batch 38,120  of  44,637.    Elapsed: 0:23:54. Training loss. 0.0026582067366689444 Num fake examples 37129 Num true examples 39111\n",
      "  Batch 38,160  of  44,637.    Elapsed: 0:23:56. Training loss. 0.0027292687445878983 Num fake examples 37166 Num true examples 39154\n",
      "  Batch 38,200  of  44,637.    Elapsed: 0:23:57. Training loss. 0.0036846124567091465 Num fake examples 37208 Num true examples 39192\n",
      "  Batch 38,240  of  44,637.    Elapsed: 0:23:59. Training loss. 0.0028823502361774445 Num fake examples 37235 Num true examples 39245\n",
      "  Batch 38,280  of  44,637.    Elapsed: 0:24:00. Training loss. 0.0046434709802269936 Num fake examples 37283 Num true examples 39277\n",
      "  Batch 38,320  of  44,637.    Elapsed: 0:24:02. Training loss. 0.0036852944176644087 Num fake examples 37322 Num true examples 39318\n",
      "  Batch 38,360  of  44,637.    Elapsed: 0:24:03. Training loss. 0.002181400079280138 Num fake examples 37358 Num true examples 39362\n",
      "  Batch 38,400  of  44,637.    Elapsed: 0:24:05. Training loss. 0.002636336488649249 Num fake examples 37396 Num true examples 39404\n",
      "  Batch 38,440  of  44,637.    Elapsed: 0:24:06. Training loss. 0.003160571912303567 Num fake examples 37439 Num true examples 39441\n",
      "  Batch 38,480  of  44,637.    Elapsed: 0:24:08. Training loss. 0.002550943987444043 Num fake examples 37478 Num true examples 39482\n",
      "  Batch 38,520  of  44,637.    Elapsed: 0:24:09. Training loss. 0.0043748063035309315 Num fake examples 37519 Num true examples 39521\n",
      "  Batch 38,560  of  44,637.    Elapsed: 0:24:11. Training loss. 0.004260663408786058 Num fake examples 37551 Num true examples 39569\n",
      "  Batch 38,600  of  44,637.    Elapsed: 0:24:12. Training loss. 0.007501847110688686 Num fake examples 37591 Num true examples 39609\n",
      "  Batch 38,640  of  44,637.    Elapsed: 0:24:14. Training loss. 0.004812871105968952 Num fake examples 37629 Num true examples 39651\n",
      "  Batch 38,680  of  44,637.    Elapsed: 0:24:15. Training loss. 0.004405728541314602 Num fake examples 37668 Num true examples 39692\n",
      "  Batch 38,720  of  44,637.    Elapsed: 0:24:17. Training loss. 3.2473292350769043 Num fake examples 37715 Num true examples 39725\n",
      "  Batch 38,760  of  44,637.    Elapsed: 0:24:18. Training loss. 0.005961368791759014 Num fake examples 37751 Num true examples 39769\n",
      "  Batch 38,800  of  44,637.    Elapsed: 0:24:19. Training loss. 0.004954486154019833 Num fake examples 37796 Num true examples 39804\n",
      "  Batch 38,840  of  44,637.    Elapsed: 0:24:21. Training loss. 0.0037543554790318012 Num fake examples 37831 Num true examples 39849\n",
      "  Batch 38,880  of  44,637.    Elapsed: 0:24:22. Training loss. 0.0022491314448416233 Num fake examples 37874 Num true examples 39886\n",
      "  Batch 38,920  of  44,637.    Elapsed: 0:24:24. Training loss. 0.002984083956107497 Num fake examples 37914 Num true examples 39926\n",
      "  Batch 38,960  of  44,637.    Elapsed: 0:24:25. Training loss. 2.8263461589813232 Num fake examples 37958 Num true examples 39962\n",
      "  Batch 39,000  of  44,637.    Elapsed: 0:24:27. Training loss. 0.0026896982453763485 Num fake examples 37994 Num true examples 40006\n",
      "  Batch 39,040  of  44,637.    Elapsed: 0:24:28. Training loss. 0.004251574631780386 Num fake examples 38032 Num true examples 40048\n",
      "  Batch 39,080  of  44,637.    Elapsed: 0:24:30. Training loss. 0.0025977406185120344 Num fake examples 38069 Num true examples 40091\n",
      "  Batch 39,120  of  44,637.    Elapsed: 0:24:31. Training loss. 0.0020264764316380024 Num fake examples 38107 Num true examples 40133\n",
      "  Batch 39,160  of  44,637.    Elapsed: 0:24:33. Training loss. 0.0022387898061424494 Num fake examples 38150 Num true examples 40170\n",
      "  Batch 39,200  of  44,637.    Elapsed: 0:24:35. Training loss. 0.004232179373502731 Num fake examples 38192 Num true examples 40208\n",
      "  Batch 39,240  of  44,637.    Elapsed: 0:24:36. Training loss. 0.00501315388828516 Num fake examples 38227 Num true examples 40253\n",
      "  Batch 39,280  of  44,637.    Elapsed: 0:24:38. Training loss. 0.0031992499716579914 Num fake examples 38261 Num true examples 40299\n",
      "  Batch 39,320  of  44,637.    Elapsed: 0:24:39. Training loss. 0.003058892907574773 Num fake examples 38291 Num true examples 40349\n",
      "  Batch 39,360  of  44,637.    Elapsed: 0:24:40. Training loss. 0.0029343459755182266 Num fake examples 38334 Num true examples 40386\n",
      "  Batch 39,400  of  44,637.    Elapsed: 0:24:42. Training loss. 0.003888912731781602 Num fake examples 38366 Num true examples 40434\n",
      "  Batch 39,440  of  44,637.    Elapsed: 0:24:43. Training loss. 0.003941638395190239 Num fake examples 38396 Num true examples 40484\n",
      "  Batch 39,480  of  44,637.    Elapsed: 0:24:45. Training loss. 2.88081431388855 Num fake examples 38431 Num true examples 40529\n",
      "  Batch 39,520  of  44,637.    Elapsed: 0:24:46. Training loss. 2.916276693344116 Num fake examples 38472 Num true examples 40568\n",
      "  Batch 39,560  of  44,637.    Elapsed: 0:24:48. Training loss. 0.003297765739262104 Num fake examples 38510 Num true examples 40610\n",
      "  Batch 39,600  of  44,637.    Elapsed: 0:24:49. Training loss. 0.0040815151296556 Num fake examples 38549 Num true examples 40651\n",
      "  Batch 39,640  of  44,637.    Elapsed: 0:24:51. Training loss. 0.0029069101437926292 Num fake examples 38584 Num true examples 40696\n",
      "  Batch 39,680  of  44,637.    Elapsed: 0:24:52. Training loss. 0.002800895832479 Num fake examples 38628 Num true examples 40732\n",
      "  Batch 39,720  of  44,637.    Elapsed: 0:24:54. Training loss. 0.0026749130338430405 Num fake examples 38659 Num true examples 40781\n",
      "  Batch 39,760  of  44,637.    Elapsed: 0:24:55. Training loss. 0.004489672835916281 Num fake examples 38702 Num true examples 40818\n",
      "  Batch 39,800  of  44,637.    Elapsed: 0:24:57. Training loss. 0.002850689459592104 Num fake examples 38733 Num true examples 40867\n",
      "  Batch 39,840  of  44,637.    Elapsed: 0:24:58. Training loss. 0.0035126851871609688 Num fake examples 38780 Num true examples 40900\n",
      "  Batch 39,880  of  44,637.    Elapsed: 0:25:00. Training loss. 0.0038950468879193068 Num fake examples 38818 Num true examples 40942\n",
      "  Batch 39,920  of  44,637.    Elapsed: 0:25:01. Training loss. 0.003754429752007127 Num fake examples 38856 Num true examples 40984\n",
      "  Batch 39,960  of  44,637.    Elapsed: 0:25:03. Training loss. 0.003057082649320364 Num fake examples 38892 Num true examples 41028\n",
      "  Batch 40,000  of  44,637.    Elapsed: 0:25:04. Training loss. 0.0035782097838819027 Num fake examples 38930 Num true examples 41070\n",
      "  Batch 40,040  of  44,637.    Elapsed: 0:25:06. Training loss. 0.0027872473001480103 Num fake examples 38971 Num true examples 41109\n",
      "  Batch 40,080  of  44,637.    Elapsed: 0:25:07. Training loss. 0.0077454992569983006 Num fake examples 39010 Num true examples 41150\n",
      "  Batch 40,120  of  44,637.    Elapsed: 0:25:09. Training loss. 0.0027210386469960213 Num fake examples 39049 Num true examples 41191\n",
      "  Batch 40,160  of  44,637.    Elapsed: 0:25:10. Training loss. 0.003731766249984503 Num fake examples 39083 Num true examples 41237\n",
      "  Batch 40,200  of  44,637.    Elapsed: 0:25:12. Training loss. 3.0745742321014404 Num fake examples 39120 Num true examples 41280\n",
      "  Batch 40,240  of  44,637.    Elapsed: 0:25:13. Training loss. 3.031015396118164 Num fake examples 39166 Num true examples 41314\n",
      "  Batch 40,280  of  44,637.    Elapsed: 0:25:14. Training loss. 0.0029106244910508394 Num fake examples 39210 Num true examples 41350\n",
      "  Batch 40,320  of  44,637.    Elapsed: 0:25:16. Training loss. 0.002030312782153487 Num fake examples 39251 Num true examples 41389\n",
      "  Batch 40,360  of  44,637.    Elapsed: 0:25:17. Training loss. 0.0026932223699986935 Num fake examples 39294 Num true examples 41426\n",
      "  Batch 40,400  of  44,637.    Elapsed: 0:25:19. Training loss. 0.0017413946334272623 Num fake examples 39334 Num true examples 41466\n",
      "  Batch 40,440  of  44,637.    Elapsed: 0:25:20. Training loss. 0.00226170988753438 Num fake examples 39379 Num true examples 41501\n",
      "  Batch 40,480  of  44,637.    Elapsed: 0:25:22. Training loss. 0.0022542658261954784 Num fake examples 39416 Num true examples 41544\n",
      "  Batch 40,520  of  44,637.    Elapsed: 0:25:24. Training loss. 0.002611219882965088 Num fake examples 39451 Num true examples 41589\n",
      "  Batch 40,560  of  44,637.    Elapsed: 0:25:25. Training loss. 0.0027214782312512398 Num fake examples 39495 Num true examples 41625\n",
      "  Batch 40,600  of  44,637.    Elapsed: 0:25:27. Training loss. 3.1681509017944336 Num fake examples 39532 Num true examples 41668\n",
      "  Batch 40,640  of  44,637.    Elapsed: 0:25:28. Training loss. 0.0024829739704728127 Num fake examples 39573 Num true examples 41707\n",
      "  Batch 40,680  of  44,637.    Elapsed: 0:25:30. Training loss. 0.0028881975449621677 Num fake examples 39614 Num true examples 41746\n",
      "  Batch 40,720  of  44,637.    Elapsed: 0:25:31. Training loss. 0.0032117273658514023 Num fake examples 39662 Num true examples 41778\n",
      "  Batch 40,760  of  44,637.    Elapsed: 0:25:33. Training loss. 0.0019236654043197632 Num fake examples 39697 Num true examples 41823\n",
      "  Batch 40,800  of  44,637.    Elapsed: 0:25:34. Training loss. 0.0022519659250974655 Num fake examples 39742 Num true examples 41858\n",
      "  Batch 40,840  of  44,637.    Elapsed: 0:25:36. Training loss. 2.9334936141967773 Num fake examples 39780 Num true examples 41900\n",
      "  Batch 40,880  of  44,637.    Elapsed: 0:25:37. Training loss. 0.0038715223781764507 Num fake examples 39821 Num true examples 41939\n",
      "  Batch 40,920  of  44,637.    Elapsed: 0:25:39. Training loss. 0.002274691127240658 Num fake examples 39862 Num true examples 41978\n",
      "  Batch 40,960  of  44,637.    Elapsed: 0:25:40. Training loss. 0.0022678819950670004 Num fake examples 39899 Num true examples 42021\n",
      "  Batch 41,000  of  44,637.    Elapsed: 0:25:42. Training loss. 2.9333302974700928 Num fake examples 39931 Num true examples 42069\n",
      "  Batch 41,040  of  44,637.    Elapsed: 0:25:43. Training loss. 0.004868840798735619 Num fake examples 39974 Num true examples 42106\n",
      "  Batch 41,080  of  44,637.    Elapsed: 0:25:45. Training loss. 0.003646557219326496 Num fake examples 40013 Num true examples 42147\n",
      "  Batch 41,120  of  44,637.    Elapsed: 0:25:46. Training loss. 0.00425689946860075 Num fake examples 40053 Num true examples 42187\n",
      "  Batch 41,160  of  44,637.    Elapsed: 0:25:48. Training loss. 0.003041256917640567 Num fake examples 40092 Num true examples 42228\n",
      "  Batch 41,200  of  44,637.    Elapsed: 0:25:49. Training loss. 0.0026414874009788036 Num fake examples 40140 Num true examples 42260\n",
      "  Batch 41,240  of  44,637.    Elapsed: 0:25:51. Training loss. 0.0027902221772819757 Num fake examples 40171 Num true examples 42309\n",
      "  Batch 41,280  of  44,637.    Elapsed: 0:25:52. Training loss. 0.0027548973448574543 Num fake examples 40215 Num true examples 42345\n",
      "  Batch 41,320  of  44,637.    Elapsed: 0:25:54. Training loss. 0.0033275496680289507 Num fake examples 40263 Num true examples 42377\n",
      "  Batch 41,360  of  44,637.    Elapsed: 0:25:55. Training loss. 0.0021732524037361145 Num fake examples 40303 Num true examples 42417\n",
      "  Batch 41,400  of  44,637.    Elapsed: 0:25:57. Training loss. 0.00340627902187407 Num fake examples 40339 Num true examples 42461\n",
      "  Batch 41,440  of  44,637.    Elapsed: 0:25:58. Training loss. 0.002334725111722946 Num fake examples 40379 Num true examples 42501\n",
      "  Batch 41,480  of  44,637.    Elapsed: 0:26:00. Training loss. 0.0024916469119489193 Num fake examples 40413 Num true examples 42547\n",
      "  Batch 41,520  of  44,637.    Elapsed: 0:26:01. Training loss. 0.0025379564613103867 Num fake examples 40449 Num true examples 42591\n",
      "  Batch 41,560  of  44,637.    Elapsed: 0:26:03. Training loss. 0.0018121686298400164 Num fake examples 40488 Num true examples 42632\n",
      "  Batch 41,600  of  44,637.    Elapsed: 0:26:05. Training loss. 0.0036631517577916384 Num fake examples 40531 Num true examples 42669\n",
      "  Batch 41,640  of  44,637.    Elapsed: 0:26:06. Training loss. 0.0016389047959819436 Num fake examples 40578 Num true examples 42702\n",
      "  Batch 41,680  of  44,637.    Elapsed: 0:26:08. Training loss. 0.001739900792017579 Num fake examples 40619 Num true examples 42741\n",
      "  Batch 41,720  of  44,637.    Elapsed: 0:26:09. Training loss. 0.003918014001101255 Num fake examples 40653 Num true examples 42787\n",
      "  Batch 41,760  of  44,637.    Elapsed: 0:26:11. Training loss. 0.0025383392348885536 Num fake examples 40701 Num true examples 42819\n",
      "  Batch 41,800  of  44,637.    Elapsed: 0:26:12. Training loss. 0.0018921426963061094 Num fake examples 40742 Num true examples 42858\n",
      "  Batch 41,840  of  44,637.    Elapsed: 0:26:14. Training loss. 3.40488338470459 Num fake examples 40777 Num true examples 42903\n",
      "  Batch 41,880  of  44,637.    Elapsed: 0:26:15. Training loss. 0.0014208591310307384 Num fake examples 40817 Num true examples 42943\n",
      "  Batch 41,920  of  44,637.    Elapsed: 0:26:17. Training loss. 0.0020023228134959936 Num fake examples 40852 Num true examples 42988\n",
      "  Batch 41,960  of  44,637.    Elapsed: 0:26:18. Training loss. 0.0027163978666067123 Num fake examples 40887 Num true examples 43033\n",
      "  Batch 42,000  of  44,637.    Elapsed: 0:26:20. Training loss. 0.0024537257850170135 Num fake examples 40928 Num true examples 43072\n",
      "  Batch 42,040  of  44,637.    Elapsed: 0:26:21. Training loss. 0.0021837775129824877 Num fake examples 40965 Num true examples 43115\n",
      "  Batch 42,080  of  44,637.    Elapsed: 0:26:23. Training loss. 0.0018827635794878006 Num fake examples 41005 Num true examples 43155\n",
      "  Batch 42,120  of  44,637.    Elapsed: 0:26:24. Training loss. 0.0027087987400591373 Num fake examples 41044 Num true examples 43196\n",
      "  Batch 42,160  of  44,637.    Elapsed: 0:26:26. Training loss. 0.0031301137059926987 Num fake examples 41082 Num true examples 43238\n",
      "  Batch 42,200  of  44,637.    Elapsed: 0:26:27. Training loss. 0.003654003608971834 Num fake examples 41111 Num true examples 43289\n",
      "  Batch 42,240  of  44,637.    Elapsed: 0:26:29. Training loss. 0.0024635768495500088 Num fake examples 41146 Num true examples 43334\n",
      "  Batch 42,280  of  44,637.    Elapsed: 0:26:30. Training loss. 0.0026731190737336874 Num fake examples 41186 Num true examples 43374\n",
      "  Batch 42,320  of  44,637.    Elapsed: 0:26:32. Training loss. 0.0024794917553663254 Num fake examples 41221 Num true examples 43419\n",
      "  Batch 42,360  of  44,637.    Elapsed: 0:26:33. Training loss. 0.0055539864115417 Num fake examples 41257 Num true examples 43463\n",
      "  Batch 42,400  of  44,637.    Elapsed: 0:26:35. Training loss. 0.006304615177214146 Num fake examples 41300 Num true examples 43500\n",
      "  Batch 42,440  of  44,637.    Elapsed: 0:26:36. Training loss. 0.005704514682292938 Num fake examples 41342 Num true examples 43538\n",
      "  Batch 42,480  of  44,637.    Elapsed: 0:26:38. Training loss. 0.0059203277342021465 Num fake examples 41385 Num true examples 43575\n",
      "  Batch 42,520  of  44,637.    Elapsed: 0:26:39. Training loss. 0.0038868659175932407 Num fake examples 41422 Num true examples 43618\n",
      "  Batch 42,560  of  44,637.    Elapsed: 0:26:41. Training loss. 0.004234922118484974 Num fake examples 41463 Num true examples 43657\n",
      "  Batch 42,600  of  44,637.    Elapsed: 0:26:42. Training loss. 0.003091533901169896 Num fake examples 41499 Num true examples 43701\n",
      "  Batch 42,640  of  44,637.    Elapsed: 0:26:44. Training loss. 0.0038789522368460894 Num fake examples 41529 Num true examples 43751\n",
      "  Batch 42,680  of  44,637.    Elapsed: 0:26:45. Training loss. 0.004084440879523754 Num fake examples 41568 Num true examples 43792\n",
      "  Batch 42,720  of  44,637.    Elapsed: 0:26:46. Training loss. 0.004089301452040672 Num fake examples 41612 Num true examples 43828\n",
      "  Batch 42,760  of  44,637.    Elapsed: 0:26:48. Training loss. 0.0044989073649048805 Num fake examples 41654 Num true examples 43866\n",
      "  Batch 42,800  of  44,637.    Elapsed: 0:26:49. Training loss. 0.003978326451033354 Num fake examples 41700 Num true examples 43900\n",
      "  Batch 42,840  of  44,637.    Elapsed: 0:26:51. Training loss. 0.003908014856278896 Num fake examples 41740 Num true examples 43940\n",
      "  Batch 42,880  of  44,637.    Elapsed: 0:26:52. Training loss. 0.007265307940542698 Num fake examples 41768 Num true examples 43992\n",
      "  Batch 42,920  of  44,637.    Elapsed: 0:26:54. Training loss. 0.004003761801868677 Num fake examples 41812 Num true examples 44028\n",
      "  Batch 42,960  of  44,637.    Elapsed: 0:26:56. Training loss. 0.002989018801599741 Num fake examples 41845 Num true examples 44075\n",
      "  Batch 43,000  of  44,637.    Elapsed: 0:26:57. Training loss. 0.0047542136162519455 Num fake examples 41883 Num true examples 44117\n",
      "  Batch 43,040  of  44,637.    Elapsed: 0:26:59. Training loss. 0.004439549054950476 Num fake examples 41926 Num true examples 44154\n",
      "  Batch 43,080  of  44,637.    Elapsed: 0:27:00. Training loss. 0.003198833204805851 Num fake examples 41972 Num true examples 44188\n",
      "  Batch 43,120  of  44,637.    Elapsed: 0:27:01. Training loss. 0.004310476128011942 Num fake examples 42010 Num true examples 44230\n",
      "  Batch 43,160  of  44,637.    Elapsed: 0:27:03. Training loss. 0.0018809654284268618 Num fake examples 42055 Num true examples 44265\n",
      "  Batch 43,200  of  44,637.    Elapsed: 0:27:04. Training loss. 0.0032561831176280975 Num fake examples 42087 Num true examples 44313\n",
      "  Batch 43,240  of  44,637.    Elapsed: 0:27:06. Training loss. 0.002577583771198988 Num fake examples 42116 Num true examples 44364\n",
      "  Batch 43,280  of  44,637.    Elapsed: 0:27:07. Training loss. 2.8993937969207764 Num fake examples 42160 Num true examples 44400\n",
      "  Batch 43,320  of  44,637.    Elapsed: 0:27:09. Training loss. 0.0033926470205187798 Num fake examples 42197 Num true examples 44443\n",
      "  Batch 43,360  of  44,637.    Elapsed: 0:27:10. Training loss. 0.0049455189146101475 Num fake examples 42243 Num true examples 44477\n",
      "  Batch 43,400  of  44,637.    Elapsed: 0:27:12. Training loss. 0.007405951619148254 Num fake examples 42287 Num true examples 44513\n",
      "  Batch 43,440  of  44,637.    Elapsed: 0:27:13. Training loss. 0.004759000614285469 Num fake examples 42322 Num true examples 44558\n",
      "  Batch 43,480  of  44,637.    Elapsed: 0:27:15. Training loss. 0.004646013490855694 Num fake examples 42362 Num true examples 44598\n",
      "  Batch 43,520  of  44,637.    Elapsed: 0:27:16. Training loss. 0.0036010160110890865 Num fake examples 42393 Num true examples 44647\n",
      "  Batch 43,560  of  44,637.    Elapsed: 0:27:18. Training loss. 0.0036181386094540358 Num fake examples 42430 Num true examples 44690\n",
      "  Batch 43,600  of  44,637.    Elapsed: 0:27:19. Training loss. 0.004800612106919289 Num fake examples 42467 Num true examples 44733\n",
      "  Batch 43,640  of  44,637.    Elapsed: 0:27:20. Training loss. 0.0048527647741138935 Num fake examples 42500 Num true examples 44780\n",
      "  Batch 43,680  of  44,637.    Elapsed: 0:27:22. Training loss. 0.0048552947118878365 Num fake examples 42539 Num true examples 44821\n",
      "  Batch 43,720  of  44,637.    Elapsed: 0:27:23. Training loss. 0.004083134233951569 Num fake examples 42579 Num true examples 44861\n",
      "  Batch 43,760  of  44,637.    Elapsed: 0:27:25. Training loss. 0.004519869107753038 Num fake examples 42620 Num true examples 44900\n",
      "  Batch 43,800  of  44,637.    Elapsed: 0:27:26. Training loss. 0.0037236155476421118 Num fake examples 42661 Num true examples 44939\n",
      "  Batch 43,840  of  44,637.    Elapsed: 0:27:28. Training loss. 0.0032191434875130653 Num fake examples 42699 Num true examples 44981\n",
      "  Batch 43,880  of  44,637.    Elapsed: 0:27:29. Training loss. 0.003841129597276449 Num fake examples 42739 Num true examples 45021\n",
      "  Batch 43,920  of  44,637.    Elapsed: 0:27:31. Training loss. 0.0028022066690027714 Num fake examples 42780 Num true examples 45060\n",
      "  Batch 43,960  of  44,637.    Elapsed: 0:27:32. Training loss. 0.0029131334740668535 Num fake examples 42821 Num true examples 45099\n",
      "  Batch 44,000  of  44,637.    Elapsed: 0:27:34. Training loss. 0.0027524023316800594 Num fake examples 42860 Num true examples 45140\n",
      "  Batch 44,040  of  44,637.    Elapsed: 0:27:35. Training loss. 0.0038030720315873623 Num fake examples 42900 Num true examples 45180\n",
      "  Batch 44,080  of  44,637.    Elapsed: 0:27:37. Training loss. 2.859933614730835 Num fake examples 42947 Num true examples 45213\n",
      "  Batch 44,120  of  44,637.    Elapsed: 0:27:39. Training loss. 0.0031934925355017185 Num fake examples 42988 Num true examples 45252\n",
      "  Batch 44,160  of  44,637.    Elapsed: 0:27:40. Training loss. 0.0025919696781784296 Num fake examples 43033 Num true examples 45287\n",
      "  Batch 44,200  of  44,637.    Elapsed: 0:27:42. Training loss. 0.00233087339438498 Num fake examples 43067 Num true examples 45333\n",
      "  Batch 44,240  of  44,637.    Elapsed: 0:27:43. Training loss. 0.00198307354003191 Num fake examples 43105 Num true examples 45375\n",
      "  Batch 44,280  of  44,637.    Elapsed: 0:27:45. Training loss. 0.002640602644532919 Num fake examples 43141 Num true examples 45419\n",
      "  Batch 44,320  of  44,637.    Elapsed: 0:27:46. Training loss. 0.0023625101894140244 Num fake examples 43176 Num true examples 45464\n",
      "  Batch 44,360  of  44,637.    Elapsed: 0:27:48. Training loss. 0.0017722208285704255 Num fake examples 43216 Num true examples 45504\n",
      "  Batch 44,400  of  44,637.    Elapsed: 0:27:49. Training loss. 0.0031332545913755894 Num fake examples 43255 Num true examples 45545\n",
      "  Batch 44,440  of  44,637.    Elapsed: 0:27:51. Training loss. 0.0034571662545204163 Num fake examples 43286 Num true examples 45594\n",
      "  Batch 44,480  of  44,637.    Elapsed: 0:27:52. Training loss. 0.0012604296207427979 Num fake examples 43330 Num true examples 45630\n",
      "  Batch 44,520  of  44,637.    Elapsed: 0:27:54. Training loss. 0.0037967399694025517 Num fake examples 43368 Num true examples 45672\n",
      "  Batch 44,560  of  44,637.    Elapsed: 0:27:55. Training loss. 0.002923213876783848 Num fake examples 43404 Num true examples 45716\n",
      "  Batch 44,600  of  44,637.    Elapsed: 0:27:57. Training loss. 0.002674310002475977 Num fake examples 43444 Num true examples 45756\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epcoh took: 0:27:58\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation Loss: 0.18\n",
      "  Validation took: 0:01:44\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  44,637.    Elapsed: 0:00:02. Training loss. 0.004736975766718388 Num fake examples 34 Num true examples 46\n",
      "  Batch    80  of  44,637.    Elapsed: 0:00:03. Training loss. 2.876314401626587 Num fake examples 75 Num true examples 85\n",
      "  Batch   120  of  44,637.    Elapsed: 0:00:05. Training loss. 0.004620320163667202 Num fake examples 118 Num true examples 122\n",
      "  Batch   160  of  44,637.    Elapsed: 0:00:06. Training loss. 0.003994501195847988 Num fake examples 158 Num true examples 162\n",
      "  Batch   200  of  44,637.    Elapsed: 0:00:08. Training loss. 0.0027345498092472553 Num fake examples 201 Num true examples 199\n",
      "  Batch   240  of  44,637.    Elapsed: 0:00:09. Training loss. 0.0024498733691871166 Num fake examples 236 Num true examples 244\n",
      "  Batch   280  of  44,637.    Elapsed: 0:00:11. Training loss. 0.0031599176581948996 Num fake examples 282 Num true examples 278\n",
      "  Batch   320  of  44,637.    Elapsed: 0:00:12. Training loss. 0.003405651543289423 Num fake examples 326 Num true examples 314\n",
      "  Batch   360  of  44,637.    Elapsed: 0:00:14. Training loss. 0.003717445768415928 Num fake examples 365 Num true examples 355\n",
      "  Batch   400  of  44,637.    Elapsed: 0:00:15. Training loss. 0.0023944845888763666 Num fake examples 398 Num true examples 402\n",
      "  Batch   440  of  44,637.    Elapsed: 0:00:17. Training loss. 0.003711684374138713 Num fake examples 431 Num true examples 449\n",
      "  Batch   480  of  44,637.    Elapsed: 0:00:18. Training loss. 0.0044481330551207066 Num fake examples 471 Num true examples 489\n",
      "  Batch   520  of  44,637.    Elapsed: 0:00:20. Training loss. 0.003103203373029828 Num fake examples 517 Num true examples 523\n",
      "  Batch   560  of  44,637.    Elapsed: 0:00:21. Training loss. 0.0044692885130643845 Num fake examples 561 Num true examples 559\n",
      "  Batch   600  of  44,637.    Elapsed: 0:00:23. Training loss. 0.0021516424603760242 Num fake examples 597 Num true examples 603\n",
      "  Batch   640  of  44,637.    Elapsed: 0:00:24. Training loss. 0.0029448391869664192 Num fake examples 638 Num true examples 642\n",
      "  Batch   680  of  44,637.    Elapsed: 0:00:26. Training loss. 3.0534114837646484 Num fake examples 670 Num true examples 690\n",
      "  Batch   720  of  44,637.    Elapsed: 0:00:27. Training loss. 0.0030781440436840057 Num fake examples 714 Num true examples 726\n",
      "  Batch   760  of  44,637.    Elapsed: 0:00:29. Training loss. 0.0028903577476739883 Num fake examples 747 Num true examples 773\n",
      "  Batch   800  of  44,637.    Elapsed: 0:00:30. Training loss. 0.0035067410208284855 Num fake examples 788 Num true examples 812\n",
      "  Batch   840  of  44,637.    Elapsed: 0:00:32. Training loss. 0.0036041298881173134 Num fake examples 828 Num true examples 852\n",
      "  Batch   880  of  44,637.    Elapsed: 0:00:33. Training loss. 0.004003251437097788 Num fake examples 866 Num true examples 894\n",
      "  Batch   920  of  44,637.    Elapsed: 0:00:35. Training loss. 0.0036022148560732603 Num fake examples 897 Num true examples 943\n",
      "  Batch   960  of  44,637.    Elapsed: 0:00:37. Training loss. 0.0029486618004739285 Num fake examples 931 Num true examples 989\n",
      "  Batch 1,000  of  44,637.    Elapsed: 0:00:38. Training loss. 0.0024227104149758816 Num fake examples 967 Num true examples 1033\n",
      "  Batch 1,040  of  44,637.    Elapsed: 0:00:40. Training loss. 0.002230310346931219 Num fake examples 1007 Num true examples 1073\n",
      "  Batch 1,080  of  44,637.    Elapsed: 0:00:41. Training loss. 0.0020080110989511013 Num fake examples 1042 Num true examples 1118\n",
      "  Batch 1,120  of  44,637.    Elapsed: 0:00:43. Training loss. 0.0031545711681246758 Num fake examples 1077 Num true examples 1163\n",
      "  Batch 1,160  of  44,637.    Elapsed: 0:00:44. Training loss. 0.003172034863382578 Num fake examples 1117 Num true examples 1203\n",
      "  Batch 1,200  of  44,637.    Elapsed: 0:00:46. Training loss. 0.0025778161361813545 Num fake examples 1154 Num true examples 1246\n",
      "  Batch 1,240  of  44,637.    Elapsed: 0:00:47. Training loss. 0.0018026449251919985 Num fake examples 1198 Num true examples 1282\n",
      "  Batch 1,280  of  44,637.    Elapsed: 0:00:49. Training loss. 0.0015738917281851172 Num fake examples 1232 Num true examples 1328\n",
      "  Batch 1,320  of  44,637.    Elapsed: 0:00:51. Training loss. 0.002260196255519986 Num fake examples 1266 Num true examples 1374\n",
      "  Batch 1,360  of  44,637.    Elapsed: 0:00:52. Training loss. 0.002563850488513708 Num fake examples 1304 Num true examples 1416\n",
      "  Batch 1,400  of  44,637.    Elapsed: 0:00:54. Training loss. 0.0030937271658331156 Num fake examples 1341 Num true examples 1459\n",
      "  Batch 1,440  of  44,637.    Elapsed: 0:00:55. Training loss. 0.003944212570786476 Num fake examples 1379 Num true examples 1501\n",
      "  Batch 1,480  of  44,637.    Elapsed: 0:00:57. Training loss. 0.00507328612729907 Num fake examples 1418 Num true examples 1542\n",
      "  Batch 1,520  of  44,637.    Elapsed: 0:00:58. Training loss. 0.004379589110612869 Num fake examples 1458 Num true examples 1582\n",
      "  Batch 1,560  of  44,637.    Elapsed: 0:01:00. Training loss. 2.794912576675415 Num fake examples 1489 Num true examples 1631\n",
      "  Batch 1,600  of  44,637.    Elapsed: 0:01:02. Training loss. 0.004400207661092281 Num fake examples 1533 Num true examples 1667\n",
      "  Batch 1,640  of  44,637.    Elapsed: 0:01:03. Training loss. 0.006715845782309771 Num fake examples 1566 Num true examples 1714\n",
      "  Batch 1,680  of  44,637.    Elapsed: 0:01:05. Training loss. 0.005586439743638039 Num fake examples 1595 Num true examples 1765\n",
      "  Batch 1,720  of  44,637.    Elapsed: 0:01:06. Training loss. 0.00461925845593214 Num fake examples 1629 Num true examples 1811\n",
      "  Batch 1,760  of  44,637.    Elapsed: 0:01:08. Training loss. 0.005995115265250206 Num fake examples 1665 Num true examples 1855\n",
      "  Batch 1,800  of  44,637.    Elapsed: 0:01:09. Training loss. 0.0037881419993937016 Num fake examples 1708 Num true examples 1892\n",
      "  Batch 1,840  of  44,637.    Elapsed: 0:01:11. Training loss. 0.004022139124572277 Num fake examples 1742 Num true examples 1938\n",
      "  Batch 1,880  of  44,637.    Elapsed: 0:01:12. Training loss. 0.002545271534472704 Num fake examples 1784 Num true examples 1976\n",
      "  Batch 1,920  of  44,637.    Elapsed: 0:01:14. Training loss. 0.0031843939796090126 Num fake examples 1827 Num true examples 2013\n",
      "  Batch 1,960  of  44,637.    Elapsed: 0:01:16. Training loss. 0.0032634977251291275 Num fake examples 1865 Num true examples 2055\n",
      "  Batch 2,000  of  44,637.    Elapsed: 0:01:17. Training loss. 0.0033228821121156216 Num fake examples 1908 Num true examples 2092\n",
      "  Batch 2,040  of  44,637.    Elapsed: 0:01:19. Training loss. 0.0031682723201811314 Num fake examples 1948 Num true examples 2132\n",
      "  Batch 2,080  of  44,637.    Elapsed: 0:01:20. Training loss. 0.0027369577437639236 Num fake examples 1992 Num true examples 2168\n",
      "  Batch 2,120  of  44,637.    Elapsed: 0:01:22. Training loss. 0.0029752831906080246 Num fake examples 2034 Num true examples 2206\n",
      "  Batch 2,160  of  44,637.    Elapsed: 0:01:23. Training loss. 0.003961207345128059 Num fake examples 2079 Num true examples 2241\n",
      "  Batch 2,200  of  44,637.    Elapsed: 0:01:25. Training loss. 0.0054271407425403595 Num fake examples 2120 Num true examples 2280\n",
      "  Batch 2,240  of  44,637.    Elapsed: 0:01:26. Training loss. 0.004085637629032135 Num fake examples 2155 Num true examples 2325\n",
      "  Batch 2,280  of  44,637.    Elapsed: 0:01:28. Training loss. 0.0028786829207092524 Num fake examples 2200 Num true examples 2360\n",
      "  Batch 2,320  of  44,637.    Elapsed: 0:01:29. Training loss. 0.0027155859861522913 Num fake examples 2232 Num true examples 2408\n",
      "  Batch 2,360  of  44,637.    Elapsed: 0:01:31. Training loss. 0.003242866601794958 Num fake examples 2266 Num true examples 2454\n",
      "  Batch 2,400  of  44,637.    Elapsed: 0:01:32. Training loss. 0.003992319107055664 Num fake examples 2304 Num true examples 2496\n",
      "  Batch 2,440  of  44,637.    Elapsed: 0:01:34. Training loss. 0.004730363376438618 Num fake examples 2346 Num true examples 2534\n",
      "  Batch 2,480  of  44,637.    Elapsed: 0:01:35. Training loss. 0.0042719533666968346 Num fake examples 2378 Num true examples 2582\n",
      "  Batch 2,520  of  44,637.    Elapsed: 0:01:37. Training loss. 0.0075201597064733505 Num fake examples 2418 Num true examples 2622\n",
      "  Batch 2,560  of  44,637.    Elapsed: 0:01:38. Training loss. 0.002562892623245716 Num fake examples 2462 Num true examples 2658\n",
      "  Batch 2,600  of  44,637.    Elapsed: 0:01:40. Training loss. 0.0033929902128875256 Num fake examples 2504 Num true examples 2696\n",
      "  Batch 2,640  of  44,637.    Elapsed: 0:01:41. Training loss. 0.0031636226922273636 Num fake examples 2541 Num true examples 2739\n",
      "  Batch 2,680  of  44,637.    Elapsed: 0:01:43. Training loss. 0.0020315637812018394 Num fake examples 2580 Num true examples 2780\n",
      "  Batch 2,720  of  44,637.    Elapsed: 0:01:45. Training loss. 0.0022059830371290445 Num fake examples 2621 Num true examples 2819\n",
      "  Batch 2,760  of  44,637.    Elapsed: 0:01:46. Training loss. 2.812448740005493 Num fake examples 2657 Num true examples 2863\n",
      "  Batch 2,800  of  44,637.    Elapsed: 0:01:48. Training loss. 0.0023390918504446745 Num fake examples 2691 Num true examples 2909\n",
      "  Batch 2,840  of  44,637.    Elapsed: 0:01:49. Training loss. 0.0034675155766308308 Num fake examples 2728 Num true examples 2952\n",
      "  Batch 2,880  of  44,637.    Elapsed: 0:01:51. Training loss. 0.002876511076465249 Num fake examples 2764 Num true examples 2996\n",
      "  Batch 2,920  of  44,637.    Elapsed: 0:01:52. Training loss. 0.002660032594576478 Num fake examples 2811 Num true examples 3029\n",
      "  Batch 2,960  of  44,637.    Elapsed: 0:01:54. Training loss. 0.001611195271834731 Num fake examples 2848 Num true examples 3072\n",
      "  Batch 3,000  of  44,637.    Elapsed: 0:01:56. Training loss. 3.0885376930236816 Num fake examples 2892 Num true examples 3108\n",
      "  Batch 3,040  of  44,637.    Elapsed: 0:01:57. Training loss. 0.003507607849314809 Num fake examples 2931 Num true examples 3149\n",
      "  Batch 3,080  of  44,637.    Elapsed: 0:01:59. Training loss. 0.00359864323399961 Num fake examples 2973 Num true examples 3187\n",
      "  Batch 3,120  of  44,637.    Elapsed: 0:02:00. Training loss. 0.0028468407690525055 Num fake examples 3017 Num true examples 3223\n",
      "  Batch 3,160  of  44,637.    Elapsed: 0:02:02. Training loss. 0.002202369272708893 Num fake examples 3060 Num true examples 3260\n",
      "  Batch 3,200  of  44,637.    Elapsed: 0:02:03. Training loss. 0.002827058546245098 Num fake examples 3095 Num true examples 3305\n",
      "  Batch 3,240  of  44,637.    Elapsed: 0:02:05. Training loss. 0.0018327340949326754 Num fake examples 3134 Num true examples 3346\n",
      "  Batch 3,280  of  44,637.    Elapsed: 0:02:06. Training loss. 0.002804096322506666 Num fake examples 3174 Num true examples 3386\n",
      "  Batch 3,320  of  44,637.    Elapsed: 0:02:08. Training loss. 0.004143030382692814 Num fake examples 3216 Num true examples 3424\n",
      "  Batch 3,360  of  44,637.    Elapsed: 0:02:10. Training loss. 0.0030735190957784653 Num fake examples 3256 Num true examples 3464\n",
      "  Batch 3,400  of  44,637.    Elapsed: 0:02:11. Training loss. 0.002479833085089922 Num fake examples 3294 Num true examples 3506\n",
      "  Batch 3,440  of  44,637.    Elapsed: 0:02:13. Training loss. 0.0033622977789491415 Num fake examples 3336 Num true examples 3544\n",
      "  Batch 3,480  of  44,637.    Elapsed: 0:02:14. Training loss. 0.0020824195817112923 Num fake examples 3371 Num true examples 3589\n",
      "  Batch 3,520  of  44,637.    Elapsed: 0:02:16. Training loss. 0.0035885318648070097 Num fake examples 3411 Num true examples 3629\n",
      "  Batch 3,560  of  44,637.    Elapsed: 0:02:17. Training loss. 0.0031143431551754475 Num fake examples 3446 Num true examples 3674\n",
      "  Batch 3,600  of  44,637.    Elapsed: 0:02:19. Training loss. 0.004595301579684019 Num fake examples 3483 Num true examples 3717\n",
      "  Batch 3,640  of  44,637.    Elapsed: 0:02:20. Training loss. 0.0022215479984879494 Num fake examples 3523 Num true examples 3757\n",
      "  Batch 3,680  of  44,637.    Elapsed: 0:02:22. Training loss. 0.0010363154578953981 Num fake examples 3561 Num true examples 3799\n",
      "  Batch 3,720  of  44,637.    Elapsed: 0:02:24. Training loss. 0.008901087567210197 Num fake examples 3599 Num true examples 3841\n",
      "  Batch 3,760  of  44,637.    Elapsed: 0:02:25. Training loss. 3.1994519233703613 Num fake examples 3643 Num true examples 3877\n",
      "  Batch 3,800  of  44,637.    Elapsed: 0:02:27. Training loss. 0.0042891791090369225 Num fake examples 3684 Num true examples 3916\n",
      "  Batch 3,840  of  44,637.    Elapsed: 0:02:28. Training loss. 0.0031989135313779116 Num fake examples 3725 Num true examples 3955\n",
      "  Batch 3,880  of  44,637.    Elapsed: 0:02:30. Training loss. 0.0017778980545699596 Num fake examples 3762 Num true examples 3998\n",
      "  Batch 3,920  of  44,637.    Elapsed: 0:02:31. Training loss. 0.0014412023592740297 Num fake examples 3802 Num true examples 4038\n",
      "  Batch 3,960  of  44,637.    Elapsed: 0:02:33. Training loss. 0.0023446092382073402 Num fake examples 3838 Num true examples 4082\n",
      "  Batch 4,000  of  44,637.    Elapsed: 0:02:34. Training loss. 0.001333689782768488 Num fake examples 3867 Num true examples 4133\n",
      "  Batch 4,040  of  44,637.    Elapsed: 0:02:36. Training loss. 3.1538374423980713 Num fake examples 3907 Num true examples 4173\n",
      "  Batch 4,080  of  44,637.    Elapsed: 0:02:37. Training loss. 0.0033197137527167797 Num fake examples 3946 Num true examples 4214\n",
      "  Batch 4,120  of  44,637.    Elapsed: 0:02:39. Training loss. 0.004045084584504366 Num fake examples 3989 Num true examples 4251\n",
      "  Batch 4,160  of  44,637.    Elapsed: 0:02:41. Training loss. 0.004512266255915165 Num fake examples 4021 Num true examples 4299\n",
      "  Batch 4,200  of  44,637.    Elapsed: 0:02:42. Training loss. 0.003490776289254427 Num fake examples 4063 Num true examples 4337\n",
      "  Batch 4,240  of  44,637.    Elapsed: 0:02:44. Training loss. 0.0032308422960340977 Num fake examples 4095 Num true examples 4385\n",
      "  Batch 4,280  of  44,637.    Elapsed: 0:02:45. Training loss. 0.003923770971596241 Num fake examples 4134 Num true examples 4426\n",
      "  Batch 4,320  of  44,637.    Elapsed: 0:02:47. Training loss. 0.003105470910668373 Num fake examples 4170 Num true examples 4470\n",
      "  Batch 4,360  of  44,637.    Elapsed: 0:02:49. Training loss. 0.0038948948495090008 Num fake examples 4216 Num true examples 4504\n",
      "  Batch 4,400  of  44,637.    Elapsed: 0:02:50. Training loss. 0.002942918334156275 Num fake examples 4259 Num true examples 4541\n",
      "  Batch 4,440  of  44,637.    Elapsed: 0:02:52. Training loss. 0.0020912042818963528 Num fake examples 4299 Num true examples 4581\n",
      "  Batch 4,480  of  44,637.    Elapsed: 0:02:53. Training loss. 0.002585910726338625 Num fake examples 4334 Num true examples 4626\n",
      "  Batch 4,520  of  44,637.    Elapsed: 0:02:55. Training loss. 0.0019531091675162315 Num fake examples 4377 Num true examples 4663\n",
      "  Batch 4,560  of  44,637.    Elapsed: 0:02:57. Training loss. 0.0025733138900250196 Num fake examples 4415 Num true examples 4705\n",
      "  Batch 4,600  of  44,637.    Elapsed: 0:02:58. Training loss. 0.0026312905829399824 Num fake examples 4452 Num true examples 4748\n",
      "  Batch 4,640  of  44,637.    Elapsed: 0:03:00. Training loss. 0.002123947022482753 Num fake examples 4495 Num true examples 4785\n",
      "  Batch 4,680  of  44,637.    Elapsed: 0:03:02. Training loss. 0.0019931397400796413 Num fake examples 4524 Num true examples 4836\n",
      "  Batch 4,720  of  44,637.    Elapsed: 0:03:03. Training loss. 0.003264288417994976 Num fake examples 4566 Num true examples 4874\n",
      "  Batch 4,760  of  44,637.    Elapsed: 0:03:05. Training loss. 0.004160174168646336 Num fake examples 4610 Num true examples 4910\n",
      "  Batch 4,800  of  44,637.    Elapsed: 0:03:06. Training loss. 0.003228591289371252 Num fake examples 4653 Num true examples 4947\n",
      "  Batch 4,840  of  44,637.    Elapsed: 0:03:08. Training loss. 0.0033019003458321095 Num fake examples 4693 Num true examples 4987\n",
      "  Batch 4,880  of  44,637.    Elapsed: 0:03:10. Training loss. 0.0021523311734199524 Num fake examples 4730 Num true examples 5030\n",
      "  Batch 4,920  of  44,637.    Elapsed: 0:03:11. Training loss. 0.0018493437673896551 Num fake examples 4770 Num true examples 5070\n",
      "  Batch 4,960  of  44,637.    Elapsed: 0:03:13. Training loss. 0.0019449812825769186 Num fake examples 4804 Num true examples 5116\n",
      "  Batch 5,000  of  44,637.    Elapsed: 0:03:14. Training loss. 0.002626969013363123 Num fake examples 4846 Num true examples 5154\n",
      "  Batch 5,040  of  44,637.    Elapsed: 0:03:16. Training loss. 0.0018476077821105719 Num fake examples 4885 Num true examples 5195\n",
      "  Batch 5,080  of  44,637.    Elapsed: 0:03:17. Training loss. 0.0024439762346446514 Num fake examples 4920 Num true examples 5240\n",
      "  Batch 5,120  of  44,637.    Elapsed: 0:03:19. Training loss. 0.0026500157546252012 Num fake examples 4959 Num true examples 5281\n",
      "  Batch 5,160  of  44,637.    Elapsed: 0:03:21. Training loss. 0.0033173596020787954 Num fake examples 4998 Num true examples 5322\n",
      "  Batch 5,200  of  44,637.    Elapsed: 0:03:22. Training loss. 0.004397711716592312 Num fake examples 5041 Num true examples 5359\n",
      "  Batch 5,240  of  44,637.    Elapsed: 0:03:24. Training loss. 0.0017724601784721017 Num fake examples 5088 Num true examples 5392\n",
      "  Batch 5,280  of  44,637.    Elapsed: 0:03:25. Training loss. 0.003535602707415819 Num fake examples 5129 Num true examples 5431\n",
      "  Batch 5,320  of  44,637.    Elapsed: 0:03:27. Training loss. 0.004258577711880207 Num fake examples 5167 Num true examples 5473\n",
      "  Batch 5,360  of  44,637.    Elapsed: 0:03:28. Training loss. 0.004998121876269579 Num fake examples 5211 Num true examples 5509\n",
      "  Batch 5,400  of  44,637.    Elapsed: 0:03:30. Training loss. 2.7740914821624756 Num fake examples 5250 Num true examples 5550\n",
      "  Batch 5,440  of  44,637.    Elapsed: 0:03:32. Training loss. 0.009756817482411861 Num fake examples 5292 Num true examples 5588\n",
      "  Batch 5,480  of  44,637.    Elapsed: 0:03:33. Training loss. 0.008989911526441574 Num fake examples 5332 Num true examples 5628\n",
      "  Batch 5,520  of  44,637.    Elapsed: 0:03:35. Training loss. 0.008997260592877865 Num fake examples 5374 Num true examples 5666\n",
      "  Batch 5,560  of  44,637.    Elapsed: 0:03:36. Training loss. 0.006325292866677046 Num fake examples 5413 Num true examples 5707\n",
      "  Batch 5,600  of  44,637.    Elapsed: 0:03:38. Training loss. 0.0052090901881456375 Num fake examples 5456 Num true examples 5744\n",
      "  Batch 5,640  of  44,637.    Elapsed: 0:03:39. Training loss. 0.00626340601593256 Num fake examples 5495 Num true examples 5785\n",
      "  Batch 5,680  of  44,637.    Elapsed: 0:03:41. Training loss. 0.0034738825634121895 Num fake examples 5530 Num true examples 5830\n",
      "  Batch 5,720  of  44,637.    Elapsed: 0:03:43. Training loss. 0.0030812094919383526 Num fake examples 5567 Num true examples 5873\n",
      "  Batch 5,760  of  44,637.    Elapsed: 0:03:44. Training loss. 0.003979026339948177 Num fake examples 5607 Num true examples 5913\n",
      "  Batch 5,800  of  44,637.    Elapsed: 0:03:46. Training loss. 0.0027869981713593006 Num fake examples 5656 Num true examples 5944\n",
      "  Batch 5,840  of  44,637.    Elapsed: 0:03:47. Training loss. 0.002201902214437723 Num fake examples 5688 Num true examples 5992\n",
      "  Batch 5,880  of  44,637.    Elapsed: 0:03:49. Training loss. 0.0024718439672142267 Num fake examples 5730 Num true examples 6030\n",
      "  Batch 5,920  of  44,637.    Elapsed: 0:03:51. Training loss. 0.001815173076465726 Num fake examples 5766 Num true examples 6074\n",
      "  Batch 5,960  of  44,637.    Elapsed: 0:03:52. Training loss. 0.0027701943181455135 Num fake examples 5805 Num true examples 6115\n",
      "  Batch 6,000  of  44,637.    Elapsed: 0:03:54. Training loss. 0.0027365433052182198 Num fake examples 5842 Num true examples 6158\n",
      "  Batch 6,040  of  44,637.    Elapsed: 0:03:55. Training loss. 0.0016863495111465454 Num fake examples 5883 Num true examples 6197\n",
      "  Batch 6,080  of  44,637.    Elapsed: 0:03:57. Training loss. 0.0018240782665088773 Num fake examples 5930 Num true examples 6230\n",
      "  Batch 6,120  of  44,637.    Elapsed: 0:03:59. Training loss. 0.0031230992171913385 Num fake examples 5969 Num true examples 6271\n",
      "  Batch 6,160  of  44,637.    Elapsed: 0:04:00. Training loss. 0.0024461960420012474 Num fake examples 6010 Num true examples 6310\n",
      "  Batch 6,200  of  44,637.    Elapsed: 0:04:02. Training loss. 0.0030764678958803415 Num fake examples 6051 Num true examples 6349\n",
      "  Batch 6,240  of  44,637.    Elapsed: 0:04:03. Training loss. 0.002908079419285059 Num fake examples 6085 Num true examples 6395\n",
      "  Batch 6,280  of  44,637.    Elapsed: 0:04:05. Training loss. 0.002949274145066738 Num fake examples 6120 Num true examples 6440\n",
      "  Batch 6,320  of  44,637.    Elapsed: 0:04:07. Training loss. 0.0030888966284692287 Num fake examples 6149 Num true examples 6491\n",
      "  Batch 6,360  of  44,637.    Elapsed: 0:04:08. Training loss. 3.2725019454956055 Num fake examples 6187 Num true examples 6533\n",
      "  Batch 6,400  of  44,637.    Elapsed: 0:04:10. Training loss. 0.0023024939000606537 Num fake examples 6226 Num true examples 6574\n",
      "  Batch 6,440  of  44,637.    Elapsed: 0:04:11. Training loss. 0.0019798947032541037 Num fake examples 6257 Num true examples 6623\n",
      "  Batch 6,480  of  44,637.    Elapsed: 0:04:13. Training loss. 0.00394855160266161 Num fake examples 6294 Num true examples 6666\n",
      "  Batch 6,520  of  44,637.    Elapsed: 0:04:15. Training loss. 0.004463670775294304 Num fake examples 6333 Num true examples 6707\n",
      "  Batch 6,560  of  44,637.    Elapsed: 0:04:16. Training loss. 0.0037739151157438755 Num fake examples 6372 Num true examples 6748\n",
      "  Batch 6,600  of  44,637.    Elapsed: 0:04:18. Training loss. 2.999713659286499 Num fake examples 6407 Num true examples 6793\n",
      "  Batch 6,640  of  44,637.    Elapsed: 0:04:19. Training loss. 0.003163284156471491 Num fake examples 6443 Num true examples 6837\n",
      "  Batch 6,680  of  44,637.    Elapsed: 0:04:21. Training loss. 0.002690528752282262 Num fake examples 6481 Num true examples 6879\n",
      "  Batch 6,720  of  44,637.    Elapsed: 0:04:23. Training loss. 3.1328282356262207 Num fake examples 6518 Num true examples 6922\n",
      "  Batch 6,760  of  44,637.    Elapsed: 0:04:24. Training loss. 0.00448388047516346 Num fake examples 6557 Num true examples 6963\n",
      "  Batch 6,800  of  44,637.    Elapsed: 0:04:26. Training loss. 3.178286552429199 Num fake examples 6598 Num true examples 7002\n",
      "  Batch 6,840  of  44,637.    Elapsed: 0:04:27. Training loss. 0.002607380272820592 Num fake examples 6633 Num true examples 7047\n",
      "  Batch 6,880  of  44,637.    Elapsed: 0:04:29. Training loss. 0.004362276755273342 Num fake examples 6672 Num true examples 7088\n",
      "  Batch 6,920  of  44,637.    Elapsed: 0:04:30. Training loss. 0.005692055448889732 Num fake examples 6711 Num true examples 7129\n",
      "  Batch 6,960  of  44,637.    Elapsed: 0:04:32. Training loss. 2.6361680030822754 Num fake examples 6761 Num true examples 7159\n",
      "  Batch 7,000  of  44,637.    Elapsed: 0:04:34. Training loss. 0.006728576961904764 Num fake examples 6801 Num true examples 7199\n",
      "  Batch 7,040  of  44,637.    Elapsed: 0:04:35. Training loss. 0.003375143278390169 Num fake examples 6835 Num true examples 7245\n",
      "  Batch 7,080  of  44,637.    Elapsed: 0:04:37. Training loss. 0.0030942654702812433 Num fake examples 6877 Num true examples 7283\n",
      "  Batch 7,120  of  44,637.    Elapsed: 0:04:38. Training loss. 0.003844672814011574 Num fake examples 6919 Num true examples 7321\n",
      "  Batch 7,160  of  44,637.    Elapsed: 0:04:40. Training loss. 0.006586413364857435 Num fake examples 6953 Num true examples 7367\n",
      "  Batch 7,200  of  44,637.    Elapsed: 0:04:41. Training loss. 0.004054434597492218 Num fake examples 6982 Num true examples 7418\n",
      "  Batch 7,240  of  44,637.    Elapsed: 0:04:43. Training loss. 0.003640970215201378 Num fake examples 7018 Num true examples 7462\n",
      "  Batch 7,280  of  44,637.    Elapsed: 0:04:45. Training loss. 0.0033385837450623512 Num fake examples 7056 Num true examples 7504\n",
      "  Batch 7,320  of  44,637.    Elapsed: 0:04:46. Training loss. 0.003643958829343319 Num fake examples 7094 Num true examples 7546\n",
      "  Batch 7,360  of  44,637.    Elapsed: 0:04:48. Training loss. 0.003918803296983242 Num fake examples 7134 Num true examples 7586\n",
      "  Batch 7,400  of  44,637.    Elapsed: 0:04:49. Training loss. 0.0029512757901102304 Num fake examples 7173 Num true examples 7627\n",
      "  Batch 7,440  of  44,637.    Elapsed: 0:04:51. Training loss. 2.791663885116577 Num fake examples 7208 Num true examples 7672\n",
      "  Batch 7,480  of  44,637.    Elapsed: 0:04:52. Training loss. 0.002860268810763955 Num fake examples 7245 Num true examples 7715\n",
      "  Batch 7,520  of  44,637.    Elapsed: 0:04:54. Training loss. 0.0035363673232495785 Num fake examples 7282 Num true examples 7758\n",
      "  Batch 7,560  of  44,637.    Elapsed: 0:04:56. Training loss. 0.0035627931356430054 Num fake examples 7320 Num true examples 7800\n",
      "  Batch 7,600  of  44,637.    Elapsed: 0:04:57. Training loss. 0.0026608461048454046 Num fake examples 7353 Num true examples 7847\n",
      "  Batch 7,640  of  44,637.    Elapsed: 0:04:59. Training loss. 0.002807069104164839 Num fake examples 7390 Num true examples 7890\n",
      "  Batch 7,680  of  44,637.    Elapsed: 0:05:00. Training loss. 0.0028455231804400682 Num fake examples 7430 Num true examples 7930\n",
      "  Batch 7,720  of  44,637.    Elapsed: 0:05:02. Training loss. 0.0025446577928960323 Num fake examples 7468 Num true examples 7972\n",
      "  Batch 7,760  of  44,637.    Elapsed: 0:05:04. Training loss. 0.002451453823596239 Num fake examples 7512 Num true examples 8008\n",
      "  Batch 7,800  of  44,637.    Elapsed: 0:05:05. Training loss. 0.0020747436210513115 Num fake examples 7545 Num true examples 8055\n",
      "  Batch 7,840  of  44,637.    Elapsed: 0:05:07. Training loss. 0.003233358496800065 Num fake examples 7588 Num true examples 8092\n",
      "  Batch 7,880  of  44,637.    Elapsed: 0:05:08. Training loss. 0.0024671617429703474 Num fake examples 7632 Num true examples 8128\n",
      "  Batch 7,920  of  44,637.    Elapsed: 0:05:10. Training loss. 0.003516428405418992 Num fake examples 7673 Num true examples 8167\n",
      "  Batch 7,960  of  44,637.    Elapsed: 0:05:12. Training loss. 0.00364125263877213 Num fake examples 7716 Num true examples 8204\n",
      "  Batch 8,000  of  44,637.    Elapsed: 0:05:13. Training loss. 0.004239760804921389 Num fake examples 7753 Num true examples 8247\n",
      "  Batch 8,040  of  44,637.    Elapsed: 0:05:15. Training loss. 0.0034316699020564556 Num fake examples 7797 Num true examples 8283\n",
      "  Batch 8,080  of  44,637.    Elapsed: 0:05:16. Training loss. 0.002880982356145978 Num fake examples 7840 Num true examples 8320\n",
      "  Batch 8,120  of  44,637.    Elapsed: 0:05:18. Training loss. 0.002822485752403736 Num fake examples 7881 Num true examples 8359\n",
      "  Batch 8,160  of  44,637.    Elapsed: 0:05:20. Training loss. 0.0018977796426042914 Num fake examples 7925 Num true examples 8395\n",
      "  Batch 8,200  of  44,637.    Elapsed: 0:05:21. Training loss. 0.004141602665185928 Num fake examples 7962 Num true examples 8438\n",
      "  Batch 8,240  of  44,637.    Elapsed: 0:05:23. Training loss. 0.0024405037984251976 Num fake examples 8004 Num true examples 8476\n",
      "  Batch 8,280  of  44,637.    Elapsed: 0:05:24. Training loss. 0.001561139477416873 Num fake examples 8037 Num true examples 8523\n",
      "  Batch 8,320  of  44,637.    Elapsed: 0:05:26. Training loss. 0.002853156765922904 Num fake examples 8067 Num true examples 8573\n",
      "  Batch 8,360  of  44,637.    Elapsed: 0:05:27. Training loss. 0.0023732215631753206 Num fake examples 8102 Num true examples 8618\n",
      "  Batch 8,400  of  44,637.    Elapsed: 0:05:29. Training loss. 0.0026320992037653923 Num fake examples 8137 Num true examples 8663\n",
      "  Batch 8,440  of  44,637.    Elapsed: 0:05:31. Training loss. 0.0019229559693485498 Num fake examples 8178 Num true examples 8702\n",
      "  Batch 8,480  of  44,637.    Elapsed: 0:05:32. Training loss. 0.0021283754613250494 Num fake examples 8219 Num true examples 8741\n",
      "  Batch 8,520  of  44,637.    Elapsed: 0:05:34. Training loss. 0.0020256349816918373 Num fake examples 8252 Num true examples 8788\n",
      "  Batch 8,560  of  44,637.    Elapsed: 0:05:35. Training loss. 0.002085549756884575 Num fake examples 8292 Num true examples 8828\n",
      "  Batch 8,600  of  44,637.    Elapsed: 0:05:37. Training loss. 0.0013093609595671296 Num fake examples 8334 Num true examples 8866\n",
      "  Batch 8,640  of  44,637.    Elapsed: 0:05:38. Training loss. 0.00259115151129663 Num fake examples 8368 Num true examples 8912\n",
      "  Batch 8,680  of  44,637.    Elapsed: 0:05:40. Training loss. 0.0026111833285540342 Num fake examples 8417 Num true examples 8943\n",
      "  Batch 8,720  of  44,637.    Elapsed: 0:05:42. Training loss. 0.002405749633908272 Num fake examples 8449 Num true examples 8991\n",
      "  Batch 8,760  of  44,637.    Elapsed: 0:05:43. Training loss. 0.0020341756753623486 Num fake examples 8488 Num true examples 9032\n",
      "  Batch 8,800  of  44,637.    Elapsed: 0:05:45. Training loss. 0.0014008337166160345 Num fake examples 8541 Num true examples 9059\n",
      "  Batch 8,840  of  44,637.    Elapsed: 0:05:46. Training loss. 0.0015325339045375586 Num fake examples 8580 Num true examples 9100\n",
      "  Batch 8,880  of  44,637.    Elapsed: 0:05:48. Training loss. 0.0021358535159379244 Num fake examples 8621 Num true examples 9139\n",
      "  Batch 8,920  of  44,637.    Elapsed: 0:05:50. Training loss. 0.0029783209320157766 Num fake examples 8659 Num true examples 9181\n",
      "  Batch 8,960  of  44,637.    Elapsed: 0:05:51. Training loss. 0.001646687975153327 Num fake examples 8694 Num true examples 9226\n",
      "  Batch 9,000  of  44,637.    Elapsed: 0:05:53. Training loss. 0.0022289350163191557 Num fake examples 8736 Num true examples 9264\n",
      "  Batch 9,040  of  44,637.    Elapsed: 0:05:54. Training loss. 0.002255767583847046 Num fake examples 8775 Num true examples 9305\n",
      "  Batch 9,080  of  44,637.    Elapsed: 0:05:56. Training loss. 0.0024994832929223776 Num fake examples 8816 Num true examples 9344\n",
      "  Batch 9,120  of  44,637.    Elapsed: 0:05:57. Training loss. 0.002515264321118593 Num fake examples 8855 Num true examples 9385\n",
      "  Batch 9,160  of  44,637.    Elapsed: 0:05:59. Training loss. 0.003951845690608025 Num fake examples 8893 Num true examples 9427\n",
      "  Batch 9,200  of  44,637.    Elapsed: 0:06:01. Training loss. 0.003799048950895667 Num fake examples 8938 Num true examples 9462\n",
      "  Batch 9,240  of  44,637.    Elapsed: 0:06:02. Training loss. 0.004002176225185394 Num fake examples 8983 Num true examples 9497\n",
      "  Batch 9,280  of  44,637.    Elapsed: 0:06:04. Training loss. 0.0030981814488768578 Num fake examples 9024 Num true examples 9536\n",
      "  Batch 9,320  of  44,637.    Elapsed: 0:06:05. Training loss. 0.0016172691248357296 Num fake examples 9067 Num true examples 9573\n",
      "  Batch 9,360  of  44,637.    Elapsed: 0:06:07. Training loss. 0.0021213050931692123 Num fake examples 9106 Num true examples 9614\n",
      "  Batch 9,400  of  44,637.    Elapsed: 0:06:09. Training loss. 0.002392882714048028 Num fake examples 9146 Num true examples 9654\n",
      "  Batch 9,440  of  44,637.    Elapsed: 0:06:10. Training loss. 0.0022413332480937243 Num fake examples 9181 Num true examples 9699\n",
      "  Batch 9,480  of  44,637.    Elapsed: 0:06:12. Training loss. 0.0019775074906647205 Num fake examples 9213 Num true examples 9747\n",
      "  Batch 9,520  of  44,637.    Elapsed: 0:06:13. Training loss. 0.001837775344029069 Num fake examples 9255 Num true examples 9785\n",
      "  Batch 9,560  of  44,637.    Elapsed: 0:06:15. Training loss. 0.004541358910501003 Num fake examples 9295 Num true examples 9825\n",
      "  Batch 9,600  of  44,637.    Elapsed: 0:06:16. Training loss. 0.004349905997514725 Num fake examples 9334 Num true examples 9866\n",
      "  Batch 9,640  of  44,637.    Elapsed: 0:06:18. Training loss. 0.004130777437239885 Num fake examples 9367 Num true examples 9913\n",
      "  Batch 9,680  of  44,637.    Elapsed: 0:06:20. Training loss. 0.0035891325678676367 Num fake examples 9404 Num true examples 9956\n",
      "  Batch 9,720  of  44,637.    Elapsed: 0:06:21. Training loss. 0.0013775021070614457 Num fake examples 9438 Num true examples 10002\n",
      "  Batch 9,760  of  44,637.    Elapsed: 0:06:23. Training loss. 0.001379885827191174 Num fake examples 9477 Num true examples 10043\n",
      "  Batch 9,800  of  44,637.    Elapsed: 0:06:24. Training loss. 0.0025096889585256577 Num fake examples 9512 Num true examples 10088\n",
      "  Batch 9,840  of  44,637.    Elapsed: 0:06:26. Training loss. 0.005379774607717991 Num fake examples 9560 Num true examples 10120\n",
      "  Batch 9,880  of  44,637.    Elapsed: 0:06:28. Training loss. 0.0038872240111231804 Num fake examples 9595 Num true examples 10165\n",
      "  Batch 9,920  of  44,637.    Elapsed: 0:06:29. Training loss. 0.005032674875110388 Num fake examples 9640 Num true examples 10200\n",
      "  Batch 9,960  of  44,637.    Elapsed: 0:06:31. Training loss. 0.001665982068516314 Num fake examples 9675 Num true examples 10245\n",
      "  Batch 10,000  of  44,637.    Elapsed: 0:06:32. Training loss. 0.00400533527135849 Num fake examples 9710 Num true examples 10290\n",
      "  Batch 10,040  of  44,637.    Elapsed: 0:06:34. Training loss. 0.002430819673463702 Num fake examples 9750 Num true examples 10330\n",
      "  Batch 10,080  of  44,637.    Elapsed: 0:06:36. Training loss. 0.00462120957672596 Num fake examples 9789 Num true examples 10371\n",
      "  Batch 10,120  of  44,637.    Elapsed: 0:06:37. Training loss. 0.0029267300851643085 Num fake examples 9832 Num true examples 10408\n",
      "  Batch 10,160  of  44,637.    Elapsed: 0:06:39. Training loss. 0.0032699769362807274 Num fake examples 9867 Num true examples 10453\n",
      "  Batch 10,200  of  44,637.    Elapsed: 0:06:40. Training loss. 0.0033235219307243824 Num fake examples 9909 Num true examples 10491\n",
      "  Batch 10,240  of  44,637.    Elapsed: 0:06:42. Training loss. 0.0030943499878048897 Num fake examples 9945 Num true examples 10535\n",
      "  Batch 10,280  of  44,637.    Elapsed: 0:06:44. Training loss. 0.0020091545302420855 Num fake examples 9980 Num true examples 10580\n",
      "  Batch 10,320  of  44,637.    Elapsed: 0:06:45. Training loss. 0.0014448401052504778 Num fake examples 10015 Num true examples 10625\n",
      "  Batch 10,360  of  44,637.    Elapsed: 0:06:47. Training loss. 0.0014911906328052282 Num fake examples 10051 Num true examples 10669\n",
      "  Batch 10,400  of  44,637.    Elapsed: 0:06:48. Training loss. 0.0019031212432309985 Num fake examples 10080 Num true examples 10720\n",
      "  Batch 10,440  of  44,637.    Elapsed: 0:06:50. Training loss. 0.002322540385648608 Num fake examples 10120 Num true examples 10760\n",
      "  Batch 10,480  of  44,637.    Elapsed: 0:06:52. Training loss. 0.0025063983630388975 Num fake examples 10166 Num true examples 10794\n",
      "  Batch 10,520  of  44,637.    Elapsed: 0:06:53. Training loss. 0.0026871596928685904 Num fake examples 10204 Num true examples 10836\n",
      "  Batch 10,560  of  44,637.    Elapsed: 0:06:55. Training loss. 0.0033894693478941917 Num fake examples 10245 Num true examples 10875\n",
      "  Batch 10,600  of  44,637.    Elapsed: 0:06:56. Training loss. 0.0030210325494408607 Num fake examples 10274 Num true examples 10926\n",
      "  Batch 10,640  of  44,637.    Elapsed: 0:06:58. Training loss. 0.0034397770650684834 Num fake examples 10310 Num true examples 10970\n",
      "  Batch 10,680  of  44,637.    Elapsed: 0:07:00. Training loss. 0.0022634081542491913 Num fake examples 10340 Num true examples 11020\n",
      "  Batch 10,720  of  44,637.    Elapsed: 0:07:01. Training loss. 0.002896092366427183 Num fake examples 10377 Num true examples 11063\n",
      "  Batch 10,760  of  44,637.    Elapsed: 0:07:03. Training loss. 0.0021168997045606375 Num fake examples 10413 Num true examples 11107\n",
      "  Batch 10,800  of  44,637.    Elapsed: 0:07:04. Training loss. 0.0021748761646449566 Num fake examples 10448 Num true examples 11152\n",
      "  Batch 10,840  of  44,637.    Elapsed: 0:07:06. Training loss. 0.0019030801486223936 Num fake examples 10489 Num true examples 11191\n",
      "  Batch 10,880  of  44,637.    Elapsed: 0:07:07. Training loss. 0.0019341859733685851 Num fake examples 10526 Num true examples 11234\n",
      "  Batch 10,920  of  44,637.    Elapsed: 0:07:09. Training loss. 0.0027219606563448906 Num fake examples 10564 Num true examples 11276\n",
      "  Batch 10,960  of  44,637.    Elapsed: 0:07:11. Training loss. 2.899688482284546 Num fake examples 10603 Num true examples 11317\n",
      "  Batch 11,000  of  44,637.    Elapsed: 0:07:12. Training loss. 0.003461967222392559 Num fake examples 10644 Num true examples 11356\n",
      "  Batch 11,040  of  44,637.    Elapsed: 0:07:14. Training loss. 2.9211323261260986 Num fake examples 10680 Num true examples 11400\n",
      "  Batch 11,080  of  44,637.    Elapsed: 0:07:15. Training loss. 0.0033263596706092358 Num fake examples 10727 Num true examples 11433\n",
      "  Batch 11,120  of  44,637.    Elapsed: 0:07:17. Training loss. 0.0029457095079123974 Num fake examples 10773 Num true examples 11467\n",
      "  Batch 11,160  of  44,637.    Elapsed: 0:07:19. Training loss. 0.0033208224922418594 Num fake examples 10814 Num true examples 11506\n",
      "  Batch 11,200  of  44,637.    Elapsed: 0:07:20. Training loss. 0.0016143580432981253 Num fake examples 10848 Num true examples 11552\n",
      "  Batch 11,240  of  44,637.    Elapsed: 0:07:22. Training loss. 0.0023476502392441034 Num fake examples 10885 Num true examples 11595\n",
      "  Batch 11,280  of  44,637.    Elapsed: 0:07:23. Training loss. 0.003471445059403777 Num fake examples 10920 Num true examples 11640\n",
      "  Batch 11,320  of  44,637.    Elapsed: 0:07:25. Training loss. 0.0026992177590727806 Num fake examples 10965 Num true examples 11675\n",
      "  Batch 11,360  of  44,637.    Elapsed: 0:07:26. Training loss. 3.1344387531280518 Num fake examples 11004 Num true examples 11716\n",
      "  Batch 11,400  of  44,637.    Elapsed: 0:07:28. Training loss. 0.0025824683252722025 Num fake examples 11035 Num true examples 11765\n",
      "  Batch 11,440  of  44,637.    Elapsed: 0:07:30. Training loss. 0.003257438773289323 Num fake examples 11078 Num true examples 11802\n",
      "  Batch 11,480  of  44,637.    Elapsed: 0:07:31. Training loss. 0.0025696498341858387 Num fake examples 11107 Num true examples 11853\n",
      "  Batch 11,520  of  44,637.    Elapsed: 0:07:33. Training loss. 0.0014987135073170066 Num fake examples 11147 Num true examples 11893\n",
      "  Batch 11,560  of  44,637.    Elapsed: 0:07:34. Training loss. 0.0038877706974744797 Num fake examples 11189 Num true examples 11931\n",
      "  Batch 11,600  of  44,637.    Elapsed: 0:07:36. Training loss. 0.0044404263608157635 Num fake examples 11231 Num true examples 11969\n",
      "  Batch 11,640  of  44,637.    Elapsed: 0:07:37. Training loss. 2.545316219329834 Num fake examples 11263 Num true examples 12017\n",
      "  Batch 11,680  of  44,637.    Elapsed: 0:07:39. Training loss. 0.006881983019411564 Num fake examples 11299 Num true examples 12061\n",
      "  Batch 11,720  of  44,637.    Elapsed: 0:07:41. Training loss. 0.004882632754743099 Num fake examples 11341 Num true examples 12099\n",
      "  Batch 11,760  of  44,637.    Elapsed: 0:07:42. Training loss. 0.007275160867720842 Num fake examples 11380 Num true examples 12140\n",
      "  Batch 11,800  of  44,637.    Elapsed: 0:07:44. Training loss. 0.00450937869027257 Num fake examples 11419 Num true examples 12181\n",
      "  Batch 11,840  of  44,637.    Elapsed: 0:07:45. Training loss. 0.004840625450015068 Num fake examples 11460 Num true examples 12220\n",
      "  Batch 11,880  of  44,637.    Elapsed: 0:07:47. Training loss. 0.0035511679016053677 Num fake examples 11501 Num true examples 12259\n",
      "  Batch 11,920  of  44,637.    Elapsed: 0:07:49. Training loss. 0.005025523714721203 Num fake examples 11545 Num true examples 12295\n",
      "  Batch 11,960  of  44,637.    Elapsed: 0:07:50. Training loss. 0.0038352145347744226 Num fake examples 11579 Num true examples 12341\n",
      "  Batch 12,000  of  44,637.    Elapsed: 0:07:52. Training loss. 0.003100259229540825 Num fake examples 11625 Num true examples 12375\n",
      "  Batch 12,040  of  44,637.    Elapsed: 0:07:53. Training loss. 0.004839383997023106 Num fake examples 11669 Num true examples 12411\n",
      "  Batch 12,080  of  44,637.    Elapsed: 0:07:55. Training loss. 0.004988234490156174 Num fake examples 11706 Num true examples 12454\n",
      "  Batch 12,120  of  44,637.    Elapsed: 0:07:56. Training loss. 0.003282225225120783 Num fake examples 11755 Num true examples 12485\n",
      "  Batch 12,160  of  44,637.    Elapsed: 0:07:58. Training loss. 0.002812210703268647 Num fake examples 11799 Num true examples 12521\n",
      "  Batch 12,200  of  44,637.    Elapsed: 0:08:00. Training loss. 2.705152750015259 Num fake examples 11834 Num true examples 12566\n",
      "  Batch 12,240  of  44,637.    Elapsed: 0:08:01. Training loss. 0.0029233095701783895 Num fake examples 11880 Num true examples 12600\n",
      "  Batch 12,280  of  44,637.    Elapsed: 0:08:03. Training loss. 0.0038309183437377214 Num fake examples 11917 Num true examples 12643\n",
      "  Batch 12,320  of  44,637.    Elapsed: 0:08:04. Training loss. 0.004492626991122961 Num fake examples 11954 Num true examples 12686\n",
      "  Batch 12,360  of  44,637.    Elapsed: 0:08:06. Training loss. 0.003224440850317478 Num fake examples 11993 Num true examples 12727\n",
      "  Batch 12,400  of  44,637.    Elapsed: 0:08:08. Training loss. 0.00367025681771338 Num fake examples 12029 Num true examples 12771\n",
      "  Batch 12,440  of  44,637.    Elapsed: 0:08:09. Training loss. 0.0036353350151330233 Num fake examples 12066 Num true examples 12814\n",
      "  Batch 12,480  of  44,637.    Elapsed: 0:08:11. Training loss. 0.0024509108625352383 Num fake examples 12094 Num true examples 12866\n",
      "  Batch 12,520  of  44,637.    Elapsed: 0:08:12. Training loss. 0.002460331888869405 Num fake examples 12131 Num true examples 12909\n",
      "  Batch 12,560  of  44,637.    Elapsed: 0:08:14. Training loss. 2.844417095184326 Num fake examples 12170 Num true examples 12950\n",
      "  Batch 12,600  of  44,637.    Elapsed: 0:08:15. Training loss. 0.0022194599732756615 Num fake examples 12202 Num true examples 12998\n",
      "  Batch 12,640  of  44,637.    Elapsed: 0:08:17. Training loss. 0.0023380755446851254 Num fake examples 12247 Num true examples 13033\n",
      "  Batch 12,680  of  44,637.    Elapsed: 0:08:19. Training loss. 0.003788950853049755 Num fake examples 12280 Num true examples 13080\n",
      "  Batch 12,720  of  44,637.    Elapsed: 0:08:20. Training loss. 0.002828245284035802 Num fake examples 12318 Num true examples 13122\n",
      "  Batch 12,760  of  44,637.    Elapsed: 0:08:22. Training loss. 0.0022666428703814745 Num fake examples 12354 Num true examples 13166\n",
      "  Batch 12,800  of  44,637.    Elapsed: 0:08:23. Training loss. 0.0015649504493921995 Num fake examples 12396 Num true examples 13204\n",
      "  Batch 12,840  of  44,637.    Elapsed: 0:08:25. Training loss. 0.0020730122923851013 Num fake examples 12439 Num true examples 13241\n",
      "  Batch 12,880  of  44,637.    Elapsed: 0:08:26. Training loss. 0.002985983155667782 Num fake examples 12474 Num true examples 13286\n",
      "  Batch 12,920  of  44,637.    Elapsed: 0:08:28. Training loss. 0.002338399412110448 Num fake examples 12522 Num true examples 13318\n",
      "  Batch 12,960  of  44,637.    Elapsed: 0:08:30. Training loss. 0.0015710275620222092 Num fake examples 12558 Num true examples 13362\n",
      "  Batch 13,000  of  44,637.    Elapsed: 0:08:31. Training loss. 0.002472149208188057 Num fake examples 12599 Num true examples 13401\n",
      "  Batch 13,040  of  44,637.    Elapsed: 0:08:33. Training loss. 0.0016634045168757439 Num fake examples 12647 Num true examples 13433\n",
      "  Batch 13,080  of  44,637.    Elapsed: 0:08:34. Training loss. 3.3060457706451416 Num fake examples 12680 Num true examples 13480\n",
      "  Batch 13,120  of  44,637.    Elapsed: 0:08:36. Training loss. 0.002748639788478613 Num fake examples 12722 Num true examples 13518\n",
      "  Batch 13,160  of  44,637.    Elapsed: 0:08:38. Training loss. 0.0031295963563024998 Num fake examples 12766 Num true examples 13554\n",
      "  Batch 13,200  of  44,637.    Elapsed: 0:08:39. Training loss. 0.0028031296096742153 Num fake examples 12803 Num true examples 13597\n",
      "  Batch 13,240  of  44,637.    Elapsed: 0:08:41. Training loss. 0.0023323134519159794 Num fake examples 12845 Num true examples 13635\n",
      "  Batch 13,280  of  44,637.    Elapsed: 0:08:42. Training loss. 0.0021461278665810823 Num fake examples 12880 Num true examples 13680\n",
      "  Batch 13,320  of  44,637.    Elapsed: 0:08:44. Training loss. 0.00223229150287807 Num fake examples 12923 Num true examples 13717\n",
      "  Batch 13,360  of  44,637.    Elapsed: 0:08:45. Training loss. 0.0023788264952600002 Num fake examples 12955 Num true examples 13765\n",
      "  Batch 13,400  of  44,637.    Elapsed: 0:08:47. Training loss. 0.0017108976608142257 Num fake examples 12984 Num true examples 13816\n",
      "  Batch 13,440  of  44,637.    Elapsed: 0:08:49. Training loss. 0.002442380180582404 Num fake examples 13021 Num true examples 13859\n",
      "  Batch 13,480  of  44,637.    Elapsed: 0:08:50. Training loss. 0.00144448212813586 Num fake examples 13062 Num true examples 13898\n",
      "  Batch 13,520  of  44,637.    Elapsed: 0:08:52. Training loss. 0.002138744341209531 Num fake examples 13104 Num true examples 13936\n",
      "  Batch 13,560  of  44,637.    Elapsed: 0:08:53. Training loss. 3.047279119491577 Num fake examples 13144 Num true examples 13976\n",
      "  Batch 13,600  of  44,637.    Elapsed: 0:08:55. Training loss. 0.001882584998384118 Num fake examples 13178 Num true examples 14022\n",
      "  Batch 13,640  of  44,637.    Elapsed: 0:08:56. Training loss. 2.8721089363098145 Num fake examples 13219 Num true examples 14061\n",
      "  Batch 13,680  of  44,637.    Elapsed: 0:08:58. Training loss. 0.0055652642622590065 Num fake examples 13258 Num true examples 14102\n",
      "  Batch 13,720  of  44,637.    Elapsed: 0:09:00. Training loss. 0.0034466227516531944 Num fake examples 13292 Num true examples 14148\n",
      "  Batch 13,760  of  44,637.    Elapsed: 0:09:01. Training loss. 0.001889206119813025 Num fake examples 13328 Num true examples 14192\n",
      "  Batch 13,800  of  44,637.    Elapsed: 0:09:03. Training loss. 0.0018125653732568026 Num fake examples 13368 Num true examples 14232\n",
      "  Batch 13,840  of  44,637.    Elapsed: 0:09:04. Training loss. 0.003383160335943103 Num fake examples 13404 Num true examples 14276\n",
      "  Batch 13,880  of  44,637.    Elapsed: 0:09:06. Training loss. 0.002970863599330187 Num fake examples 13442 Num true examples 14318\n",
      "  Batch 13,920  of  44,637.    Elapsed: 0:09:08. Training loss. 0.0016644655261188745 Num fake examples 13474 Num true examples 14366\n",
      "  Batch 13,960  of  44,637.    Elapsed: 0:09:09. Training loss. 0.003048516344279051 Num fake examples 13516 Num true examples 14404\n",
      "  Batch 14,000  of  44,637.    Elapsed: 0:09:11. Training loss. 0.0051064928993582726 Num fake examples 13558 Num true examples 14442\n",
      "  Batch 14,040  of  44,637.    Elapsed: 0:09:12. Training loss. 0.0033682263456285 Num fake examples 13597 Num true examples 14483\n",
      "  Batch 14,080  of  44,637.    Elapsed: 0:09:14. Training loss. 0.003197652753442526 Num fake examples 13637 Num true examples 14523\n",
      "  Batch 14,120  of  44,637.    Elapsed: 0:09:15. Training loss. 0.0022248434834182262 Num fake examples 13674 Num true examples 14566\n",
      "  Batch 14,160  of  44,637.    Elapsed: 0:09:17. Training loss. 0.0019854209385812283 Num fake examples 13713 Num true examples 14607\n",
      "  Batch 14,200  of  44,637.    Elapsed: 0:09:19. Training loss. 0.002527917269617319 Num fake examples 13755 Num true examples 14645\n",
      "  Batch 14,240  of  44,637.    Elapsed: 0:09:20. Training loss. 0.004402135033160448 Num fake examples 13790 Num true examples 14690\n",
      "  Batch 14,280  of  44,637.    Elapsed: 0:09:22. Training loss. 0.004590404219925404 Num fake examples 13833 Num true examples 14727\n",
      "  Batch 14,320  of  44,637.    Elapsed: 0:09:23. Training loss. 0.00251571461558342 Num fake examples 13869 Num true examples 14771\n",
      "  Batch 14,360  of  44,637.    Elapsed: 0:09:25. Training loss. 3.1925008296966553 Num fake examples 13918 Num true examples 14802\n",
      "  Batch 14,400  of  44,637.    Elapsed: 0:09:26. Training loss. 0.004100222140550613 Num fake examples 13951 Num true examples 14849\n",
      "  Batch 14,440  of  44,637.    Elapsed: 0:09:28. Training loss. 0.004426809959113598 Num fake examples 13983 Num true examples 14897\n",
      "  Batch 14,480  of  44,637.    Elapsed: 0:09:30. Training loss. 2.954718589782715 Num fake examples 14028 Num true examples 14932\n",
      "  Batch 14,520  of  44,637.    Elapsed: 0:09:31. Training loss. 0.004131525754928589 Num fake examples 14064 Num true examples 14976\n",
      "  Batch 14,560  of  44,637.    Elapsed: 0:09:33. Training loss. 0.004197108093649149 Num fake examples 14102 Num true examples 15018\n",
      "  Batch 14,600  of  44,637.    Elapsed: 0:09:34. Training loss. 0.004443050362169743 Num fake examples 14134 Num true examples 15066\n",
      "  Batch 14,640  of  44,637.    Elapsed: 0:09:36. Training loss. 0.004738847725093365 Num fake examples 14175 Num true examples 15105\n",
      "  Batch 14,680  of  44,637.    Elapsed: 0:09:38. Training loss. 0.007247713394463062 Num fake examples 14207 Num true examples 15153\n",
      "  Batch 14,720  of  44,637.    Elapsed: 0:09:39. Training loss. 0.0027287297416478395 Num fake examples 14241 Num true examples 15199\n",
      "  Batch 14,760  of  44,637.    Elapsed: 0:09:41. Training loss. 0.00421702116727829 Num fake examples 14288 Num true examples 15232\n",
      "  Batch 14,800  of  44,637.    Elapsed: 0:09:42. Training loss. 2.5036985874176025 Num fake examples 14331 Num true examples 15269\n",
      "  Batch 14,840  of  44,637.    Elapsed: 0:09:44. Training loss. 0.003974658437073231 Num fake examples 14370 Num true examples 15310\n",
      "  Batch 14,880  of  44,637.    Elapsed: 0:09:46. Training loss. 0.007365864235907793 Num fake examples 14413 Num true examples 15347\n",
      "  Batch 14,920  of  44,637.    Elapsed: 0:09:47. Training loss. 0.0070215677842497826 Num fake examples 14448 Num true examples 15392\n",
      "  Batch 14,960  of  44,637.    Elapsed: 0:09:49. Training loss. 0.004951801151037216 Num fake examples 14489 Num true examples 15431\n",
      "  Batch 15,000  of  44,637.    Elapsed: 0:09:50. Training loss. 0.0038294438272714615 Num fake examples 14525 Num true examples 15475\n",
      "  Batch 15,040  of  44,637.    Elapsed: 0:09:52. Training loss. 0.0036987080238759518 Num fake examples 14567 Num true examples 15513\n",
      "  Batch 15,080  of  44,637.    Elapsed: 0:09:53. Training loss. 0.003362359246239066 Num fake examples 14611 Num true examples 15549\n",
      "  Batch 15,120  of  44,637.    Elapsed: 0:09:55. Training loss. 0.003374432446435094 Num fake examples 14648 Num true examples 15592\n",
      "  Batch 15,160  of  44,637.    Elapsed: 0:09:57. Training loss. 0.005214508157223463 Num fake examples 14683 Num true examples 15637\n",
      "  Batch 15,200  of  44,637.    Elapsed: 0:09:58. Training loss. 0.0029380095656961203 Num fake examples 14720 Num true examples 15680\n",
      "  Batch 15,240  of  44,637.    Elapsed: 0:10:00. Training loss. 0.003870027605444193 Num fake examples 14754 Num true examples 15726\n",
      "  Batch 15,280  of  44,637.    Elapsed: 0:10:01. Training loss. 0.0041631776839494705 Num fake examples 14792 Num true examples 15768\n",
      "  Batch 15,320  of  44,637.    Elapsed: 0:10:03. Training loss. 0.0066492585465312 Num fake examples 14837 Num true examples 15803\n",
      "  Batch 15,360  of  44,637.    Elapsed: 0:10:04. Training loss. 0.0043601347133517265 Num fake examples 14869 Num true examples 15851\n",
      "  Batch 15,400  of  44,637.    Elapsed: 0:10:06. Training loss. 0.005466064438223839 Num fake examples 14903 Num true examples 15897\n",
      "  Batch 15,440  of  44,637.    Elapsed: 0:10:08. Training loss. 0.003916567657142878 Num fake examples 14937 Num true examples 15943\n",
      "  Batch 15,480  of  44,637.    Elapsed: 0:10:09. Training loss. 2.7033660411834717 Num fake examples 14981 Num true examples 15979\n",
      "  Batch 15,520  of  44,637.    Elapsed: 0:10:11. Training loss. 0.008502183482050896 Num fake examples 15019 Num true examples 16021\n",
      "  Batch 15,560  of  44,637.    Elapsed: 0:10:12. Training loss. 0.0024248440749943256 Num fake examples 15057 Num true examples 16063\n",
      "  Batch 15,600  of  44,637.    Elapsed: 0:10:14. Training loss. 0.002676304429769516 Num fake examples 15094 Num true examples 16106\n",
      "  Batch 15,640  of  44,637.    Elapsed: 0:10:16. Training loss. 0.003542932914569974 Num fake examples 15128 Num true examples 16152\n",
      "  Batch 15,680  of  44,637.    Elapsed: 0:10:17. Training loss. 0.005244814790785313 Num fake examples 15170 Num true examples 16190\n",
      "  Batch 15,720  of  44,637.    Elapsed: 0:10:19. Training loss. 0.004917528945952654 Num fake examples 15209 Num true examples 16231\n",
      "  Batch 15,760  of  44,637.    Elapsed: 0:10:20. Training loss. 0.0038537667132914066 Num fake examples 15254 Num true examples 16266\n",
      "  Batch 15,800  of  44,637.    Elapsed: 0:10:22. Training loss. 0.002308465540409088 Num fake examples 15296 Num true examples 16304\n",
      "  Batch 15,840  of  44,637.    Elapsed: 0:10:23. Training loss. 0.00576606672257185 Num fake examples 15339 Num true examples 16341\n",
      "  Batch 15,880  of  44,637.    Elapsed: 0:10:25. Training loss. 0.004440554417669773 Num fake examples 15375 Num true examples 16385\n",
      "  Batch 15,920  of  44,637.    Elapsed: 0:10:27. Training loss. 0.0035631279461085796 Num fake examples 15409 Num true examples 16431\n",
      "  Batch 15,960  of  44,637.    Elapsed: 0:10:28. Training loss. 0.004398527555167675 Num fake examples 15446 Num true examples 16474\n",
      "  Batch 16,000  of  44,637.    Elapsed: 0:10:30. Training loss. 0.0027607916854321957 Num fake examples 15485 Num true examples 16515\n",
      "  Batch 16,040  of  44,637.    Elapsed: 0:10:31. Training loss. 0.005594559945166111 Num fake examples 15520 Num true examples 16560\n",
      "  Batch 16,080  of  44,637.    Elapsed: 0:10:33. Training loss. 2.4815330505371094 Num fake examples 15558 Num true examples 16602\n",
      "  Batch 16,120  of  44,637.    Elapsed: 0:10:35. Training loss. 0.003973158076405525 Num fake examples 15599 Num true examples 16641\n",
      "  Batch 16,160  of  44,637.    Elapsed: 0:10:36. Training loss. 0.0037894719280302525 Num fake examples 15630 Num true examples 16690\n",
      "  Batch 16,200  of  44,637.    Elapsed: 0:10:38. Training loss. 0.004950891248881817 Num fake examples 15664 Num true examples 16736\n",
      "  Batch 16,240  of  44,637.    Elapsed: 0:10:39. Training loss. 0.002971785143017769 Num fake examples 15697 Num true examples 16783\n",
      "  Batch 16,280  of  44,637.    Elapsed: 0:10:41. Training loss. 0.003828645683825016 Num fake examples 15737 Num true examples 16823\n",
      "  Batch 16,320  of  44,637.    Elapsed: 0:10:43. Training loss. 0.0034790176432579756 Num fake examples 15775 Num true examples 16865\n",
      "  Batch 16,360  of  44,637.    Elapsed: 0:10:44. Training loss. 0.0025715187657624483 Num fake examples 15814 Num true examples 16906\n",
      "  Batch 16,400  of  44,637.    Elapsed: 0:10:46. Training loss. 0.0036369701847434044 Num fake examples 15847 Num true examples 16953\n",
      "  Batch 16,440  of  44,637.    Elapsed: 0:10:47. Training loss. 0.0032076328061521053 Num fake examples 15895 Num true examples 16985\n",
      "  Batch 16,480  of  44,637.    Elapsed: 0:10:49. Training loss. 0.0025266974698752165 Num fake examples 15934 Num true examples 17026\n",
      "  Batch 16,520  of  44,637.    Elapsed: 0:10:51. Training loss. 0.002644481137394905 Num fake examples 15970 Num true examples 17070\n",
      "  Batch 16,560  of  44,637.    Elapsed: 0:10:52. Training loss. 0.0037093383725732565 Num fake examples 16008 Num true examples 17112\n",
      "  Batch 16,600  of  44,637.    Elapsed: 0:10:54. Training loss. 0.004020987544208765 Num fake examples 16042 Num true examples 17158\n",
      "  Batch 16,640  of  44,637.    Elapsed: 0:10:55. Training loss. 0.005343450233340263 Num fake examples 16075 Num true examples 17205\n",
      "  Batch 16,680  of  44,637.    Elapsed: 0:10:57. Training loss. 0.0040161991491913795 Num fake examples 16118 Num true examples 17242\n",
      "  Batch 16,720  of  44,637.    Elapsed: 0:10:58. Training loss. 0.004417567513883114 Num fake examples 16155 Num true examples 17285\n",
      "  Batch 16,760  of  44,637.    Elapsed: 0:11:00. Training loss. 0.004446567967534065 Num fake examples 16193 Num true examples 17327\n",
      "  Batch 16,800  of  44,637.    Elapsed: 0:11:02. Training loss. 0.005259568803012371 Num fake examples 16237 Num true examples 17363\n",
      "  Batch 16,840  of  44,637.    Elapsed: 0:11:03. Training loss. 0.00352514092810452 Num fake examples 16284 Num true examples 17396\n",
      "  Batch 16,880  of  44,637.    Elapsed: 0:11:05. Training loss. 0.00370807945728302 Num fake examples 16331 Num true examples 17429\n",
      "  Batch 16,920  of  44,637.    Elapsed: 0:11:06. Training loss. 0.0055254134349524975 Num fake examples 16378 Num true examples 17462\n",
      "  Batch 16,960  of  44,637.    Elapsed: 0:11:08. Training loss. 2.5300707817077637 Num fake examples 16419 Num true examples 17501\n",
      "  Batch 17,000  of  44,637.    Elapsed: 0:11:10. Training loss. 0.003965333104133606 Num fake examples 16465 Num true examples 17535\n",
      "  Batch 17,040  of  44,637.    Elapsed: 0:11:11. Training loss. 0.005135626997798681 Num fake examples 16512 Num true examples 17568\n",
      "  Batch 17,080  of  44,637.    Elapsed: 0:11:13. Training loss. 0.0022770266514271498 Num fake examples 16555 Num true examples 17605\n",
      "  Batch 17,120  of  44,637.    Elapsed: 0:11:14. Training loss. 0.002744500059634447 Num fake examples 16604 Num true examples 17636\n",
      "  Batch 17,160  of  44,637.    Elapsed: 0:11:16. Training loss. 0.002394638955593109 Num fake examples 16644 Num true examples 17676\n",
      "  Batch 17,200  of  44,637.    Elapsed: 0:11:17. Training loss. 0.0035480703227221966 Num fake examples 16684 Num true examples 17716\n",
      "  Batch 17,240  of  44,637.    Elapsed: 0:11:19. Training loss. 0.003388407174497843 Num fake examples 16724 Num true examples 17756\n",
      "  Batch 17,280  of  44,637.    Elapsed: 0:11:21. Training loss. 0.0031037398148328066 Num fake examples 16765 Num true examples 17795\n",
      "  Batch 17,320  of  44,637.    Elapsed: 0:11:22. Training loss. 2.9434573650360107 Num fake examples 16805 Num true examples 17835\n",
      "  Batch 17,360  of  44,637.    Elapsed: 0:11:24. Training loss. 2.7486164569854736 Num fake examples 16844 Num true examples 17876\n",
      "  Batch 17,400  of  44,637.    Elapsed: 0:11:25. Training loss. 0.0025502752978354692 Num fake examples 16878 Num true examples 17922\n",
      "  Batch 17,440  of  44,637.    Elapsed: 0:11:27. Training loss. 0.0027725426480174065 Num fake examples 16914 Num true examples 17966\n",
      "  Batch 17,480  of  44,637.    Elapsed: 0:11:28. Training loss. 0.0026801677886396646 Num fake examples 16955 Num true examples 18005\n",
      "  Batch 17,520  of  44,637.    Elapsed: 0:11:30. Training loss. 0.0036612856201827526 Num fake examples 17007 Num true examples 18033\n",
      "  Batch 17,560  of  44,637.    Elapsed: 0:11:32. Training loss. 0.0030701295472681522 Num fake examples 17041 Num true examples 18079\n",
      "  Batch 17,600  of  44,637.    Elapsed: 0:11:33. Training loss. 0.0034809093922376633 Num fake examples 17076 Num true examples 18124\n",
      "  Batch 17,640  of  44,637.    Elapsed: 0:11:35. Training loss. 0.0023774420842528343 Num fake examples 17120 Num true examples 18160\n",
      "  Batch 17,680  of  44,637.    Elapsed: 0:11:36. Training loss. 0.002750049578025937 Num fake examples 17148 Num true examples 18212\n",
      "  Batch 17,720  of  44,637.    Elapsed: 0:11:38. Training loss. 0.0024880676064640284 Num fake examples 17192 Num true examples 18248\n",
      "  Batch 17,760  of  44,637.    Elapsed: 0:11:40. Training loss. 0.0018791911425068974 Num fake examples 17229 Num true examples 18291\n",
      "  Batch 17,800  of  44,637.    Elapsed: 0:11:41. Training loss. 0.002541252877563238 Num fake examples 17268 Num true examples 18332\n",
      "  Batch 17,840  of  44,637.    Elapsed: 0:11:43. Training loss. 0.0017870012670755386 Num fake examples 17310 Num true examples 18370\n",
      "  Batch 17,880  of  44,637.    Elapsed: 0:11:44. Training loss. 0.0023322966881096363 Num fake examples 17359 Num true examples 18401\n",
      "  Batch 17,920  of  44,637.    Elapsed: 0:11:46. Training loss. 0.0018003571312874556 Num fake examples 17399 Num true examples 18441\n",
      "  Batch 17,960  of  44,637.    Elapsed: 0:11:48. Training loss. 0.003956262953579426 Num fake examples 17435 Num true examples 18485\n",
      "  Batch 18,000  of  44,637.    Elapsed: 0:11:49. Training loss. 0.0025153392925858498 Num fake examples 17473 Num true examples 18527\n",
      "  Batch 18,040  of  44,637.    Elapsed: 0:11:51. Training loss. 0.0019018292659893632 Num fake examples 17506 Num true examples 18574\n",
      "  Batch 18,080  of  44,637.    Elapsed: 0:11:52. Training loss. 0.0014318173052743077 Num fake examples 17542 Num true examples 18618\n",
      "  Batch 18,120  of  44,637.    Elapsed: 0:11:54. Training loss. 0.0021769413724541664 Num fake examples 17581 Num true examples 18659\n",
      "  Batch 18,160  of  44,637.    Elapsed: 0:11:55. Training loss. 0.002688358770683408 Num fake examples 17620 Num true examples 18700\n",
      "  Batch 18,200  of  44,637.    Elapsed: 0:11:57. Training loss. 0.0014796890318393707 Num fake examples 17655 Num true examples 18745\n",
      "  Batch 18,240  of  44,637.    Elapsed: 0:11:59. Training loss. 0.002253398299217224 Num fake examples 17699 Num true examples 18781\n",
      "  Batch 18,280  of  44,637.    Elapsed: 0:12:00. Training loss. 0.003410717938095331 Num fake examples 17740 Num true examples 18820\n",
      "  Batch 18,320  of  44,637.    Elapsed: 0:12:02. Training loss. 0.0033073024824261665 Num fake examples 17778 Num true examples 18862\n",
      "  Batch 18,360  of  44,637.    Elapsed: 0:12:03. Training loss. 0.003811418078839779 Num fake examples 17822 Num true examples 18898\n",
      "  Batch 18,400  of  44,637.    Elapsed: 0:12:05. Training loss. 0.002730910200625658 Num fake examples 17865 Num true examples 18935\n",
      "  Batch 18,440  of  44,637.    Elapsed: 0:12:06. Training loss. 0.0037953138817101717 Num fake examples 17902 Num true examples 18978\n",
      "  Batch 18,480  of  44,637.    Elapsed: 0:12:08. Training loss. 0.0027301900554448366 Num fake examples 17946 Num true examples 19014\n",
      "  Batch 18,520  of  44,637.    Elapsed: 0:12:10. Training loss. 0.002760456409305334 Num fake examples 17987 Num true examples 19053\n",
      "  Batch 18,560  of  44,637.    Elapsed: 0:12:11. Training loss. 0.0027535976842045784 Num fake examples 18028 Num true examples 19092\n",
      "  Batch 18,600  of  44,637.    Elapsed: 0:12:13. Training loss. 0.003250334644690156 Num fake examples 18065 Num true examples 19135\n",
      "  Batch 18,640  of  44,637.    Elapsed: 0:12:14. Training loss. 0.002816994208842516 Num fake examples 18099 Num true examples 19181\n",
      "  Batch 18,680  of  44,637.    Elapsed: 0:12:16. Training loss. 0.0031470302492380142 Num fake examples 18133 Num true examples 19227\n",
      "  Batch 18,720  of  44,637.    Elapsed: 0:12:18. Training loss. 0.003742970060557127 Num fake examples 18168 Num true examples 19272\n",
      "  Batch 18,760  of  44,637.    Elapsed: 0:12:19. Training loss. 0.0036780438385903835 Num fake examples 18204 Num true examples 19316\n",
      "  Batch 18,800  of  44,637.    Elapsed: 0:12:21. Training loss. 0.00330091523937881 Num fake examples 18248 Num true examples 19352\n",
      "  Batch 18,840  of  44,637.    Elapsed: 0:12:22. Training loss. 0.003427947172895074 Num fake examples 18289 Num true examples 19391\n",
      "  Batch 18,880  of  44,637.    Elapsed: 0:12:24. Training loss. 0.003681125119328499 Num fake examples 18327 Num true examples 19433\n",
      "  Batch 18,920  of  44,637.    Elapsed: 0:12:25. Training loss. 0.002252284437417984 Num fake examples 18361 Num true examples 19479\n",
      "  Batch 18,960  of  44,637.    Elapsed: 0:12:27. Training loss. 2.725769519805908 Num fake examples 18404 Num true examples 19516\n",
      "  Batch 19,000  of  44,637.    Elapsed: 0:12:29. Training loss. 0.003013086039572954 Num fake examples 18446 Num true examples 19554\n",
      "  Batch 19,040  of  44,637.    Elapsed: 0:12:30. Training loss. 0.0035691678058356047 Num fake examples 18483 Num true examples 19597\n",
      "  Batch 19,080  of  44,637.    Elapsed: 0:12:32. Training loss. 0.0036263116635382175 Num fake examples 18531 Num true examples 19629\n",
      "  Batch 19,120  of  44,637.    Elapsed: 0:12:33. Training loss. 0.003169881645590067 Num fake examples 18573 Num true examples 19667\n",
      "  Batch 19,160  of  44,637.    Elapsed: 0:12:35. Training loss. 0.005808569490909576 Num fake examples 18610 Num true examples 19710\n",
      "  Batch 19,200  of  44,637.    Elapsed: 0:12:37. Training loss. 0.003106095129624009 Num fake examples 18650 Num true examples 19750\n",
      "  Batch 19,240  of  44,637.    Elapsed: 0:12:38. Training loss. 0.002533471677452326 Num fake examples 18692 Num true examples 19788\n",
      "  Batch 19,280  of  44,637.    Elapsed: 0:12:40. Training loss. 0.001662860275246203 Num fake examples 18728 Num true examples 19832\n",
      "  Batch 19,320  of  44,637.    Elapsed: 0:12:41. Training loss. 0.0024207844398915768 Num fake examples 18774 Num true examples 19866\n",
      "  Batch 19,360  of  44,637.    Elapsed: 0:12:43. Training loss. 0.0030281005892902613 Num fake examples 18815 Num true examples 19905\n",
      "  Batch 19,400  of  44,637.    Elapsed: 0:12:44. Training loss. 0.002776237204670906 Num fake examples 18862 Num true examples 19938\n",
      "  Batch 19,440  of  44,637.    Elapsed: 0:12:46. Training loss. 3.1705479621887207 Num fake examples 18898 Num true examples 19982\n",
      "  Batch 19,480  of  44,637.    Elapsed: 0:12:48. Training loss. 0.002152138389647007 Num fake examples 18937 Num true examples 20023\n",
      "  Batch 19,520  of  44,637.    Elapsed: 0:12:49. Training loss. 0.0033077378757297993 Num fake examples 18982 Num true examples 20058\n",
      "  Batch 19,560  of  44,637.    Elapsed: 0:12:51. Training loss. 0.0026150518096983433 Num fake examples 19022 Num true examples 20098\n",
      "  Batch 19,600  of  44,637.    Elapsed: 0:12:52. Training loss. 3.210054397583008 Num fake examples 19060 Num true examples 20140\n",
      "  Batch 19,640  of  44,637.    Elapsed: 0:12:54. Training loss. 0.002987401559948921 Num fake examples 19099 Num true examples 20181\n",
      "  Batch 19,680  of  44,637.    Elapsed: 0:12:56. Training loss. 0.002558572217822075 Num fake examples 19135 Num true examples 20225\n",
      "  Batch 19,720  of  44,637.    Elapsed: 0:12:57. Training loss. 2.916257858276367 Num fake examples 19178 Num true examples 20262\n",
      "  Batch 19,760  of  44,637.    Elapsed: 0:12:59. Training loss. 0.0036944483872503042 Num fake examples 19221 Num true examples 20299\n",
      "  Batch 19,800  of  44,637.    Elapsed: 0:13:00. Training loss. 0.004395089112222195 Num fake examples 19270 Num true examples 20330\n",
      "  Batch 19,840  of  44,637.    Elapsed: 0:13:02. Training loss. 0.00480571249499917 Num fake examples 19310 Num true examples 20370\n",
      "  Batch 19,880  of  44,637.    Elapsed: 0:13:03. Training loss. 0.003406031522899866 Num fake examples 19348 Num true examples 20412\n",
      "  Batch 19,920  of  44,637.    Elapsed: 0:13:05. Training loss. 0.0036543633323162794 Num fake examples 19388 Num true examples 20452\n",
      "  Batch 19,960  of  44,637.    Elapsed: 0:13:07. Training loss. 0.004555939696729183 Num fake examples 19427 Num true examples 20493\n",
      "  Batch 20,000  of  44,637.    Elapsed: 0:13:08. Training loss. 0.003805851563811302 Num fake examples 19467 Num true examples 20533\n",
      "  Batch 20,040  of  44,637.    Elapsed: 0:13:10. Training loss. 2.862107276916504 Num fake examples 19512 Num true examples 20568\n",
      "  Batch 20,080  of  44,637.    Elapsed: 0:13:11. Training loss. 2.691262722015381 Num fake examples 19554 Num true examples 20606\n",
      "  Batch 20,120  of  44,637.    Elapsed: 0:13:13. Training loss. 0.003889434039592743 Num fake examples 19598 Num true examples 20642\n",
      "  Batch 20,160  of  44,637.    Elapsed: 0:13:14. Training loss. 0.0036705080419778824 Num fake examples 19640 Num true examples 20680\n",
      "  Batch 20,200  of  44,637.    Elapsed: 0:13:16. Training loss. 0.003002137877047062 Num fake examples 19677 Num true examples 20723\n",
      "  Batch 20,240  of  44,637.    Elapsed: 0:13:18. Training loss. 0.002807845361530781 Num fake examples 19717 Num true examples 20763\n",
      "  Batch 20,280  of  44,637.    Elapsed: 0:13:19. Training loss. 0.003228881862014532 Num fake examples 19760 Num true examples 20800\n",
      "  Batch 20,320  of  44,637.    Elapsed: 0:13:21. Training loss. 0.002500941278412938 Num fake examples 19803 Num true examples 20837\n",
      "  Batch 20,360  of  44,637.    Elapsed: 0:13:22. Training loss. 0.002434113994240761 Num fake examples 19828 Num true examples 20892\n",
      "  Batch 20,400  of  44,637.    Elapsed: 0:13:24. Training loss. 0.002758646383881569 Num fake examples 19870 Num true examples 20930\n",
      "  Batch 20,440  of  44,637.    Elapsed: 0:13:26. Training loss. 0.0050896089524030685 Num fake examples 19911 Num true examples 20969\n",
      "  Batch 20,480  of  44,637.    Elapsed: 0:13:27. Training loss. 2.6443395614624023 Num fake examples 19944 Num true examples 21016\n",
      "  Batch 20,520  of  44,637.    Elapsed: 0:13:29. Training loss. 0.003380583133548498 Num fake examples 19988 Num true examples 21052\n",
      "  Batch 20,560  of  44,637.    Elapsed: 0:13:30. Training loss. 0.004454770591109991 Num fake examples 20024 Num true examples 21096\n",
      "  Batch 20,600  of  44,637.    Elapsed: 0:13:32. Training loss. 0.0029904290568083525 Num fake examples 20064 Num true examples 21136\n",
      "  Batch 20,640  of  44,637.    Elapsed: 0:13:33. Training loss. 0.003374933497980237 Num fake examples 20106 Num true examples 21174\n",
      "  Batch 20,680  of  44,637.    Elapsed: 0:13:35. Training loss. 0.0034490986727178097 Num fake examples 20143 Num true examples 21217\n",
      "  Batch 20,720  of  44,637.    Elapsed: 0:13:37. Training loss. 0.004505661316215992 Num fake examples 20186 Num true examples 21254\n",
      "  Batch 20,760  of  44,637.    Elapsed: 0:13:38. Training loss. 0.004055436234921217 Num fake examples 20221 Num true examples 21299\n",
      "  Batch 20,800  of  44,637.    Elapsed: 0:13:40. Training loss. 0.003497024066746235 Num fake examples 20258 Num true examples 21342\n",
      "  Batch 20,840  of  44,637.    Elapsed: 0:13:41. Training loss. 2.6438510417938232 Num fake examples 20291 Num true examples 21389\n",
      "  Batch 20,880  of  44,637.    Elapsed: 0:13:43. Training loss. 0.0037266560830175877 Num fake examples 20331 Num true examples 21429\n",
      "  Batch 20,920  of  44,637.    Elapsed: 0:13:45. Training loss. 0.002956344746053219 Num fake examples 20370 Num true examples 21470\n",
      "  Batch 20,960  of  44,637.    Elapsed: 0:13:46. Training loss. 0.004134842194616795 Num fake examples 20416 Num true examples 21504\n",
      "  Batch 21,000  of  44,637.    Elapsed: 0:13:48. Training loss. 0.0027451221831142902 Num fake examples 20461 Num true examples 21539\n",
      "  Batch 21,040  of  44,637.    Elapsed: 0:13:49. Training loss. 0.0037941280752420425 Num fake examples 20497 Num true examples 21583\n",
      "  Batch 21,080  of  44,637.    Elapsed: 0:13:51. Training loss. 0.002927811350673437 Num fake examples 20539 Num true examples 21621\n",
      "  Batch 21,120  of  44,637.    Elapsed: 0:13:52. Training loss. 2.745803117752075 Num fake examples 20583 Num true examples 21657\n",
      "  Batch 21,160  of  44,637.    Elapsed: 0:13:54. Training loss. 0.005570698995143175 Num fake examples 20618 Num true examples 21702\n",
      "  Batch 21,200  of  44,637.    Elapsed: 0:13:56. Training loss. 0.0014806580729782581 Num fake examples 20654 Num true examples 21746\n",
      "  Batch 21,240  of  44,637.    Elapsed: 0:13:57. Training loss. 0.002093807328492403 Num fake examples 20692 Num true examples 21788\n",
      "  Batch 21,280  of  44,637.    Elapsed: 0:13:59. Training loss. 0.004574050195515156 Num fake examples 20737 Num true examples 21823\n",
      "  Batch 21,320  of  44,637.    Elapsed: 0:14:00. Training loss. 2.6701037883758545 Num fake examples 20772 Num true examples 21868\n",
      "  Batch 21,360  of  44,637.    Elapsed: 0:14:02. Training loss. 0.007218661718070507 Num fake examples 20809 Num true examples 21911\n",
      "  Batch 21,400  of  44,637.    Elapsed: 0:14:03. Training loss. 0.0035529453307390213 Num fake examples 20852 Num true examples 21948\n",
      "  Batch 21,440  of  44,637.    Elapsed: 0:14:05. Training loss. 0.0032434440217912197 Num fake examples 20891 Num true examples 21989\n",
      "  Batch 21,480  of  44,637.    Elapsed: 0:14:07. Training loss. 0.004336862359195948 Num fake examples 20923 Num true examples 22037\n",
      "  Batch 21,520  of  44,637.    Elapsed: 0:14:08. Training loss. 0.003222288331016898 Num fake examples 20956 Num true examples 22084\n",
      "  Batch 21,560  of  44,637.    Elapsed: 0:14:10. Training loss. 0.0044561391696333885 Num fake examples 21001 Num true examples 22119\n",
      "  Batch 21,600  of  44,637.    Elapsed: 0:14:11. Training loss. 0.003749141003936529 Num fake examples 21040 Num true examples 22160\n",
      "  Batch 21,640  of  44,637.    Elapsed: 0:14:13. Training loss. 0.004648271948099136 Num fake examples 21081 Num true examples 22199\n",
      "  Batch 21,680  of  44,637.    Elapsed: 0:14:15. Training loss. 0.0042820586822927 Num fake examples 21118 Num true examples 22242\n",
      "  Batch 21,720  of  44,637.    Elapsed: 0:14:16. Training loss. 0.0033466422464698553 Num fake examples 21156 Num true examples 22284\n",
      "  Batch 21,760  of  44,637.    Elapsed: 0:14:18. Training loss. 0.0033395164646208286 Num fake examples 21198 Num true examples 22322\n",
      "  Batch 21,800  of  44,637.    Elapsed: 0:14:19. Training loss. 0.0028058290481567383 Num fake examples 21239 Num true examples 22361\n",
      "  Batch 21,840  of  44,637.    Elapsed: 0:14:21. Training loss. 0.0032639182172715664 Num fake examples 21270 Num true examples 22410\n",
      "  Batch 21,880  of  44,637.    Elapsed: 0:14:22. Training loss. 0.0032876310870051384 Num fake examples 21303 Num true examples 22457\n",
      "  Batch 21,920  of  44,637.    Elapsed: 0:14:24. Training loss. 0.0034910503309220076 Num fake examples 21344 Num true examples 22496\n",
      "  Batch 21,960  of  44,637.    Elapsed: 0:14:26. Training loss. 0.003054875647649169 Num fake examples 21384 Num true examples 22536\n",
      "  Batch 22,000  of  44,637.    Elapsed: 0:14:27. Training loss. 0.005269927904009819 Num fake examples 21430 Num true examples 22570\n",
      "  Batch 22,040  of  44,637.    Elapsed: 0:14:29. Training loss. 0.004061347804963589 Num fake examples 21468 Num true examples 22612\n",
      "  Batch 22,080  of  44,637.    Elapsed: 0:14:30. Training loss. 0.003164079040288925 Num fake examples 21509 Num true examples 22651\n",
      "  Batch 22,120  of  44,637.    Elapsed: 0:14:32. Training loss. 0.002797850640490651 Num fake examples 21550 Num true examples 22690\n",
      "  Batch 22,160  of  44,637.    Elapsed: 0:14:34. Training loss. 0.003717455081641674 Num fake examples 21583 Num true examples 22737\n",
      "  Batch 22,200  of  44,637.    Elapsed: 0:14:35. Training loss. 0.0046773916110396385 Num fake examples 21622 Num true examples 22778\n",
      "  Batch 22,240  of  44,637.    Elapsed: 0:14:37. Training loss. 0.002401187550276518 Num fake examples 21666 Num true examples 22814\n",
      "  Batch 22,280  of  44,637.    Elapsed: 0:14:38. Training loss. 0.0023040524683892727 Num fake examples 21713 Num true examples 22847\n",
      "  Batch 22,320  of  44,637.    Elapsed: 0:14:40. Training loss. 0.00239593256264925 Num fake examples 21750 Num true examples 22890\n",
      "  Batch 22,360  of  44,637.    Elapsed: 0:14:41. Training loss. 0.0022817524150013924 Num fake examples 21798 Num true examples 22922\n",
      "  Batch 22,400  of  44,637.    Elapsed: 0:14:43. Training loss. 0.0014878244837746024 Num fake examples 21835 Num true examples 22965\n",
      "  Batch 22,440  of  44,637.    Elapsed: 0:14:45. Training loss. 0.002083233557641506 Num fake examples 21866 Num true examples 23014\n",
      "  Batch 22,480  of  44,637.    Elapsed: 0:14:46. Training loss. 0.002577158622443676 Num fake examples 21901 Num true examples 23059\n",
      "  Batch 22,520  of  44,637.    Elapsed: 0:14:48. Training loss. 0.0044636232778429985 Num fake examples 21937 Num true examples 23103\n",
      "  Batch 22,560  of  44,637.    Elapsed: 0:14:49. Training loss. 0.0027028261683881283 Num fake examples 21979 Num true examples 23141\n",
      "  Batch 22,600  of  44,637.    Elapsed: 0:14:51. Training loss. 0.0034202151000499725 Num fake examples 22026 Num true examples 23174\n",
      "  Batch 22,640  of  44,637.    Elapsed: 0:14:53. Training loss. 0.0026072480250149965 Num fake examples 22068 Num true examples 23212\n",
      "  Batch 22,680  of  44,637.    Elapsed: 0:14:54. Training loss. 0.004066933877766132 Num fake examples 22109 Num true examples 23251\n",
      "  Batch 22,720  of  44,637.    Elapsed: 0:14:56. Training loss. 0.003942434675991535 Num fake examples 22148 Num true examples 23292\n",
      "  Batch 22,760  of  44,637.    Elapsed: 0:14:57. Training loss. 0.0034851543605327606 Num fake examples 22187 Num true examples 23333\n",
      "  Batch 22,800  of  44,637.    Elapsed: 0:14:59. Training loss. 0.004085558466613293 Num fake examples 22223 Num true examples 23377\n",
      "  Batch 22,840  of  44,637.    Elapsed: 0:15:01. Training loss. 0.004044025205075741 Num fake examples 22264 Num true examples 23416\n",
      "  Batch 22,880  of  44,637.    Elapsed: 0:15:02. Training loss. 0.0058889249339699745 Num fake examples 22304 Num true examples 23456\n",
      "  Batch 22,920  of  44,637.    Elapsed: 0:15:04. Training loss. 0.004283539485186338 Num fake examples 22347 Num true examples 23493\n",
      "  Batch 22,960  of  44,637.    Elapsed: 0:15:05. Training loss. 0.0034816726110875607 Num fake examples 22386 Num true examples 23534\n",
      "  Batch 23,000  of  44,637.    Elapsed: 0:15:07. Training loss. 0.004483948461711407 Num fake examples 22429 Num true examples 23571\n",
      "  Batch 23,040  of  44,637.    Elapsed: 0:15:08. Training loss. 0.0033958880230784416 Num fake examples 22466 Num true examples 23614\n",
      "  Batch 23,080  of  44,637.    Elapsed: 0:15:10. Training loss. 0.00385739142075181 Num fake examples 22509 Num true examples 23651\n",
      "  Batch 23,120  of  44,637.    Elapsed: 0:15:12. Training loss. 0.004019353538751602 Num fake examples 22554 Num true examples 23686\n",
      "  Batch 23,160  of  44,637.    Elapsed: 0:15:13. Training loss. 0.003979478031396866 Num fake examples 22585 Num true examples 23735\n",
      "  Batch 23,200  of  44,637.    Elapsed: 0:15:15. Training loss. 0.004613317083567381 Num fake examples 22622 Num true examples 23778\n",
      "  Batch 23,240  of  44,637.    Elapsed: 0:15:16. Training loss. 0.003055217210203409 Num fake examples 22661 Num true examples 23819\n",
      "  Batch 23,280  of  44,637.    Elapsed: 0:15:18. Training loss. 0.0022951511200517416 Num fake examples 22695 Num true examples 23865\n",
      "  Batch 23,320  of  44,637.    Elapsed: 0:15:20. Training loss. 0.004416004754602909 Num fake examples 22740 Num true examples 23900\n",
      "  Batch 23,360  of  44,637.    Elapsed: 0:15:21. Training loss. 0.004289582371711731 Num fake examples 22779 Num true examples 23941\n",
      "  Batch 23,400  of  44,637.    Elapsed: 0:15:23. Training loss. 0.004418553784489632 Num fake examples 22825 Num true examples 23975\n",
      "  Batch 23,440  of  44,637.    Elapsed: 0:15:24. Training loss. 2.7737152576446533 Num fake examples 22868 Num true examples 24012\n",
      "  Batch 23,480  of  44,637.    Elapsed: 0:15:26. Training loss. 0.004692581947892904 Num fake examples 22907 Num true examples 24053\n",
      "  Batch 23,520  of  44,637.    Elapsed: 0:15:27. Training loss. 0.003530562622472644 Num fake examples 22944 Num true examples 24096\n",
      "  Batch 23,560  of  44,637.    Elapsed: 0:15:29. Training loss. 0.0022071802522987127 Num fake examples 22977 Num true examples 24143\n",
      "  Batch 23,600  of  44,637.    Elapsed: 0:15:31. Training loss. 0.003065916243940592 Num fake examples 23018 Num true examples 24182\n",
      "  Batch 23,640  of  44,637.    Elapsed: 0:15:32. Training loss. 0.003067838028073311 Num fake examples 23059 Num true examples 24221\n",
      "  Batch 23,680  of  44,637.    Elapsed: 0:15:34. Training loss. 0.0038635788951069117 Num fake examples 23101 Num true examples 24259\n",
      "  Batch 23,720  of  44,637.    Elapsed: 0:15:35. Training loss. 0.003415483981370926 Num fake examples 23141 Num true examples 24299\n",
      "  Batch 23,760  of  44,637.    Elapsed: 0:15:37. Training loss. 0.003618981223553419 Num fake examples 23178 Num true examples 24342\n",
      "  Batch 23,800  of  44,637.    Elapsed: 0:15:39. Training loss. 0.002909940667450428 Num fake examples 23220 Num true examples 24380\n",
      "  Batch 23,840  of  44,637.    Elapsed: 0:15:40. Training loss. 0.002444562502205372 Num fake examples 23261 Num true examples 24419\n",
      "  Batch 23,880  of  44,637.    Elapsed: 0:15:42. Training loss. 0.0030369339510798454 Num fake examples 23298 Num true examples 24462\n",
      "  Batch 23,920  of  44,637.    Elapsed: 0:15:43. Training loss. 0.002349668648093939 Num fake examples 23343 Num true examples 24497\n",
      "  Batch 23,960  of  44,637.    Elapsed: 0:15:45. Training loss. 2.766784429550171 Num fake examples 23382 Num true examples 24538\n",
      "  Batch 24,000  of  44,637.    Elapsed: 0:15:46. Training loss. 0.004148500971496105 Num fake examples 23425 Num true examples 24575\n",
      "  Batch 24,040  of  44,637.    Elapsed: 0:15:48. Training loss. 0.003453571116551757 Num fake examples 23469 Num true examples 24611\n",
      "  Batch 24,080  of  44,637.    Elapsed: 0:15:50. Training loss. 0.0048298463225364685 Num fake examples 23505 Num true examples 24655\n",
      "  Batch 24,120  of  44,637.    Elapsed: 0:15:51. Training loss. 0.003503215964883566 Num fake examples 23543 Num true examples 24697\n",
      "  Batch 24,160  of  44,637.    Elapsed: 0:15:53. Training loss. 0.003662683768197894 Num fake examples 23577 Num true examples 24743\n",
      "  Batch 24,200  of  44,637.    Elapsed: 0:15:55. Training loss. 2.8683149814605713 Num fake examples 23619 Num true examples 24781\n",
      "  Batch 24,240  of  44,637.    Elapsed: 0:15:56. Training loss. 0.002569153206422925 Num fake examples 23665 Num true examples 24815\n",
      "  Batch 24,280  of  44,637.    Elapsed: 0:15:58. Training loss. 0.0030852099880576134 Num fake examples 23707 Num true examples 24853\n",
      "  Batch 24,320  of  44,637.    Elapsed: 0:15:59. Training loss. 0.0032447343692183495 Num fake examples 23744 Num true examples 24896\n",
      "  Batch 24,360  of  44,637.    Elapsed: 0:16:01. Training loss. 0.0032733373809605837 Num fake examples 23782 Num true examples 24938\n",
      "  Batch 24,400  of  44,637.    Elapsed: 0:16:03. Training loss. 0.003007348161190748 Num fake examples 23825 Num true examples 24975\n",
      "  Batch 24,440  of  44,637.    Elapsed: 0:16:04. Training loss. 0.0022326207254081964 Num fake examples 23863 Num true examples 25017\n",
      "  Batch 24,480  of  44,637.    Elapsed: 0:16:06. Training loss. 0.003348736325278878 Num fake examples 23906 Num true examples 25054\n",
      "  Batch 24,520  of  44,637.    Elapsed: 0:16:07. Training loss. 0.001326642232015729 Num fake examples 23951 Num true examples 25089\n",
      "  Batch 24,560  of  44,637.    Elapsed: 0:16:09. Training loss. 0.002161349868401885 Num fake examples 23997 Num true examples 25123\n",
      "  Batch 24,600  of  44,637.    Elapsed: 0:16:11. Training loss. 0.0030856155790388584 Num fake examples 24039 Num true examples 25161\n",
      "  Batch 24,640  of  44,637.    Elapsed: 0:16:12. Training loss. 0.001927474164403975 Num fake examples 24088 Num true examples 25192\n",
      "  Batch 24,680  of  44,637.    Elapsed: 0:16:14. Training loss. 0.002547509502619505 Num fake examples 24122 Num true examples 25238\n",
      "  Batch 24,720  of  44,637.    Elapsed: 0:16:15. Training loss. 0.002452058484777808 Num fake examples 24158 Num true examples 25282\n",
      "  Batch 24,760  of  44,637.    Elapsed: 0:16:17. Training loss. 0.0023131694179028273 Num fake examples 24190 Num true examples 25330\n",
      "  Batch 24,800  of  44,637.    Elapsed: 0:16:19. Training loss. 0.002962779952213168 Num fake examples 24230 Num true examples 25370\n",
      "  Batch 24,840  of  44,637.    Elapsed: 0:16:20. Training loss. 0.003007023362442851 Num fake examples 24267 Num true examples 25413\n",
      "  Batch 24,880  of  44,637.    Elapsed: 0:16:22. Training loss. 0.0031131901778280735 Num fake examples 24303 Num true examples 25457\n",
      "  Batch 24,920  of  44,637.    Elapsed: 0:16:23. Training loss. 0.003911280073225498 Num fake examples 24344 Num true examples 25496\n",
      "  Batch 24,960  of  44,637.    Elapsed: 0:16:25. Training loss. 0.0058511910028755665 Num fake examples 24376 Num true examples 25544\n",
      "  Batch 25,000  of  44,637.    Elapsed: 0:16:27. Training loss. 0.0025926148518919945 Num fake examples 24412 Num true examples 25588\n",
      "  Batch 25,040  of  44,637.    Elapsed: 0:16:28. Training loss. 0.005769846960902214 Num fake examples 24451 Num true examples 25629\n",
      "  Batch 25,080  of  44,637.    Elapsed: 0:16:30. Training loss. 0.004634914454072714 Num fake examples 24488 Num true examples 25672\n",
      "  Batch 25,120  of  44,637.    Elapsed: 0:16:31. Training loss. 0.005460858345031738 Num fake examples 24526 Num true examples 25714\n",
      "  Batch 25,160  of  44,637.    Elapsed: 0:16:33. Training loss. 0.0033138724975287914 Num fake examples 24564 Num true examples 25756\n",
      "  Batch 25,200  of  44,637.    Elapsed: 0:16:34. Training loss. 0.002343837171792984 Num fake examples 24602 Num true examples 25798\n",
      "  Batch 25,240  of  44,637.    Elapsed: 0:16:36. Training loss. 0.0034804530441761017 Num fake examples 24646 Num true examples 25834\n",
      "  Batch 25,280  of  44,637.    Elapsed: 0:16:38. Training loss. 0.002770290244370699 Num fake examples 24678 Num true examples 25882\n",
      "  Batch 25,320  of  44,637.    Elapsed: 0:16:39. Training loss. 0.004073865711688995 Num fake examples 24721 Num true examples 25919\n",
      "  Batch 25,360  of  44,637.    Elapsed: 0:16:41. Training loss. 0.003719738218933344 Num fake examples 24762 Num true examples 25958\n",
      "  Batch 25,400  of  44,637.    Elapsed: 0:16:42. Training loss. 0.002595968544483185 Num fake examples 24802 Num true examples 25998\n",
      "  Batch 25,440  of  44,637.    Elapsed: 0:16:44. Training loss. 0.004368373658508062 Num fake examples 24845 Num true examples 26035\n",
      "  Batch 25,480  of  44,637.    Elapsed: 0:16:46. Training loss. 0.0029550069011747837 Num fake examples 24882 Num true examples 26078\n",
      "  Batch 25,520  of  44,637.    Elapsed: 0:16:47. Training loss. 0.00332092447206378 Num fake examples 24920 Num true examples 26120\n",
      "  Batch 25,560  of  44,637.    Elapsed: 0:16:49. Training loss. 0.003227395936846733 Num fake examples 24952 Num true examples 26168\n",
      "  Batch 25,600  of  44,637.    Elapsed: 0:16:50. Training loss. 0.0032085191924124956 Num fake examples 24988 Num true examples 26212\n",
      "  Batch 25,640  of  44,637.    Elapsed: 0:16:52. Training loss. 0.0041937995702028275 Num fake examples 25025 Num true examples 26255\n",
      "  Batch 25,680  of  44,637.    Elapsed: 0:16:53. Training loss. 0.002552343998104334 Num fake examples 25067 Num true examples 26293\n",
      "  Batch 25,720  of  44,637.    Elapsed: 0:16:55. Training loss. 0.0029559675604104996 Num fake examples 25113 Num true examples 26327\n",
      "  Batch 25,760  of  44,637.    Elapsed: 0:16:57. Training loss. 0.002901372965425253 Num fake examples 25146 Num true examples 26374\n",
      "  Batch 25,800  of  44,637.    Elapsed: 0:16:58. Training loss. 0.0034495587460696697 Num fake examples 25187 Num true examples 26413\n",
      "  Batch 25,840  of  44,637.    Elapsed: 0:17:00. Training loss. 0.003812622046098113 Num fake examples 25227 Num true examples 26453\n",
      "  Batch 25,880  of  44,637.    Elapsed: 0:17:01. Training loss. 0.004162997473031282 Num fake examples 25268 Num true examples 26492\n",
      "  Batch 25,920  of  44,637.    Elapsed: 0:17:03. Training loss. 0.0036820003297179937 Num fake examples 25303 Num true examples 26537\n",
      "  Batch 25,960  of  44,637.    Elapsed: 0:17:05. Training loss. 0.0030122403986752033 Num fake examples 25346 Num true examples 26574\n",
      "  Batch 26,000  of  44,637.    Elapsed: 0:17:06. Training loss. 0.003553056623786688 Num fake examples 25392 Num true examples 26608\n",
      "  Batch 26,040  of  44,637.    Elapsed: 0:17:08. Training loss. 0.005293216090649366 Num fake examples 25432 Num true examples 26648\n",
      "  Batch 26,080  of  44,637.    Elapsed: 0:17:09. Training loss. 0.005129908211529255 Num fake examples 25477 Num true examples 26683\n",
      "  Batch 26,120  of  44,637.    Elapsed: 0:17:11. Training loss. 0.0034288919996470213 Num fake examples 25510 Num true examples 26730\n",
      "  Batch 26,160  of  44,637.    Elapsed: 0:17:13. Training loss. 0.002603673841804266 Num fake examples 25547 Num true examples 26773\n",
      "  Batch 26,200  of  44,637.    Elapsed: 0:17:14. Training loss. 0.0028658253140747547 Num fake examples 25592 Num true examples 26808\n",
      "  Batch 26,240  of  44,637.    Elapsed: 0:17:16. Training loss. 0.004135526716709137 Num fake examples 25641 Num true examples 26839\n",
      "  Batch 26,280  of  44,637.    Elapsed: 0:17:17. Training loss. 0.00414256239309907 Num fake examples 25680 Num true examples 26880\n",
      "  Batch 26,320  of  44,637.    Elapsed: 0:17:19. Training loss. 0.0032589042093604803 Num fake examples 25708 Num true examples 26932\n",
      "  Batch 26,360  of  44,637.    Elapsed: 0:17:20. Training loss. 0.003982228692620993 Num fake examples 25742 Num true examples 26978\n",
      "  Batch 26,400  of  44,637.    Elapsed: 0:17:22. Training loss. 0.003504660911858082 Num fake examples 25779 Num true examples 27021\n",
      "  Batch 26,440  of  44,637.    Elapsed: 0:17:24. Training loss. 0.0036967620253562927 Num fake examples 25819 Num true examples 27061\n",
      "  Batch 26,480  of  44,637.    Elapsed: 0:17:25. Training loss. 0.005369111895561218 Num fake examples 25855 Num true examples 27105\n",
      "  Batch 26,520  of  44,637.    Elapsed: 0:17:27. Training loss. 3.0529801845550537 Num fake examples 25890 Num true examples 27150\n",
      "  Batch 26,560  of  44,637.    Elapsed: 0:17:28. Training loss. 3.2279412746429443 Num fake examples 25926 Num true examples 27194\n",
      "  Batch 26,600  of  44,637.    Elapsed: 0:17:30. Training loss. 0.003269987879320979 Num fake examples 25967 Num true examples 27233\n",
      "  Batch 26,640  of  44,637.    Elapsed: 0:17:32. Training loss. 0.003967571072280407 Num fake examples 26008 Num true examples 27272\n",
      "  Batch 26,680  of  44,637.    Elapsed: 0:17:33. Training loss. 0.004439092706888914 Num fake examples 26054 Num true examples 27306\n",
      "  Batch 26,720  of  44,637.    Elapsed: 0:17:35. Training loss. 0.004153568763285875 Num fake examples 26096 Num true examples 27344\n",
      "  Batch 26,760  of  44,637.    Elapsed: 0:17:36. Training loss. 0.0032368607353419065 Num fake examples 26139 Num true examples 27381\n",
      "  Batch 26,800  of  44,637.    Elapsed: 0:17:38. Training loss. 0.002827528864145279 Num fake examples 26180 Num true examples 27420\n",
      "  Batch 26,840  of  44,637.    Elapsed: 0:17:40. Training loss. 2.5183022022247314 Num fake examples 26225 Num true examples 27455\n",
      "  Batch 26,880  of  44,637.    Elapsed: 0:17:41. Training loss. 0.003092521568760276 Num fake examples 26266 Num true examples 27494\n",
      "  Batch 26,920  of  44,637.    Elapsed: 0:17:43. Training loss. 0.003693793900310993 Num fake examples 26300 Num true examples 27540\n",
      "  Batch 26,960  of  44,637.    Elapsed: 0:17:44. Training loss. 0.004757629707455635 Num fake examples 26338 Num true examples 27582\n",
      "  Batch 27,000  of  44,637.    Elapsed: 0:17:46. Training loss. 0.004864872433245182 Num fake examples 26378 Num true examples 27622\n",
      "  Batch 27,040  of  44,637.    Elapsed: 0:17:47. Training loss. 0.0030217214953154325 Num fake examples 26419 Num true examples 27661\n",
      "  Batch 27,080  of  44,637.    Elapsed: 0:17:49. Training loss. 0.0038632163777947426 Num fake examples 26450 Num true examples 27710\n",
      "  Batch 27,120  of  44,637.    Elapsed: 0:17:51. Training loss. 0.0032573610078543425 Num fake examples 26490 Num true examples 27750\n",
      "  Batch 27,160  of  44,637.    Elapsed: 0:17:52. Training loss. 0.0023570077028125525 Num fake examples 26523 Num true examples 27797\n",
      "  Batch 27,200  of  44,637.    Elapsed: 0:17:54. Training loss. 0.003417651169002056 Num fake examples 26560 Num true examples 27840\n",
      "  Batch 27,240  of  44,637.    Elapsed: 0:17:55. Training loss. 2.6366162300109863 Num fake examples 26604 Num true examples 27876\n",
      "  Batch 27,280  of  44,637.    Elapsed: 0:17:57. Training loss. 0.005301474593579769 Num fake examples 26641 Num true examples 27919\n",
      "  Batch 27,320  of  44,637.    Elapsed: 0:17:59. Training loss. 0.003948543220758438 Num fake examples 26681 Num true examples 27959\n",
      "  Batch 27,360  of  44,637.    Elapsed: 0:18:00. Training loss. 0.002496981993317604 Num fake examples 26717 Num true examples 28003\n",
      "  Batch 27,400  of  44,637.    Elapsed: 0:18:02. Training loss. 0.002855961676687002 Num fake examples 26752 Num true examples 28048\n",
      "  Batch 27,440  of  44,637.    Elapsed: 0:18:03. Training loss. 0.0031030892860144377 Num fake examples 26789 Num true examples 28091\n",
      "  Batch 27,480  of  44,637.    Elapsed: 0:18:05. Training loss. 0.0043913619592785835 Num fake examples 26832 Num true examples 28128\n",
      "  Batch 27,520  of  44,637.    Elapsed: 0:18:06. Training loss. 0.004505541175603867 Num fake examples 26879 Num true examples 28161\n",
      "  Batch 27,560  of  44,637.    Elapsed: 0:18:08. Training loss. 0.003695493098348379 Num fake examples 26922 Num true examples 28198\n",
      "  Batch 27,600  of  44,637.    Elapsed: 0:18:10. Training loss. 0.004467859864234924 Num fake examples 26964 Num true examples 28236\n",
      "  Batch 27,640  of  44,637.    Elapsed: 0:18:11. Training loss. 0.004516017623245716 Num fake examples 27001 Num true examples 28279\n",
      "  Batch 27,680  of  44,637.    Elapsed: 0:18:13. Training loss. 0.0027506810147315264 Num fake examples 27037 Num true examples 28323\n",
      "  Batch 27,720  of  44,637.    Elapsed: 0:18:14. Training loss. 2.775560140609741 Num fake examples 27071 Num true examples 28369\n",
      "  Batch 27,760  of  44,637.    Elapsed: 0:18:16. Training loss. 0.0031052702106535435 Num fake examples 27114 Num true examples 28406\n",
      "  Batch 27,800  of  44,637.    Elapsed: 0:18:18. Training loss. 0.003039028961211443 Num fake examples 27144 Num true examples 28456\n",
      "  Batch 27,840  of  44,637.    Elapsed: 0:18:19. Training loss. 0.002634623786434531 Num fake examples 27183 Num true examples 28497\n",
      "  Batch 27,880  of  44,637.    Elapsed: 0:18:21. Training loss. 2.989388942718506 Num fake examples 27220 Num true examples 28540\n",
      "  Batch 27,920  of  44,637.    Elapsed: 0:18:22. Training loss. 0.0031435685232281685 Num fake examples 27259 Num true examples 28581\n",
      "  Batch 27,960  of  44,637.    Elapsed: 0:18:24. Training loss. 0.002319379011169076 Num fake examples 27294 Num true examples 28626\n",
      "  Batch 28,000  of  44,637.    Elapsed: 0:18:25. Training loss. 2.809084415435791 Num fake examples 27336 Num true examples 28664\n",
      "  Batch 28,040  of  44,637.    Elapsed: 0:18:27. Training loss. 2.6754262447357178 Num fake examples 27373 Num true examples 28707\n",
      "  Batch 28,080  of  44,637.    Elapsed: 0:18:29. Training loss. 0.002430389868095517 Num fake examples 27412 Num true examples 28748\n",
      "  Batch 28,120  of  44,637.    Elapsed: 0:18:30. Training loss. 0.0024925596080720425 Num fake examples 27455 Num true examples 28785\n",
      "  Batch 28,160  of  44,637.    Elapsed: 0:18:32. Training loss. 0.0019465623190626502 Num fake examples 27489 Num true examples 28831\n",
      "  Batch 28,200  of  44,637.    Elapsed: 0:18:33. Training loss. 0.004015772137790918 Num fake examples 27530 Num true examples 28870\n",
      "  Batch 28,240  of  44,637.    Elapsed: 0:18:35. Training loss. 0.0027412506751716137 Num fake examples 27570 Num true examples 28910\n",
      "  Batch 28,280  of  44,637.    Elapsed: 0:18:37. Training loss. 0.0024138418957591057 Num fake examples 27604 Num true examples 28956\n",
      "  Batch 28,320  of  44,637.    Elapsed: 0:18:38. Training loss. 0.004322797060012817 Num fake examples 27642 Num true examples 28998\n",
      "  Batch 28,360  of  44,637.    Elapsed: 0:18:40. Training loss. 0.004671339876949787 Num fake examples 27678 Num true examples 29042\n",
      "  Batch 28,400  of  44,637.    Elapsed: 0:18:41. Training loss. 0.0036017093807458878 Num fake examples 27708 Num true examples 29092\n",
      "  Batch 28,440  of  44,637.    Elapsed: 0:18:43. Training loss. 0.004449575208127499 Num fake examples 27745 Num true examples 29135\n",
      "  Batch 28,480  of  44,637.    Elapsed: 0:18:45. Training loss. 0.0018010410713031888 Num fake examples 27778 Num true examples 29182\n",
      "  Batch 28,520  of  44,637.    Elapsed: 0:18:46. Training loss. 0.0036381264217197895 Num fake examples 27810 Num true examples 29230\n",
      "  Batch 28,560  of  44,637.    Elapsed: 0:18:48. Training loss. 2.7749602794647217 Num fake examples 27843 Num true examples 29277\n",
      "  Batch 28,600  of  44,637.    Elapsed: 0:18:49. Training loss. 0.00273251929320395 Num fake examples 27883 Num true examples 29317\n",
      "  Batch 28,640  of  44,637.    Elapsed: 0:18:51. Training loss. 0.005198082886636257 Num fake examples 27925 Num true examples 29355\n",
      "  Batch 28,680  of  44,637.    Elapsed: 0:18:52. Training loss. 0.002391515765339136 Num fake examples 27955 Num true examples 29405\n",
      "  Batch 28,720  of  44,637.    Elapsed: 0:18:54. Training loss. 0.0024035966489464045 Num fake examples 27993 Num true examples 29447\n",
      "  Batch 28,760  of  44,637.    Elapsed: 0:18:56. Training loss. 0.002627365756779909 Num fake examples 28037 Num true examples 29483\n",
      "  Batch 28,800  of  44,637.    Elapsed: 0:18:57. Training loss. 0.0024122665636241436 Num fake examples 28078 Num true examples 29522\n",
      "  Batch 28,840  of  44,637.    Elapsed: 0:18:59. Training loss. 0.0028685389552265406 Num fake examples 28125 Num true examples 29555\n",
      "  Batch 28,880  of  44,637.    Elapsed: 0:19:00. Training loss. 0.0018773105693981051 Num fake examples 28160 Num true examples 29600\n",
      "  Batch 28,920  of  44,637.    Elapsed: 0:19:02. Training loss. 0.0022211603354662657 Num fake examples 28201 Num true examples 29639\n",
      "  Batch 28,960  of  44,637.    Elapsed: 0:19:04. Training loss. 0.0021902925800532103 Num fake examples 28238 Num true examples 29682\n",
      "  Batch 29,000  of  44,637.    Elapsed: 0:19:05. Training loss. 0.0019145308760926127 Num fake examples 28276 Num true examples 29724\n",
      "  Batch 29,040  of  44,637.    Elapsed: 0:19:07. Training loss. 0.0027524810284376144 Num fake examples 28310 Num true examples 29770\n",
      "  Batch 29,080  of  44,637.    Elapsed: 0:19:08. Training loss. 0.002379136858507991 Num fake examples 28345 Num true examples 29815\n",
      "  Batch 29,120  of  44,637.    Elapsed: 0:19:10. Training loss. 0.0020805425010621548 Num fake examples 28384 Num true examples 29856\n",
      "  Batch 29,160  of  44,637.    Elapsed: 0:19:11. Training loss. 0.0027371582109481096 Num fake examples 28426 Num true examples 29894\n",
      "  Batch 29,200  of  44,637.    Elapsed: 0:19:13. Training loss. 0.003088906407356262 Num fake examples 28467 Num true examples 29933\n",
      "  Batch 29,240  of  44,637.    Elapsed: 0:19:15. Training loss. 0.002817117841914296 Num fake examples 28512 Num true examples 29968\n",
      "  Batch 29,280  of  44,637.    Elapsed: 0:19:16. Training loss. 0.004002733156085014 Num fake examples 28543 Num true examples 30017\n",
      "  Batch 29,320  of  44,637.    Elapsed: 0:19:18. Training loss. 0.0025585154071450233 Num fake examples 28581 Num true examples 30059\n",
      "  Batch 29,360  of  44,637.    Elapsed: 0:19:19. Training loss. 3.08954119682312 Num fake examples 28625 Num true examples 30095\n",
      "  Batch 29,400  of  44,637.    Elapsed: 0:19:21. Training loss. 0.0037798339035362005 Num fake examples 28668 Num true examples 30132\n",
      "  Batch 29,440  of  44,637.    Elapsed: 0:19:22. Training loss. 0.0023140073753893375 Num fake examples 28711 Num true examples 30169\n",
      "  Batch 29,480  of  44,637.    Elapsed: 0:19:24. Training loss. 0.0028597142081707716 Num fake examples 28748 Num true examples 30212\n",
      "  Batch 29,520  of  44,637.    Elapsed: 0:19:26. Training loss. 0.002903643064200878 Num fake examples 28776 Num true examples 30264\n",
      "  Batch 29,560  of  44,637.    Elapsed: 0:19:27. Training loss. 0.00294314743950963 Num fake examples 28815 Num true examples 30305\n",
      "  Batch 29,600  of  44,637.    Elapsed: 0:19:29. Training loss. 2.8297691345214844 Num fake examples 28854 Num true examples 30346\n",
      "  Batch 29,640  of  44,637.    Elapsed: 0:19:30. Training loss. 0.004169140011072159 Num fake examples 28894 Num true examples 30386\n",
      "  Batch 29,680  of  44,637.    Elapsed: 0:19:32. Training loss. 0.004253569059073925 Num fake examples 28938 Num true examples 30422\n",
      "  Batch 29,720  of  44,637.    Elapsed: 0:19:34. Training loss. 0.003499450162053108 Num fake examples 28975 Num true examples 30465\n",
      "  Batch 29,760  of  44,637.    Elapsed: 0:19:35. Training loss. 0.0026423430535942316 Num fake examples 29018 Num true examples 30502\n",
      "  Batch 29,800  of  44,637.    Elapsed: 0:19:37. Training loss. 0.002594124060124159 Num fake examples 29057 Num true examples 30543\n",
      "  Batch 29,840  of  44,637.    Elapsed: 0:19:38. Training loss. 0.0022817966528236866 Num fake examples 29099 Num true examples 30581\n",
      "  Batch 29,880  of  44,637.    Elapsed: 0:19:40. Training loss. 0.0030389996245503426 Num fake examples 29136 Num true examples 30624\n",
      "  Batch 29,920  of  44,637.    Elapsed: 0:19:41. Training loss. 0.0023792185820639133 Num fake examples 29179 Num true examples 30661\n",
      "  Batch 29,960  of  44,637.    Elapsed: 0:19:43. Training loss. 0.0023174118250608444 Num fake examples 29219 Num true examples 30701\n",
      "  Batch 30,000  of  44,637.    Elapsed: 0:19:45. Training loss. 0.0025613135658204556 Num fake examples 29262 Num true examples 30738\n",
      "  Batch 30,040  of  44,637.    Elapsed: 0:19:46. Training loss. 0.0025587657000869513 Num fake examples 29297 Num true examples 30783\n",
      "  Batch 30,080  of  44,637.    Elapsed: 0:19:48. Training loss. 0.0030895364470779896 Num fake examples 29333 Num true examples 30827\n",
      "  Batch 30,120  of  44,637.    Elapsed: 0:19:49. Training loss. 2.859905242919922 Num fake examples 29371 Num true examples 30869\n",
      "  Batch 30,160  of  44,637.    Elapsed: 0:19:51. Training loss. 0.0033118282444775105 Num fake examples 29412 Num true examples 30908\n",
      "  Batch 30,200  of  44,637.    Elapsed: 0:19:53. Training loss. 0.001690147677436471 Num fake examples 29452 Num true examples 30948\n",
      "  Batch 30,240  of  44,637.    Elapsed: 0:19:54. Training loss. 0.0029600518755614758 Num fake examples 29492 Num true examples 30988\n",
      "  Batch 30,280  of  44,637.    Elapsed: 0:19:56. Training loss. 0.0013571018353104591 Num fake examples 29527 Num true examples 31033\n",
      "  Batch 30,320  of  44,637.    Elapsed: 0:19:57. Training loss. 0.0027876747772097588 Num fake examples 29569 Num true examples 31071\n",
      "  Batch 30,360  of  44,637.    Elapsed: 0:19:59. Training loss. 0.002022015629336238 Num fake examples 29617 Num true examples 31103\n",
      "  Batch 30,400  of  44,637.    Elapsed: 0:20:00. Training loss. 0.0026499461382627487 Num fake examples 29653 Num true examples 31147\n",
      "  Batch 30,440  of  44,637.    Elapsed: 0:20:02. Training loss. 0.0016439579194411635 Num fake examples 29684 Num true examples 31196\n",
      "  Batch 30,480  of  44,637.    Elapsed: 0:20:04. Training loss. 0.002503987867385149 Num fake examples 29735 Num true examples 31225\n",
      "  Batch 30,520  of  44,637.    Elapsed: 0:20:05. Training loss. 0.0025672849733382463 Num fake examples 29766 Num true examples 31274\n",
      "  Batch 30,560  of  44,637.    Elapsed: 0:20:07. Training loss. 0.0024009973276406527 Num fake examples 29805 Num true examples 31315\n",
      "  Batch 30,600  of  44,637.    Elapsed: 0:20:08. Training loss. 0.0027586249634623528 Num fake examples 29844 Num true examples 31356\n",
      "  Batch 30,640  of  44,637.    Elapsed: 0:20:10. Training loss. 0.002462209202349186 Num fake examples 29880 Num true examples 31400\n",
      "  Batch 30,680  of  44,637.    Elapsed: 0:20:12. Training loss. 0.0027072476223111153 Num fake examples 29919 Num true examples 31441\n",
      "  Batch 30,720  of  44,637.    Elapsed: 0:20:13. Training loss. 0.001960056833922863 Num fake examples 29962 Num true examples 31478\n",
      "  Batch 30,760  of  44,637.    Elapsed: 0:20:15. Training loss. 0.0021943910978734493 Num fake examples 30007 Num true examples 31513\n",
      "  Batch 30,800  of  44,637.    Elapsed: 0:20:16. Training loss. 0.0016517267795279622 Num fake examples 30038 Num true examples 31562\n",
      "  Batch 30,840  of  44,637.    Elapsed: 0:20:18. Training loss. 0.0019325686153024435 Num fake examples 30070 Num true examples 31610\n",
      "  Batch 30,880  of  44,637.    Elapsed: 0:20:19. Training loss. 0.0022899138275533915 Num fake examples 30109 Num true examples 31651\n",
      "  Batch 30,920  of  44,637.    Elapsed: 0:20:21. Training loss. 0.0035979317035526037 Num fake examples 30146 Num true examples 31694\n",
      "  Batch 30,960  of  44,637.    Elapsed: 0:20:23. Training loss. 0.0025260872207581997 Num fake examples 30191 Num true examples 31729\n",
      "  Batch 31,000  of  44,637.    Elapsed: 0:20:24. Training loss. 0.002469654195010662 Num fake examples 30234 Num true examples 31766\n",
      "  Batch 31,040  of  44,637.    Elapsed: 0:20:26. Training loss. 0.0020947568118572235 Num fake examples 30276 Num true examples 31804\n",
      "  Batch 31,080  of  44,637.    Elapsed: 0:20:27. Training loss. 0.003243612125515938 Num fake examples 30315 Num true examples 31845\n",
      "  Batch 31,120  of  44,637.    Elapsed: 0:20:29. Training loss. 0.0019047032110393047 Num fake examples 30358 Num true examples 31882\n",
      "  Batch 31,160  of  44,637.    Elapsed: 0:20:30. Training loss. 0.0024195201694965363 Num fake examples 30388 Num true examples 31932\n",
      "  Batch 31,200  of  44,637.    Elapsed: 0:20:32. Training loss. 0.002956294920295477 Num fake examples 30427 Num true examples 31973\n",
      "  Batch 31,240  of  44,637.    Elapsed: 0:20:34. Training loss. 3.0894930362701416 Num fake examples 30470 Num true examples 32010\n",
      "  Batch 31,280  of  44,637.    Elapsed: 0:20:35. Training loss. 0.002960460726171732 Num fake examples 30504 Num true examples 32056\n",
      "  Batch 31,320  of  44,637.    Elapsed: 0:20:37. Training loss. 0.0048778243362903595 Num fake examples 30537 Num true examples 32103\n",
      "  Batch 31,360  of  44,637.    Elapsed: 0:20:38. Training loss. 0.0036437581293284893 Num fake examples 30576 Num true examples 32144\n",
      "  Batch 31,400  of  44,637.    Elapsed: 0:20:40. Training loss. 0.002928047440946102 Num fake examples 30627 Num true examples 32173\n",
      "  Batch 31,440  of  44,637.    Elapsed: 0:20:42. Training loss. 0.0022576109040528536 Num fake examples 30671 Num true examples 32209\n",
      "  Batch 31,480  of  44,637.    Elapsed: 0:20:43. Training loss. 0.002462805015966296 Num fake examples 30723 Num true examples 32237\n",
      "  Batch 31,520  of  44,637.    Elapsed: 0:20:45. Training loss. 0.0037552982103079557 Num fake examples 30756 Num true examples 32284\n",
      "  Batch 31,560  of  44,637.    Elapsed: 0:20:46. Training loss. 0.00242607737891376 Num fake examples 30799 Num true examples 32321\n",
      "  Batch 31,600  of  44,637.    Elapsed: 0:20:48. Training loss. 0.0028609249275177717 Num fake examples 30834 Num true examples 32366\n",
      "  Batch 31,640  of  44,637.    Elapsed: 0:20:49. Training loss. 0.002405292121693492 Num fake examples 30867 Num true examples 32413\n",
      "  Batch 31,680  of  44,637.    Elapsed: 0:20:51. Training loss. 0.004122340586036444 Num fake examples 30902 Num true examples 32458\n",
      "  Batch 31,720  of  44,637.    Elapsed: 0:20:53. Training loss. 0.0027142472099512815 Num fake examples 30944 Num true examples 32496\n",
      "  Batch 31,760  of  44,637.    Elapsed: 0:20:54. Training loss. 2.6284961700439453 Num fake examples 30985 Num true examples 32535\n",
      "  Batch 31,800  of  44,637.    Elapsed: 0:20:56. Training loss. 0.0033674740698188543 Num fake examples 31022 Num true examples 32578\n",
      "  Batch 31,840  of  44,637.    Elapsed: 0:20:57. Training loss. 0.0014028314035385847 Num fake examples 31063 Num true examples 32617\n",
      "  Batch 31,880  of  44,637.    Elapsed: 0:20:59. Training loss. 0.0030576339922845364 Num fake examples 31097 Num true examples 32663\n",
      "  Batch 31,920  of  44,637.    Elapsed: 0:21:00. Training loss. 0.00247795763425529 Num fake examples 31137 Num true examples 32703\n",
      "  Batch 31,960  of  44,637.    Elapsed: 0:21:02. Training loss. 0.00311626517213881 Num fake examples 31167 Num true examples 32753\n",
      "  Batch 32,000  of  44,637.    Elapsed: 0:21:04. Training loss. 0.002750388579443097 Num fake examples 31203 Num true examples 32797\n",
      "  Batch 32,040  of  44,637.    Elapsed: 0:21:05. Training loss. 0.0027323192916810513 Num fake examples 31241 Num true examples 32839\n",
      "  Batch 32,080  of  44,637.    Elapsed: 0:21:07. Training loss. 0.005497046746313572 Num fake examples 31292 Num true examples 32868\n",
      "  Batch 32,120  of  44,637.    Elapsed: 0:21:08. Training loss. 0.0030232975259423256 Num fake examples 31321 Num true examples 32919\n",
      "  Batch 32,160  of  44,637.    Elapsed: 0:21:10. Training loss. 0.0039370618760585785 Num fake examples 31358 Num true examples 32962\n",
      "  Batch 32,200  of  44,637.    Elapsed: 0:21:11. Training loss. 0.003967084921896458 Num fake examples 31403 Num true examples 32997\n",
      "  Batch 32,240  of  44,637.    Elapsed: 0:21:13. Training loss. 0.0037030621897429228 Num fake examples 31445 Num true examples 33035\n",
      "  Batch 32,280  of  44,637.    Elapsed: 0:21:15. Training loss. 0.005341101437807083 Num fake examples 31484 Num true examples 33076\n",
      "  Batch 32,320  of  44,637.    Elapsed: 0:21:16. Training loss. 0.004969845525920391 Num fake examples 31523 Num true examples 33117\n",
      "  Batch 32,360  of  44,637.    Elapsed: 0:21:18. Training loss. 0.0025620257947593927 Num fake examples 31557 Num true examples 33163\n",
      "  Batch 32,400  of  44,637.    Elapsed: 0:21:19. Training loss. 0.0038516935892403126 Num fake examples 31590 Num true examples 33210\n",
      "  Batch 32,440  of  44,637.    Elapsed: 0:21:21. Training loss. 0.002630334347486496 Num fake examples 31627 Num true examples 33253\n",
      "  Batch 32,480  of  44,637.    Elapsed: 0:21:23. Training loss. 0.002567910123616457 Num fake examples 31674 Num true examples 33286\n",
      "  Batch 32,520  of  44,637.    Elapsed: 0:21:24. Training loss. 0.002790256403386593 Num fake examples 31715 Num true examples 33325\n",
      "  Batch 32,560  of  44,637.    Elapsed: 0:21:26. Training loss. 0.0031480176839977503 Num fake examples 31753 Num true examples 33367\n",
      "  Batch 32,600  of  44,637.    Elapsed: 0:21:27. Training loss. 0.0025112349539995193 Num fake examples 31794 Num true examples 33406\n",
      "  Batch 32,640  of  44,637.    Elapsed: 0:21:29. Training loss. 0.0036114053800702095 Num fake examples 31830 Num true examples 33450\n",
      "  Batch 32,680  of  44,637.    Elapsed: 0:21:30. Training loss. 0.003563009900972247 Num fake examples 31869 Num true examples 33491\n",
      "  Batch 32,720  of  44,637.    Elapsed: 0:21:32. Training loss. 0.0035293581895530224 Num fake examples 31908 Num true examples 33532\n",
      "  Batch 32,760  of  44,637.    Elapsed: 0:21:34. Training loss. 0.003357924986630678 Num fake examples 31952 Num true examples 33568\n",
      "  Batch 32,800  of  44,637.    Elapsed: 0:21:35. Training loss. 0.003510938258841634 Num fake examples 31983 Num true examples 33617\n",
      "  Batch 32,840  of  44,637.    Elapsed: 0:21:37. Training loss. 0.0030566086061298847 Num fake examples 32023 Num true examples 33657\n",
      "  Batch 32,880  of  44,637.    Elapsed: 0:21:38. Training loss. 0.0024337894283235073 Num fake examples 32062 Num true examples 33698\n",
      "  Batch 32,920  of  44,637.    Elapsed: 0:21:40. Training loss. 0.0037907171063125134 Num fake examples 32099 Num true examples 33741\n",
      "  Batch 32,960  of  44,637.    Elapsed: 0:21:42. Training loss. 0.0027355714701116085 Num fake examples 32143 Num true examples 33777\n",
      "  Batch 33,000  of  44,637.    Elapsed: 0:21:43. Training loss. 0.0035799089819192886 Num fake examples 32181 Num true examples 33819\n",
      "  Batch 33,040  of  44,637.    Elapsed: 0:21:45. Training loss. 0.004134277813136578 Num fake examples 32220 Num true examples 33860\n",
      "  Batch 33,080  of  44,637.    Elapsed: 0:21:46. Training loss. 0.004236286506056786 Num fake examples 32261 Num true examples 33899\n",
      "  Batch 33,120  of  44,637.    Elapsed: 0:21:48. Training loss. 0.0036509381607174873 Num fake examples 32295 Num true examples 33945\n",
      "  Batch 33,160  of  44,637.    Elapsed: 0:21:49. Training loss. 0.0030962717719376087 Num fake examples 32325 Num true examples 33995\n",
      "  Batch 33,200  of  44,637.    Elapsed: 0:21:51. Training loss. 0.004150470718741417 Num fake examples 32366 Num true examples 34034\n",
      "  Batch 33,240  of  44,637.    Elapsed: 0:21:53. Training loss. 0.004036204889416695 Num fake examples 32404 Num true examples 34076\n",
      "  Batch 33,280  of  44,637.    Elapsed: 0:21:54. Training loss. 0.0037839910946786404 Num fake examples 32442 Num true examples 34118\n",
      "  Batch 33,320  of  44,637.    Elapsed: 0:21:56. Training loss. 2.7502198219299316 Num fake examples 32484 Num true examples 34156\n",
      "  Batch 33,360  of  44,637.    Elapsed: 0:21:58. Training loss. 0.003646479919552803 Num fake examples 32520 Num true examples 34200\n",
      "  Batch 33,400  of  44,637.    Elapsed: 0:21:59. Training loss. 0.004274450242519379 Num fake examples 32570 Num true examples 34230\n",
      "  Batch 33,440  of  44,637.    Elapsed: 0:22:01. Training loss. 0.004679256118834019 Num fake examples 32610 Num true examples 34270\n",
      "  Batch 33,480  of  44,637.    Elapsed: 0:22:02. Training loss. 0.004455177113413811 Num fake examples 32643 Num true examples 34317\n",
      "  Batch 33,520  of  44,637.    Elapsed: 0:22:04. Training loss. 0.0030510269571095705 Num fake examples 32677 Num true examples 34363\n",
      "  Batch 33,560  of  44,637.    Elapsed: 0:22:06. Training loss. 0.003017398063093424 Num fake examples 32710 Num true examples 34410\n",
      "  Batch 33,600  of  44,637.    Elapsed: 0:22:07. Training loss. 0.0031099109910428524 Num fake examples 32750 Num true examples 34450\n",
      "  Batch 33,640  of  44,637.    Elapsed: 0:22:09. Training loss. 0.0036085122264921665 Num fake examples 32789 Num true examples 34491\n",
      "  Batch 33,680  of  44,637.    Elapsed: 0:22:10. Training loss. 0.003524376079440117 Num fake examples 32822 Num true examples 34538\n",
      "  Batch 33,720  of  44,637.    Elapsed: 0:22:12. Training loss. 0.004964493215084076 Num fake examples 32857 Num true examples 34583\n",
      "  Batch 33,760  of  44,637.    Elapsed: 0:22:13. Training loss. 0.0022288556210696697 Num fake examples 32896 Num true examples 34624\n",
      "  Batch 33,800  of  44,637.    Elapsed: 0:22:15. Training loss. 0.002881180727854371 Num fake examples 32942 Num true examples 34658\n",
      "  Batch 33,840  of  44,637.    Elapsed: 0:22:17. Training loss. 0.0031643141992390156 Num fake examples 32973 Num true examples 34707\n",
      "  Batch 33,880  of  44,637.    Elapsed: 0:22:18. Training loss. 0.0014060188550502062 Num fake examples 33015 Num true examples 34745\n",
      "  Batch 33,920  of  44,637.    Elapsed: 0:22:20. Training loss. 2.841453790664673 Num fake examples 33044 Num true examples 34796\n",
      "  Batch 33,960  of  44,637.    Elapsed: 0:22:21. Training loss. 0.003322489792481065 Num fake examples 33078 Num true examples 34842\n",
      "  Batch 34,000  of  44,637.    Elapsed: 0:22:23. Training loss. 2.6407294273376465 Num fake examples 33120 Num true examples 34880\n",
      "  Batch 34,040  of  44,637.    Elapsed: 0:22:25. Training loss. 2.840573787689209 Num fake examples 33161 Num true examples 34919\n",
      "  Batch 34,080  of  44,637.    Elapsed: 0:22:26. Training loss. 2.8761343955993652 Num fake examples 33203 Num true examples 34957\n",
      "  Batch 34,120  of  44,637.    Elapsed: 0:22:28. Training loss. 0.002641553757712245 Num fake examples 33237 Num true examples 35003\n",
      "  Batch 34,160  of  44,637.    Elapsed: 0:22:29. Training loss. 0.0035907903220504522 Num fake examples 33275 Num true examples 35045\n",
      "  Batch 34,200  of  44,637.    Elapsed: 0:22:31. Training loss. 0.0031593581661581993 Num fake examples 33324 Num true examples 35076\n",
      "  Batch 34,240  of  44,637.    Elapsed: 0:22:33. Training loss. 0.003736465238034725 Num fake examples 33364 Num true examples 35116\n",
      "  Batch 34,280  of  44,637.    Elapsed: 0:22:34. Training loss. 0.0037715050857514143 Num fake examples 33397 Num true examples 35163\n",
      "  Batch 34,320  of  44,637.    Elapsed: 0:22:36. Training loss. 0.0041087474673986435 Num fake examples 33432 Num true examples 35208\n",
      "  Batch 34,360  of  44,637.    Elapsed: 0:22:37. Training loss. 0.0031232372857630253 Num fake examples 33473 Num true examples 35247\n",
      "  Batch 34,400  of  44,637.    Elapsed: 0:22:39. Training loss. 0.002277611754834652 Num fake examples 33518 Num true examples 35282\n",
      "  Batch 34,440  of  44,637.    Elapsed: 0:22:41. Training loss. 0.0027799836825579405 Num fake examples 33562 Num true examples 35318\n",
      "  Batch 34,480  of  44,637.    Elapsed: 0:22:42. Training loss. 0.00332071166485548 Num fake examples 33600 Num true examples 35360\n",
      "  Batch 34,520  of  44,637.    Elapsed: 0:22:44. Training loss. 0.0037021818570792675 Num fake examples 33639 Num true examples 35401\n",
      "  Batch 34,560  of  44,637.    Elapsed: 0:22:46. Training loss. 0.002477517817169428 Num fake examples 33679 Num true examples 35441\n",
      "  Batch 34,600  of  44,637.    Elapsed: 0:22:47. Training loss. 0.0018933964893221855 Num fake examples 33718 Num true examples 35482\n",
      "  Batch 34,640  of  44,637.    Elapsed: 0:22:49. Training loss. 0.0026441016234457493 Num fake examples 33752 Num true examples 35528\n",
      "  Batch 34,680  of  44,637.    Elapsed: 0:22:50. Training loss. 0.002583516063168645 Num fake examples 33792 Num true examples 35568\n",
      "  Batch 34,720  of  44,637.    Elapsed: 0:22:52. Training loss. 0.0030227582901716232 Num fake examples 33830 Num true examples 35610\n",
      "  Batch 34,760  of  44,637.    Elapsed: 0:22:53. Training loss. 0.0035497169010341167 Num fake examples 33877 Num true examples 35643\n",
      "  Batch 34,800  of  44,637.    Elapsed: 0:22:55. Training loss. 0.0029930085875093937 Num fake examples 33915 Num true examples 35685\n",
      "  Batch 34,840  of  44,637.    Elapsed: 0:22:57. Training loss. 0.0029785316437482834 Num fake examples 33960 Num true examples 35720\n",
      "  Batch 34,880  of  44,637.    Elapsed: 0:22:58. Training loss. 0.003489063121378422 Num fake examples 34006 Num true examples 35754\n",
      "  Batch 34,920  of  44,637.    Elapsed: 0:23:00. Training loss. 0.0036014297511428595 Num fake examples 34045 Num true examples 35795\n",
      "  Batch 34,960  of  44,637.    Elapsed: 0:23:01. Training loss. 0.0036411175969988108 Num fake examples 34087 Num true examples 35833\n",
      "  Batch 35,000  of  44,637.    Elapsed: 0:23:03. Training loss. 0.004834881983697414 Num fake examples 34123 Num true examples 35877\n",
      "  Batch 35,040  of  44,637.    Elapsed: 0:23:05. Training loss. 0.003983356989920139 Num fake examples 34166 Num true examples 35914\n",
      "  Batch 35,080  of  44,637.    Elapsed: 0:23:06. Training loss. 0.0031743766739964485 Num fake examples 34200 Num true examples 35960\n",
      "  Batch 35,120  of  44,637.    Elapsed: 0:23:08. Training loss. 0.0033546818885952234 Num fake examples 34229 Num true examples 36011\n",
      "  Batch 35,160  of  44,637.    Elapsed: 0:23:09. Training loss. 0.0021077822893857956 Num fake examples 34269 Num true examples 36051\n",
      "  Batch 35,200  of  44,637.    Elapsed: 0:23:11. Training loss. 0.0025106705725193024 Num fake examples 34311 Num true examples 36089\n",
      "  Batch 35,240  of  44,637.    Elapsed: 0:23:12. Training loss. 0.0023469384759664536 Num fake examples 34347 Num true examples 36133\n",
      "  Batch 35,280  of  44,637.    Elapsed: 0:23:14. Training loss. 0.003039292059838772 Num fake examples 34392 Num true examples 36168\n",
      "  Batch 35,320  of  44,637.    Elapsed: 0:23:16. Training loss. 0.0017933564959093928 Num fake examples 34432 Num true examples 36208\n",
      "  Batch 35,360  of  44,637.    Elapsed: 0:23:17. Training loss. 0.0021957815624773502 Num fake examples 34471 Num true examples 36249\n",
      "  Batch 35,400  of  44,637.    Elapsed: 0:23:19. Training loss. 0.0029097492806613445 Num fake examples 34510 Num true examples 36290\n",
      "  Batch 35,440  of  44,637.    Elapsed: 0:23:20. Training loss. 0.0030472995713353157 Num fake examples 34543 Num true examples 36337\n",
      "  Batch 35,480  of  44,637.    Elapsed: 0:23:22. Training loss. 0.0037952105049043894 Num fake examples 34577 Num true examples 36383\n",
      "  Batch 35,520  of  44,637.    Elapsed: 0:23:23. Training loss. 0.002722968813031912 Num fake examples 34618 Num true examples 36422\n",
      "  Batch 35,560  of  44,637.    Elapsed: 0:23:25. Training loss. 0.002766865771263838 Num fake examples 34659 Num true examples 36461\n",
      "  Batch 35,600  of  44,637.    Elapsed: 0:23:27. Training loss. 0.003306474070996046 Num fake examples 34698 Num true examples 36502\n",
      "  Batch 35,640  of  44,637.    Elapsed: 0:23:28. Training loss. 2.9609363079071045 Num fake examples 34743 Num true examples 36537\n",
      "  Batch 35,680  of  44,637.    Elapsed: 0:23:30. Training loss. 0.0027301479130983353 Num fake examples 34782 Num true examples 36578\n",
      "  Batch 35,720  of  44,637.    Elapsed: 0:23:31. Training loss. 0.004386178217828274 Num fake examples 34827 Num true examples 36613\n",
      "  Batch 35,760  of  44,637.    Elapsed: 0:23:33. Training loss. 0.0029737104196101427 Num fake examples 34874 Num true examples 36646\n",
      "  Batch 35,800  of  44,637.    Elapsed: 0:23:35. Training loss. 0.0032835754100233316 Num fake examples 34913 Num true examples 36687\n",
      "  Batch 35,840  of  44,637.    Elapsed: 0:23:36. Training loss. 0.0023120595142245293 Num fake examples 34945 Num true examples 36735\n",
      "  Batch 35,880  of  44,637.    Elapsed: 0:23:38. Training loss. 0.0025979720521718264 Num fake examples 34983 Num true examples 36777\n",
      "  Batch 35,920  of  44,637.    Elapsed: 0:23:39. Training loss. 0.002460966818034649 Num fake examples 35024 Num true examples 36816\n",
      "  Batch 35,960  of  44,637.    Elapsed: 0:23:41. Training loss. 0.0026236786507070065 Num fake examples 35063 Num true examples 36857\n",
      "  Batch 36,000  of  44,637.    Elapsed: 0:23:42. Training loss. 0.0024291323497891426 Num fake examples 35103 Num true examples 36897\n",
      "  Batch 36,040  of  44,637.    Elapsed: 0:23:44. Training loss. 3.087110996246338 Num fake examples 35146 Num true examples 36934\n",
      "  Batch 36,080  of  44,637.    Elapsed: 0:23:46. Training loss. 0.0027359328232705593 Num fake examples 35181 Num true examples 36979\n",
      "  Batch 36,120  of  44,637.    Elapsed: 0:23:47. Training loss. 0.004191224463284016 Num fake examples 35225 Num true examples 37015\n",
      "  Batch 36,160  of  44,637.    Elapsed: 0:23:49. Training loss. 0.002130660926923156 Num fake examples 35265 Num true examples 37055\n",
      "  Batch 36,200  of  44,637.    Elapsed: 0:23:50. Training loss. 0.0016356096602976322 Num fake examples 35309 Num true examples 37091\n",
      "  Batch 36,240  of  44,637.    Elapsed: 0:23:52. Training loss. 0.003530771005898714 Num fake examples 35356 Num true examples 37124\n",
      "  Batch 36,280  of  44,637.    Elapsed: 0:23:53. Training loss. 0.004115703981369734 Num fake examples 35391 Num true examples 37169\n",
      "  Batch 36,320  of  44,637.    Elapsed: 0:23:55. Training loss. 0.004453534726053476 Num fake examples 35435 Num true examples 37205\n",
      "  Batch 36,360  of  44,637.    Elapsed: 0:23:57. Training loss. 0.0025588241405785084 Num fake examples 35484 Num true examples 37236\n",
      "  Batch 36,400  of  44,637.    Elapsed: 0:23:58. Training loss. 0.0030199913308024406 Num fake examples 35521 Num true examples 37279\n",
      "  Batch 36,440  of  44,637.    Elapsed: 0:24:00. Training loss. 0.0027924906462430954 Num fake examples 35563 Num true examples 37317\n",
      "  Batch 36,480  of  44,637.    Elapsed: 0:24:01. Training loss. 0.0029885678086429834 Num fake examples 35595 Num true examples 37365\n",
      "  Batch 36,520  of  44,637.    Elapsed: 0:24:03. Training loss. 0.00251837563700974 Num fake examples 35629 Num true examples 37411\n",
      "  Batch 36,560  of  44,637.    Elapsed: 0:24:05. Training loss. 0.0022614686749875546 Num fake examples 35663 Num true examples 37457\n",
      "  Batch 36,600  of  44,637.    Elapsed: 0:24:06. Training loss. 2.9621565341949463 Num fake examples 35703 Num true examples 37497\n",
      "  Batch 36,640  of  44,637.    Elapsed: 0:24:08. Training loss. 0.0030753780156373978 Num fake examples 35746 Num true examples 37534\n",
      "  Batch 36,680  of  44,637.    Elapsed: 0:24:09. Training loss. 0.003547772066667676 Num fake examples 35786 Num true examples 37574\n",
      "  Batch 36,720  of  44,637.    Elapsed: 0:24:11. Training loss. 0.00339804170653224 Num fake examples 35822 Num true examples 37618\n",
      "  Batch 36,760  of  44,637.    Elapsed: 0:24:12. Training loss. 0.0030592193361371756 Num fake examples 35865 Num true examples 37655\n",
      "  Batch 36,800  of  44,637.    Elapsed: 0:24:14. Training loss. 0.0029957639053463936 Num fake examples 35899 Num true examples 37701\n",
      "  Batch 36,840  of  44,637.    Elapsed: 0:24:16. Training loss. 0.0036633629351854324 Num fake examples 35942 Num true examples 37738\n",
      "  Batch 36,880  of  44,637.    Elapsed: 0:24:17. Training loss. 0.005191500298678875 Num fake examples 35979 Num true examples 37781\n",
      "  Batch 36,920  of  44,637.    Elapsed: 0:24:19. Training loss. 0.003462887601926923 Num fake examples 36015 Num true examples 37825\n",
      "  Batch 36,960  of  44,637.    Elapsed: 0:24:20. Training loss. 0.004129280801862478 Num fake examples 36050 Num true examples 37870\n",
      "  Batch 37,000  of  44,637.    Elapsed: 0:24:22. Training loss. 0.003322226693853736 Num fake examples 36092 Num true examples 37908\n",
      "  Batch 37,040  of  44,637.    Elapsed: 0:24:23. Training loss. 2.9720349311828613 Num fake examples 36127 Num true examples 37953\n",
      "  Batch 37,080  of  44,637.    Elapsed: 0:24:25. Training loss. 0.0024019284173846245 Num fake examples 36164 Num true examples 37996\n",
      "  Batch 37,120  of  44,637.    Elapsed: 0:24:27. Training loss. 0.0035563495475798845 Num fake examples 36200 Num true examples 38040\n",
      "  Batch 37,160  of  44,637.    Elapsed: 0:24:28. Training loss. 2.75775408744812 Num fake examples 36245 Num true examples 38075\n",
      "  Batch 37,200  of  44,637.    Elapsed: 0:24:30. Training loss. 0.004519280977547169 Num fake examples 36272 Num true examples 38128\n",
      "  Batch 37,240  of  44,637.    Elapsed: 0:24:31. Training loss. 0.0056031253188848495 Num fake examples 36309 Num true examples 38171\n",
      "  Batch 37,280  of  44,637.    Elapsed: 0:24:33. Training loss. 0.0029754710849374533 Num fake examples 36351 Num true examples 38209\n",
      "  Batch 37,320  of  44,637.    Elapsed: 0:24:35. Training loss. 0.005395346321165562 Num fake examples 36391 Num true examples 38249\n",
      "  Batch 37,360  of  44,637.    Elapsed: 0:24:36. Training loss. 0.0022866001818329096 Num fake examples 36435 Num true examples 38285\n",
      "  Batch 37,400  of  44,637.    Elapsed: 0:24:38. Training loss. 0.004540367983281612 Num fake examples 36475 Num true examples 38325\n",
      "  Batch 37,440  of  44,637.    Elapsed: 0:24:39. Training loss. 0.0017971787601709366 Num fake examples 36516 Num true examples 38364\n",
      "  Batch 37,480  of  44,637.    Elapsed: 0:24:41. Training loss. 0.004054097458720207 Num fake examples 36563 Num true examples 38397\n",
      "  Batch 37,520  of  44,637.    Elapsed: 0:24:43. Training loss. 0.0034941155463457108 Num fake examples 36599 Num true examples 38441\n",
      "  Batch 37,560  of  44,637.    Elapsed: 0:24:44. Training loss. 0.0029709069058299065 Num fake examples 36632 Num true examples 38488\n",
      "  Batch 37,600  of  44,637.    Elapsed: 0:24:46. Training loss. 0.003173793898895383 Num fake examples 36674 Num true examples 38526\n",
      "  Batch 37,640  of  44,637.    Elapsed: 0:24:47. Training loss. 0.003879702650010586 Num fake examples 36711 Num true examples 38569\n",
      "  Batch 37,680  of  44,637.    Elapsed: 0:24:49. Training loss. 0.004504854790866375 Num fake examples 36752 Num true examples 38608\n",
      "  Batch 37,720  of  44,637.    Elapsed: 0:24:51. Training loss. 0.00286317290738225 Num fake examples 36796 Num true examples 38644\n",
      "  Batch 37,760  of  44,637.    Elapsed: 0:24:52. Training loss. 0.002966004190966487 Num fake examples 36837 Num true examples 38683\n",
      "  Batch 37,800  of  44,637.    Elapsed: 0:24:54. Training loss. 0.0028994777239859104 Num fake examples 36870 Num true examples 38730\n",
      "  Batch 37,840  of  44,637.    Elapsed: 0:24:55. Training loss. 0.0040636262856423855 Num fake examples 36907 Num true examples 38773\n",
      "  Batch 37,880  of  44,637.    Elapsed: 0:24:57. Training loss. 2.584791898727417 Num fake examples 36945 Num true examples 38815\n",
      "  Batch 37,920  of  44,637.    Elapsed: 0:24:58. Training loss. 0.003780813654884696 Num fake examples 36984 Num true examples 38856\n",
      "  Batch 37,960  of  44,637.    Elapsed: 0:25:00. Training loss. 0.004295630380511284 Num fake examples 37021 Num true examples 38899\n",
      "  Batch 38,000  of  44,637.    Elapsed: 0:25:02. Training loss. 0.0026441840454936028 Num fake examples 37057 Num true examples 38943\n",
      "  Batch 38,040  of  44,637.    Elapsed: 0:25:03. Training loss. 0.0030660480260849 Num fake examples 37095 Num true examples 38985\n",
      "  Batch 38,080  of  44,637.    Elapsed: 0:25:05. Training loss. 0.0025637808721512556 Num fake examples 37136 Num true examples 39024\n",
      "  Batch 38,120  of  44,637.    Elapsed: 0:25:06. Training loss. 0.0018918710993602872 Num fake examples 37170 Num true examples 39070\n",
      "  Batch 38,160  of  44,637.    Elapsed: 0:25:08. Training loss. 0.002500389004126191 Num fake examples 37212 Num true examples 39108\n",
      "  Batch 38,200  of  44,637.    Elapsed: 0:25:09. Training loss. 0.003819230245426297 Num fake examples 37249 Num true examples 39151\n",
      "  Batch 38,240  of  44,637.    Elapsed: 0:25:11. Training loss. 0.00329603161662817 Num fake examples 37290 Num true examples 39190\n",
      "  Batch 38,280  of  44,637.    Elapsed: 0:25:13. Training loss. 3.0739166736602783 Num fake examples 37321 Num true examples 39239\n",
      "  Batch 38,320  of  44,637.    Elapsed: 0:25:14. Training loss. 0.0029310104437172413 Num fake examples 37359 Num true examples 39281\n",
      "  Batch 38,360  of  44,637.    Elapsed: 0:25:16. Training loss. 0.0031352038495242596 Num fake examples 37395 Num true examples 39325\n",
      "  Batch 38,400  of  44,637.    Elapsed: 0:25:17. Training loss. 0.0028676113579422235 Num fake examples 37431 Num true examples 39369\n",
      "  Batch 38,440  of  44,637.    Elapsed: 0:25:19. Training loss. 0.00332180829718709 Num fake examples 37466 Num true examples 39414\n",
      "  Batch 38,480  of  44,637.    Elapsed: 0:25:21. Training loss. 0.0016130771255120635 Num fake examples 37509 Num true examples 39451\n",
      "  Batch 38,520  of  44,637.    Elapsed: 0:25:22. Training loss. 0.003982066176831722 Num fake examples 37557 Num true examples 39483\n",
      "  Batch 38,560  of  44,637.    Elapsed: 0:25:24. Training loss. 0.002627280540764332 Num fake examples 37586 Num true examples 39534\n",
      "  Batch 38,600  of  44,637.    Elapsed: 0:25:26. Training loss. 0.002454918110743165 Num fake examples 37621 Num true examples 39579\n",
      "  Batch 38,640  of  44,637.    Elapsed: 0:25:27. Training loss. 0.004974115639925003 Num fake examples 37656 Num true examples 39624\n",
      "  Batch 38,680  of  44,637.    Elapsed: 0:25:29. Training loss. 0.003591920481994748 Num fake examples 37697 Num true examples 39663\n",
      "  Batch 38,720  of  44,637.    Elapsed: 0:25:30. Training loss. 0.003731660544872284 Num fake examples 37731 Num true examples 39709\n",
      "  Batch 38,760  of  44,637.    Elapsed: 0:25:32. Training loss. 0.0029745243955403566 Num fake examples 37767 Num true examples 39753\n",
      "  Batch 38,800  of  44,637.    Elapsed: 0:25:34. Training loss. 0.0038194311782717705 Num fake examples 37809 Num true examples 39791\n",
      "  Batch 38,840  of  44,637.    Elapsed: 0:25:35. Training loss. 0.003915200009942055 Num fake examples 37855 Num true examples 39825\n",
      "  Batch 38,880  of  44,637.    Elapsed: 0:25:37. Training loss. 0.004380934871733189 Num fake examples 37886 Num true examples 39874\n",
      "  Batch 38,920  of  44,637.    Elapsed: 0:25:38. Training loss. 0.004278120119124651 Num fake examples 37925 Num true examples 39915\n",
      "  Batch 38,960  of  44,637.    Elapsed: 0:25:40. Training loss. 0.004028533585369587 Num fake examples 37964 Num true examples 39956\n",
      "  Batch 39,000  of  44,637.    Elapsed: 0:25:42. Training loss. 0.00437235739082098 Num fake examples 37999 Num true examples 40001\n",
      "  Batch 39,040  of  44,637.    Elapsed: 0:25:43. Training loss. 0.004365809727460146 Num fake examples 38035 Num true examples 40045\n",
      "  Batch 39,080  of  44,637.    Elapsed: 0:25:45. Training loss. 0.004329284653067589 Num fake examples 38069 Num true examples 40091\n",
      "  Batch 39,120  of  44,637.    Elapsed: 0:25:46. Training loss. 0.00390535406768322 Num fake examples 38104 Num true examples 40136\n",
      "  Batch 39,160  of  44,637.    Elapsed: 0:25:48. Training loss. 0.0023611183278262615 Num fake examples 38140 Num true examples 40180\n",
      "  Batch 39,200  of  44,637.    Elapsed: 0:25:49. Training loss. 0.0036665750667452812 Num fake examples 38187 Num true examples 40213\n",
      "  Batch 39,240  of  44,637.    Elapsed: 0:25:51. Training loss. 0.0034221564419567585 Num fake examples 38231 Num true examples 40249\n",
      "  Batch 39,280  of  44,637.    Elapsed: 0:25:52. Training loss. 2.7560577392578125 Num fake examples 38269 Num true examples 40291\n",
      "  Batch 39,320  of  44,637.    Elapsed: 0:25:54. Training loss. 0.005637005437165499 Num fake examples 38303 Num true examples 40337\n",
      "  Batch 39,360  of  44,637.    Elapsed: 0:25:55. Training loss. 0.0034425973426550627 Num fake examples 38345 Num true examples 40375\n",
      "  Batch 39,400  of  44,637.    Elapsed: 0:25:57. Training loss. 0.004406745545566082 Num fake examples 38393 Num true examples 40407\n",
      "  Batch 39,440  of  44,637.    Elapsed: 0:25:59. Training loss. 0.003986048046499491 Num fake examples 38428 Num true examples 40452\n",
      "  Batch 39,480  of  44,637.    Elapsed: 0:26:00. Training loss. 0.004223205614835024 Num fake examples 38464 Num true examples 40496\n",
      "  Batch 39,520  of  44,637.    Elapsed: 0:26:02. Training loss. 0.003518792800605297 Num fake examples 38503 Num true examples 40537\n",
      "  Batch 39,560  of  44,637.    Elapsed: 0:26:03. Training loss. 0.003830299014225602 Num fake examples 38537 Num true examples 40583\n",
      "  Batch 39,600  of  44,637.    Elapsed: 0:26:05. Training loss. 0.003958971239626408 Num fake examples 38582 Num true examples 40618\n",
      "  Batch 39,640  of  44,637.    Elapsed: 0:26:06. Training loss. 0.004524778574705124 Num fake examples 38616 Num true examples 40664\n",
      "  Batch 39,680  of  44,637.    Elapsed: 0:26:08. Training loss. 0.003055606037378311 Num fake examples 38650 Num true examples 40710\n",
      "  Batch 39,720  of  44,637.    Elapsed: 0:26:09. Training loss. 0.00365534913726151 Num fake examples 38684 Num true examples 40756\n",
      "  Batch 39,760  of  44,637.    Elapsed: 0:26:10. Training loss. 0.003489820519462228 Num fake examples 38721 Num true examples 40799\n",
      "  Batch 39,800  of  44,637.    Elapsed: 0:26:12. Training loss. 0.0025281349662691355 Num fake examples 38752 Num true examples 40848\n",
      "  Batch 39,840  of  44,637.    Elapsed: 0:26:13. Training loss. 0.004945340100675821 Num fake examples 38794 Num true examples 40886\n",
      "  Batch 39,880  of  44,637.    Elapsed: 0:26:15. Training loss. 0.004539940040558577 Num fake examples 38839 Num true examples 40921\n",
      "  Batch 39,920  of  44,637.    Elapsed: 0:26:16. Training loss. 0.005037310067564249 Num fake examples 38884 Num true examples 40956\n",
      "  Batch 39,960  of  44,637.    Elapsed: 0:26:18. Training loss. 0.0036687178071588278 Num fake examples 38925 Num true examples 40995\n",
      "  Batch 40,000  of  44,637.    Elapsed: 0:26:19. Training loss. 0.003727781353518367 Num fake examples 38964 Num true examples 41036\n",
      "  Batch 40,040  of  44,637.    Elapsed: 0:26:21. Training loss. 0.0028288259636610746 Num fake examples 39001 Num true examples 41079\n",
      "  Batch 40,080  of  44,637.    Elapsed: 0:26:22. Training loss. 0.0029117893427610397 Num fake examples 39038 Num true examples 41122\n",
      "  Batch 40,120  of  44,637.    Elapsed: 0:26:24. Training loss. 0.0020499767269939184 Num fake examples 39081 Num true examples 41159\n",
      "  Batch 40,160  of  44,637.    Elapsed: 0:26:26. Training loss. 0.0021942483726888895 Num fake examples 39118 Num true examples 41202\n",
      "  Batch 40,200  of  44,637.    Elapsed: 0:26:27. Training loss. 0.002183535136282444 Num fake examples 39161 Num true examples 41239\n",
      "  Batch 40,240  of  44,637.    Elapsed: 0:26:29. Training loss. 0.002276394050568342 Num fake examples 39195 Num true examples 41285\n",
      "  Batch 40,280  of  44,637.    Elapsed: 0:26:30. Training loss. 0.0027266000397503376 Num fake examples 39235 Num true examples 41325\n",
      "  Batch 40,320  of  44,637.    Elapsed: 0:26:32. Training loss. 0.0014532210770994425 Num fake examples 39278 Num true examples 41362\n",
      "  Batch 40,360  of  44,637.    Elapsed: 0:26:33. Training loss. 0.002387410495430231 Num fake examples 39319 Num true examples 41401\n",
      "  Batch 40,400  of  44,637.    Elapsed: 0:26:35. Training loss. 0.0018069373909384012 Num fake examples 39355 Num true examples 41445\n",
      "  Batch 40,440  of  44,637.    Elapsed: 0:26:36. Training loss. 0.004471864551305771 Num fake examples 39401 Num true examples 41479\n",
      "  Batch 40,480  of  44,637.    Elapsed: 0:26:38. Training loss. 0.0046044159680604935 Num fake examples 39441 Num true examples 41519\n",
      "  Batch 40,520  of  44,637.    Elapsed: 0:26:39. Training loss. 0.004525113385170698 Num fake examples 39482 Num true examples 41558\n",
      "  Batch 40,560  of  44,637.    Elapsed: 0:26:41. Training loss. 0.004522382747381926 Num fake examples 39519 Num true examples 41601\n",
      "  Batch 40,600  of  44,637.    Elapsed: 0:26:42. Training loss. 0.00429145060479641 Num fake examples 39557 Num true examples 41643\n",
      "  Batch 40,640  of  44,637.    Elapsed: 0:26:44. Training loss. 0.003438206622377038 Num fake examples 39600 Num true examples 41680\n",
      "  Batch 40,680  of  44,637.    Elapsed: 0:26:45. Training loss. 0.003074992448091507 Num fake examples 39636 Num true examples 41724\n",
      "  Batch 40,720  of  44,637.    Elapsed: 0:26:47. Training loss. 0.0023657523561269045 Num fake examples 39677 Num true examples 41763\n",
      "  Batch 40,760  of  44,637.    Elapsed: 0:26:48. Training loss. 0.0025490394327789545 Num fake examples 39719 Num true examples 41801\n",
      "  Batch 40,800  of  44,637.    Elapsed: 0:26:50. Training loss. 0.002690730383619666 Num fake examples 39760 Num true examples 41840\n",
      "  Batch 40,840  of  44,637.    Elapsed: 0:26:51. Training loss. 0.0028556273318827152 Num fake examples 39804 Num true examples 41876\n",
      "  Batch 40,880  of  44,637.    Elapsed: 0:26:53. Training loss. 0.004408631939440966 Num fake examples 39841 Num true examples 41919\n",
      "  Batch 40,920  of  44,637.    Elapsed: 0:26:54. Training loss. 0.003665328724309802 Num fake examples 39883 Num true examples 41957\n",
      "  Batch 40,960  of  44,637.    Elapsed: 0:26:56. Training loss. 0.003044965909793973 Num fake examples 39917 Num true examples 42003\n",
      "  Batch 41,000  of  44,637.    Elapsed: 0:26:57. Training loss. 0.00335159944370389 Num fake examples 39956 Num true examples 42044\n",
      "  Batch 41,040  of  44,637.    Elapsed: 0:26:59. Training loss. 0.0034443007316440344 Num fake examples 39992 Num true examples 42088\n",
      "  Batch 41,080  of  44,637.    Elapsed: 0:27:00. Training loss. 0.005060818046331406 Num fake examples 40038 Num true examples 42122\n",
      "  Batch 41,120  of  44,637.    Elapsed: 0:27:02. Training loss. 2.6280503273010254 Num fake examples 40075 Num true examples 42165\n",
      "  Batch 41,160  of  44,637.    Elapsed: 0:27:03. Training loss. 0.0038566773291677237 Num fake examples 40120 Num true examples 42200\n",
      "  Batch 41,200  of  44,637.    Elapsed: 0:27:05. Training loss. 0.004059509839862585 Num fake examples 40164 Num true examples 42236\n",
      "  Batch 41,240  of  44,637.    Elapsed: 0:27:06. Training loss. 0.003433600068092346 Num fake examples 40198 Num true examples 42282\n",
      "  Batch 41,280  of  44,637.    Elapsed: 0:27:08. Training loss. 0.003270764835178852 Num fake examples 40240 Num true examples 42320\n",
      "  Batch 41,320  of  44,637.    Elapsed: 0:27:10. Training loss. 0.005738621577620506 Num fake examples 40274 Num true examples 42366\n",
      "  Batch 41,360  of  44,637.    Elapsed: 0:27:11. Training loss. 0.00647544814273715 Num fake examples 40306 Num true examples 42414\n",
      "  Batch 41,400  of  44,637.    Elapsed: 0:27:13. Training loss. 0.004737225826829672 Num fake examples 40350 Num true examples 42450\n",
      "  Batch 41,440  of  44,637.    Elapsed: 0:27:14. Training loss. 0.005013108253479004 Num fake examples 40388 Num true examples 42492\n",
      "  Batch 41,480  of  44,637.    Elapsed: 0:27:16. Training loss. 0.005048150662332773 Num fake examples 40429 Num true examples 42531\n",
      "  Batch 41,520  of  44,637.    Elapsed: 0:27:17. Training loss. 0.0037395735271275043 Num fake examples 40462 Num true examples 42578\n",
      "  Batch 41,560  of  44,637.    Elapsed: 0:27:19. Training loss. 0.0035355710424482822 Num fake examples 40502 Num true examples 42618\n",
      "  Batch 41,600  of  44,637.    Elapsed: 0:27:20. Training loss. 0.0036599317099899054 Num fake examples 40545 Num true examples 42655\n",
      "  Batch 41,640  of  44,637.    Elapsed: 0:27:22. Training loss. 0.0028448360972106457 Num fake examples 40575 Num true examples 42705\n",
      "  Batch 41,680  of  44,637.    Elapsed: 0:27:23. Training loss. 0.003540400415658951 Num fake examples 40619 Num true examples 42741\n",
      "  Batch 41,720  of  44,637.    Elapsed: 0:27:24. Training loss. 0.0032780496403574944 Num fake examples 40658 Num true examples 42782\n",
      "  Batch 41,760  of  44,637.    Elapsed: 0:27:26. Training loss. 0.0031396427657455206 Num fake examples 40707 Num true examples 42813\n",
      "  Batch 41,800  of  44,637.    Elapsed: 0:27:27. Training loss. 0.0027532712556421757 Num fake examples 40745 Num true examples 42855\n",
      "  Batch 41,840  of  44,637.    Elapsed: 0:27:29. Training loss. 0.0021828748285770416 Num fake examples 40781 Num true examples 42899\n",
      "  Batch 41,880  of  44,637.    Elapsed: 0:27:30. Training loss. 0.003549458459019661 Num fake examples 40817 Num true examples 42943\n",
      "  Batch 41,920  of  44,637.    Elapsed: 0:27:32. Training loss. 0.0035328222438693047 Num fake examples 40853 Num true examples 42987\n",
      "  Batch 41,960  of  44,637.    Elapsed: 0:27:33. Training loss. 0.0037586777471005917 Num fake examples 40900 Num true examples 43020\n",
      "  Batch 42,000  of  44,637.    Elapsed: 0:27:35. Training loss. 0.002618966158479452 Num fake examples 40944 Num true examples 43056\n",
      "  Batch 42,040  of  44,637.    Elapsed: 0:27:36. Training loss. 2.9401276111602783 Num fake examples 40983 Num true examples 43097\n",
      "  Batch 42,080  of  44,637.    Elapsed: 0:27:38. Training loss. 0.0028018015436828136 Num fake examples 41017 Num true examples 43143\n",
      "  Batch 42,120  of  44,637.    Elapsed: 0:27:39. Training loss. 0.0026540947146713734 Num fake examples 41056 Num true examples 43184\n",
      "  Batch 42,160  of  44,637.    Elapsed: 0:27:41. Training loss. 0.0038248873315751553 Num fake examples 41096 Num true examples 43224\n",
      "  Batch 42,200  of  44,637.    Elapsed: 0:27:42. Training loss. 0.0021321766544133425 Num fake examples 41134 Num true examples 43266\n",
      "  Batch 42,240  of  44,637.    Elapsed: 0:27:44. Training loss. 3.124854564666748 Num fake examples 41169 Num true examples 43311\n",
      "  Batch 42,280  of  44,637.    Elapsed: 0:27:45. Training loss. 0.003479551523923874 Num fake examples 41206 Num true examples 43354\n",
      "  Batch 42,320  of  44,637.    Elapsed: 0:27:47. Training loss. 0.0051405527628958225 Num fake examples 41239 Num true examples 43401\n",
      "  Batch 42,360  of  44,637.    Elapsed: 0:27:48. Training loss. 0.003590328386053443 Num fake examples 41283 Num true examples 43437\n",
      "  Batch 42,400  of  44,637.    Elapsed: 0:27:50. Training loss. 0.003404916264116764 Num fake examples 41321 Num true examples 43479\n",
      "  Batch 42,440  of  44,637.    Elapsed: 0:27:51. Training loss. 0.004305838607251644 Num fake examples 41357 Num true examples 43523\n",
      "  Batch 42,480  of  44,637.    Elapsed: 0:27:53. Training loss. 0.0035309731028974056 Num fake examples 41400 Num true examples 43560\n",
      "  Batch 42,520  of  44,637.    Elapsed: 0:27:54. Training loss. 0.004531715996563435 Num fake examples 41430 Num true examples 43610\n",
      "  Batch 42,560  of  44,637.    Elapsed: 0:27:55. Training loss. 0.0032346800435334444 Num fake examples 41465 Num true examples 43655\n",
      "  Batch 42,600  of  44,637.    Elapsed: 0:27:57. Training loss. 0.003473052056506276 Num fake examples 41510 Num true examples 43690\n",
      "  Batch 42,640  of  44,637.    Elapsed: 0:27:58. Training loss. 0.0028297959361225367 Num fake examples 41552 Num true examples 43728\n",
      "  Batch 42,680  of  44,637.    Elapsed: 0:28:00. Training loss. 0.0021844191942363977 Num fake examples 41588 Num true examples 43772\n",
      "  Batch 42,720  of  44,637.    Elapsed: 0:28:01. Training loss. 0.0019908626563847065 Num fake examples 41624 Num true examples 43816\n",
      "  Batch 42,760  of  44,637.    Elapsed: 0:28:03. Training loss. 0.0036873449571430683 Num fake examples 41658 Num true examples 43862\n",
      "  Batch 42,800  of  44,637.    Elapsed: 0:28:04. Training loss. 0.0030157845467329025 Num fake examples 41695 Num true examples 43905\n",
      "  Batch 42,840  of  44,637.    Elapsed: 0:28:06. Training loss. 0.0022117700427770615 Num fake examples 41738 Num true examples 43942\n",
      "  Batch 42,880  of  44,637.    Elapsed: 0:28:07. Training loss. 0.0026172525249421597 Num fake examples 41780 Num true examples 43980\n",
      "  Batch 42,920  of  44,637.    Elapsed: 0:28:09. Training loss. 0.0021246392279863358 Num fake examples 41821 Num true examples 44019\n",
      "  Batch 42,960  of  44,637.    Elapsed: 0:28:10. Training loss. 0.003613042179495096 Num fake examples 41853 Num true examples 44067\n",
      "  Batch 43,000  of  44,637.    Elapsed: 0:28:12. Training loss. 0.002972368849441409 Num fake examples 41892 Num true examples 44108\n",
      "  Batch 43,040  of  44,637.    Elapsed: 0:28:13. Training loss. 0.0028226079884916544 Num fake examples 41931 Num true examples 44149\n",
      "  Batch 43,080  of  44,637.    Elapsed: 0:28:15. Training loss. 0.004071248695254326 Num fake examples 41969 Num true examples 44191\n",
      "  Batch 43,120  of  44,637.    Elapsed: 0:28:16. Training loss. 0.004642189480364323 Num fake examples 42012 Num true examples 44228\n",
      "  Batch 43,160  of  44,637.    Elapsed: 0:28:18. Training loss. 0.001728658564388752 Num fake examples 42058 Num true examples 44262\n",
      "  Batch 43,200  of  44,637.    Elapsed: 0:28:19. Training loss. 0.001524456194601953 Num fake examples 42101 Num true examples 44299\n",
      "  Batch 43,240  of  44,637.    Elapsed: 0:28:21. Training loss. 0.005233346950262785 Num fake examples 42137 Num true examples 44343\n",
      "  Batch 43,280  of  44,637.    Elapsed: 0:28:22. Training loss. 0.002824735827744007 Num fake examples 42180 Num true examples 44380\n",
      "  Batch 43,320  of  44,637.    Elapsed: 0:28:24. Training loss. 0.003634190419688821 Num fake examples 42217 Num true examples 44423\n",
      "  Batch 43,360  of  44,637.    Elapsed: 0:28:25. Training loss. 0.002648549387231469 Num fake examples 42272 Num true examples 44448\n",
      "  Batch 43,400  of  44,637.    Elapsed: 0:28:26. Training loss. 0.0019186550052836537 Num fake examples 42308 Num true examples 44492\n",
      "  Batch 43,440  of  44,637.    Elapsed: 0:28:28. Training loss. 0.0023659211583435535 Num fake examples 42350 Num true examples 44530\n",
      "  Batch 43,480  of  44,637.    Elapsed: 0:28:29. Training loss. 3.382730722427368 Num fake examples 42385 Num true examples 44575\n",
      "  Batch 43,520  of  44,637.    Elapsed: 0:28:31. Training loss. 0.002716310787945986 Num fake examples 42419 Num true examples 44621\n",
      "  Batch 43,560  of  44,637.    Elapsed: 0:28:32. Training loss. 0.002388298511505127 Num fake examples 42464 Num true examples 44656\n",
      "  Batch 43,600  of  44,637.    Elapsed: 0:28:34. Training loss. 0.002561056986451149 Num fake examples 42500 Num true examples 44700\n",
      "  Batch 43,640  of  44,637.    Elapsed: 0:28:35. Training loss. 0.002241171197965741 Num fake examples 42532 Num true examples 44748\n",
      "  Batch 43,680  of  44,637.    Elapsed: 0:28:37. Training loss. 0.003199470229446888 Num fake examples 42570 Num true examples 44790\n",
      "  Batch 43,720  of  44,637.    Elapsed: 0:28:39. Training loss. 0.003845914965495467 Num fake examples 42616 Num true examples 44824\n",
      "  Batch 43,760  of  44,637.    Elapsed: 0:28:40. Training loss. 0.0027811808977276087 Num fake examples 42659 Num true examples 44861\n",
      "  Batch 43,800  of  44,637.    Elapsed: 0:28:42. Training loss. 0.0035258799325674772 Num fake examples 42694 Num true examples 44906\n",
      "  Batch 43,840  of  44,637.    Elapsed: 0:28:43. Training loss. 0.0018065350595861673 Num fake examples 42735 Num true examples 44945\n",
      "  Batch 43,880  of  44,637.    Elapsed: 0:28:45. Training loss. 0.0013117437483742833 Num fake examples 42772 Num true examples 44988\n",
      "  Batch 43,920  of  44,637.    Elapsed: 0:28:46. Training loss. 0.004724138416349888 Num fake examples 42814 Num true examples 45026\n",
      "  Batch 43,960  of  44,637.    Elapsed: 0:28:48. Training loss. 0.0013849504757672548 Num fake examples 42851 Num true examples 45069\n",
      "  Batch 44,000  of  44,637.    Elapsed: 0:28:49. Training loss. 0.003052952466532588 Num fake examples 42888 Num true examples 45112\n",
      "  Batch 44,040  of  44,637.    Elapsed: 0:28:51. Training loss. 0.0028799509163945913 Num fake examples 42930 Num true examples 45150\n",
      "  Batch 44,080  of  44,637.    Elapsed: 0:28:53. Training loss. 0.003131309524178505 Num fake examples 42960 Num true examples 45200\n",
      "  Batch 44,120  of  44,637.    Elapsed: 0:28:54. Training loss. 0.002718560863286257 Num fake examples 42996 Num true examples 45244\n",
      "  Batch 44,160  of  44,637.    Elapsed: 0:28:56. Training loss. 0.003142808098345995 Num fake examples 43032 Num true examples 45288\n",
      "  Batch 44,200  of  44,637.    Elapsed: 0:28:57. Training loss. 0.0023222710005939007 Num fake examples 43072 Num true examples 45328\n",
      "  Batch 44,240  of  44,637.    Elapsed: 0:28:59. Training loss. 3.0167031288146973 Num fake examples 43107 Num true examples 45373\n",
      "  Batch 44,280  of  44,637.    Elapsed: 0:29:00. Training loss. 0.0021484731696546078 Num fake examples 43133 Num true examples 45427\n",
      "  Batch 44,320  of  44,637.    Elapsed: 0:29:02. Training loss. 0.0025097753386944532 Num fake examples 43172 Num true examples 45468\n",
      "  Batch 44,360  of  44,637.    Elapsed: 0:29:04. Training loss. 0.0015171532286331058 Num fake examples 43207 Num true examples 45513\n",
      "  Batch 44,400  of  44,637.    Elapsed: 0:29:05. Training loss. 0.0020737498998641968 Num fake examples 43246 Num true examples 45554\n",
      "  Batch 44,440  of  44,637.    Elapsed: 0:29:07. Training loss. 0.0021821034606546164 Num fake examples 43290 Num true examples 45590\n",
      "  Batch 44,480  of  44,637.    Elapsed: 0:29:08. Training loss. 0.0025008306838572025 Num fake examples 43325 Num true examples 45635\n",
      "  Batch 44,520  of  44,637.    Elapsed: 0:29:10. Training loss. 0.002049666829407215 Num fake examples 43362 Num true examples 45678\n",
      "  Batch 44,560  of  44,637.    Elapsed: 0:29:11. Training loss. 3.3198583126068115 Num fake examples 43413 Num true examples 45707\n",
      "  Batch 44,600  of  44,637.    Elapsed: 0:29:13. Training loss. 0.0022435944993048906 Num fake examples 43447 Num true examples 45753\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epcoh took: 0:29:15\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation Loss: 0.19\n",
      "  Validation took: 0:01:47\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of  44,637.    Elapsed: 0:00:02. Training loss. 0.0022184066474437714 Num fake examples 44 Num true examples 36\n",
      "  Batch    80  of  44,637.    Elapsed: 0:00:03. Training loss. 0.0028724977746605873 Num fake examples 81 Num true examples 79\n",
      "  Batch   120  of  44,637.    Elapsed: 0:00:04. Training loss. 0.0021839202381670475 Num fake examples 113 Num true examples 127\n",
      "  Batch   160  of  44,637.    Elapsed: 0:00:06. Training loss. 0.0032433061860501766 Num fake examples 151 Num true examples 169\n",
      "  Batch   200  of  44,637.    Elapsed: 0:00:07. Training loss. 0.0027549220249056816 Num fake examples 182 Num true examples 218\n",
      "  Batch   240  of  44,637.    Elapsed: 0:00:09. Training loss. 0.003247195156291127 Num fake examples 213 Num true examples 267\n",
      "  Batch   280  of  44,637.    Elapsed: 0:00:10. Training loss. 0.0027576121501624584 Num fake examples 245 Num true examples 315\n",
      "  Batch   320  of  44,637.    Elapsed: 0:00:12. Training loss. 0.002058311365544796 Num fake examples 274 Num true examples 366\n",
      "  Batch   360  of  44,637.    Elapsed: 0:00:13. Training loss. 0.0013207249576225877 Num fake examples 310 Num true examples 410\n",
      "  Batch   400  of  44,637.    Elapsed: 0:00:15. Training loss. 0.0012268747668713331 Num fake examples 346 Num true examples 454\n",
      "  Batch   440  of  44,637.    Elapsed: 0:00:16. Training loss. 0.0036152428947389126 Num fake examples 377 Num true examples 503\n",
      "  Batch   480  of  44,637.    Elapsed: 0:00:18. Training loss. 0.002899632789194584 Num fake examples 419 Num true examples 541\n",
      "  Batch   520  of  44,637.    Elapsed: 0:00:19. Training loss. 0.0014688740484416485 Num fake examples 459 Num true examples 581\n",
      "  Batch   560  of  44,637.    Elapsed: 0:00:21. Training loss. 0.0016226618317887187 Num fake examples 498 Num true examples 622\n",
      "  Batch   600  of  44,637.    Elapsed: 0:00:22. Training loss. 0.002074143849313259 Num fake examples 533 Num true examples 667\n",
      "  Batch   640  of  44,637.    Elapsed: 0:00:24. Training loss. 0.0023159258998930454 Num fake examples 575 Num true examples 705\n",
      "  Batch   680  of  44,637.    Elapsed: 0:00:25. Training loss. 0.002760272938758135 Num fake examples 614 Num true examples 746\n",
      "  Batch   720  of  44,637.    Elapsed: 0:00:27. Training loss. 2.886826515197754 Num fake examples 650 Num true examples 790\n",
      "  Batch   760  of  44,637.    Elapsed: 0:00:28. Training loss. 0.0035399338230490685 Num fake examples 682 Num true examples 838\n",
      "  Batch   800  of  44,637.    Elapsed: 0:00:30. Training loss. 0.0027760483790189028 Num fake examples 725 Num true examples 875\n",
      "  Batch   840  of  44,637.    Elapsed: 0:00:31. Training loss. 0.00431935815140605 Num fake examples 761 Num true examples 919\n",
      "  Batch   880  of  44,637.    Elapsed: 0:00:32. Training loss. 0.0029292916879057884 Num fake examples 793 Num true examples 967\n",
      "  Batch   920  of  44,637.    Elapsed: 0:00:34. Training loss. 0.0036221195477992296 Num fake examples 831 Num true examples 1009\n",
      "  Batch   960  of  44,637.    Elapsed: 0:00:35. Training loss. 0.004001344554126263 Num fake examples 865 Num true examples 1055\n",
      "  Batch 1,000  of  44,637.    Elapsed: 0:00:37. Training loss. 0.003379060421139002 Num fake examples 906 Num true examples 1094\n",
      "  Batch 1,040  of  44,637.    Elapsed: 0:00:38. Training loss. 0.003373995190486312 Num fake examples 941 Num true examples 1139\n",
      "  Batch 1,080  of  44,637.    Elapsed: 0:00:40. Training loss. 0.00331238005310297 Num fake examples 981 Num true examples 1179\n",
      "  Batch 1,120  of  44,637.    Elapsed: 0:00:41. Training loss. 0.002033956814557314 Num fake examples 1017 Num true examples 1223\n",
      "  Batch 1,160  of  44,637.    Elapsed: 0:00:43. Training loss. 0.0029915457125753164 Num fake examples 1052 Num true examples 1268\n",
      "  Batch 1,200  of  44,637.    Elapsed: 0:00:44. Training loss. 0.0041907187551259995 Num fake examples 1095 Num true examples 1305\n",
      "  Batch 1,240  of  44,637.    Elapsed: 0:00:46. Training loss. 0.0060214689001441 Num fake examples 1130 Num true examples 1350\n",
      "  Batch 1,280  of  44,637.    Elapsed: 0:00:47. Training loss. 0.003227441804483533 Num fake examples 1171 Num true examples 1389\n",
      "  Batch 1,320  of  44,637.    Elapsed: 0:00:49. Training loss. 0.002229556906968355 Num fake examples 1204 Num true examples 1436\n",
      "  Batch 1,360  of  44,637.    Elapsed: 0:00:50. Training loss. 0.0029886383563280106 Num fake examples 1247 Num true examples 1473\n",
      "  Batch 1,400  of  44,637.    Elapsed: 0:00:52. Training loss. 0.002332202857360244 Num fake examples 1287 Num true examples 1513\n",
      "  Batch 1,440  of  44,637.    Elapsed: 0:00:53. Training loss. 0.003609889652580023 Num fake examples 1328 Num true examples 1552\n",
      "  Batch 1,480  of  44,637.    Elapsed: 0:00:55. Training loss. 0.003831230103969574 Num fake examples 1372 Num true examples 1588\n",
      "  Batch 1,520  of  44,637.    Elapsed: 0:00:56. Training loss. 0.0025714286603033543 Num fake examples 1406 Num true examples 1634\n",
      "  Batch 1,560  of  44,637.    Elapsed: 0:00:58. Training loss. 0.002858915366232395 Num fake examples 1439 Num true examples 1681\n",
      "  Batch 1,600  of  44,637.    Elapsed: 0:00:59. Training loss. 0.0018278701463714242 Num fake examples 1478 Num true examples 1722\n",
      "  Batch 1,640  of  44,637.    Elapsed: 0:01:01. Training loss. 0.001710585318505764 Num fake examples 1507 Num true examples 1773\n",
      "  Batch 1,680  of  44,637.    Elapsed: 0:01:02. Training loss. 3.126922607421875 Num fake examples 1543 Num true examples 1817\n",
      "  Batch 1,720  of  44,637.    Elapsed: 0:01:04. Training loss. 0.0020037703216075897 Num fake examples 1581 Num true examples 1859\n",
      "  Batch 1,760  of  44,637.    Elapsed: 0:01:05. Training loss. 0.004712814465165138 Num fake examples 1615 Num true examples 1905\n",
      "  Batch 1,800  of  44,637.    Elapsed: 0:01:06. Training loss. 0.0034309292677789927 Num fake examples 1653 Num true examples 1947\n",
      "  Batch 1,840  of  44,637.    Elapsed: 0:01:08. Training loss. 0.0037020305171608925 Num fake examples 1688 Num true examples 1992\n",
      "  Batch 1,880  of  44,637.    Elapsed: 0:01:09. Training loss. 0.002271596109494567 Num fake examples 1733 Num true examples 2027\n",
      "  Batch 1,920  of  44,637.    Elapsed: 0:01:11. Training loss. 0.0025347592309117317 Num fake examples 1772 Num true examples 2068\n",
      "  Batch 1,960  of  44,637.    Elapsed: 0:01:12. Training loss. 0.0028167148120701313 Num fake examples 1812 Num true examples 2108\n",
      "  Batch 2,000  of  44,637.    Elapsed: 0:01:14. Training loss. 0.00307395588606596 Num fake examples 1854 Num true examples 2146\n",
      "  Batch 2,040  of  44,637.    Elapsed: 0:01:15. Training loss. 0.0029631746001541615 Num fake examples 1894 Num true examples 2186\n",
      "  Batch 2,080  of  44,637.    Elapsed: 0:01:17. Training loss. 0.0017670111265033484 Num fake examples 1936 Num true examples 2224\n",
      "  Batch 2,120  of  44,637.    Elapsed: 0:01:18. Training loss. 3.1599302291870117 Num fake examples 1977 Num true examples 2263\n",
      "  Batch 2,160  of  44,637.    Elapsed: 0:01:20. Training loss. 0.0031267409212887287 Num fake examples 2024 Num true examples 2296\n",
      "  Batch 2,200  of  44,637.    Elapsed: 0:01:21. Training loss. 2.871201753616333 Num fake examples 2058 Num true examples 2342\n",
      "  Batch 2,240  of  44,637.    Elapsed: 0:01:23. Training loss. 0.0039892541244626045 Num fake examples 2089 Num true examples 2391\n",
      "  Batch 2,280  of  44,637.    Elapsed: 0:01:24. Training loss. 0.002711435779929161 Num fake examples 2123 Num true examples 2437\n",
      "  Batch 2,320  of  44,637.    Elapsed: 0:01:26. Training loss. 0.0025547838304191828 Num fake examples 2158 Num true examples 2482\n",
      "  Batch 2,360  of  44,637.    Elapsed: 0:01:27. Training loss. 0.0029039636719971895 Num fake examples 2201 Num true examples 2519\n",
      "  Batch 2,400  of  44,637.    Elapsed: 0:01:29. Training loss. 0.00482537504285574 Num fake examples 2243 Num true examples 2557\n",
      "  Batch 2,440  of  44,637.    Elapsed: 0:01:30. Training loss. 0.002412636298686266 Num fake examples 2295 Num true examples 2585\n",
      "  Batch 2,480  of  44,637.    Elapsed: 0:01:32. Training loss. 0.006018331740051508 Num fake examples 2330 Num true examples 2630\n",
      "  Batch 2,520  of  44,637.    Elapsed: 0:01:33. Training loss. 0.0030021306592971087 Num fake examples 2374 Num true examples 2666\n",
      "  Batch 2,560  of  44,637.    Elapsed: 0:01:35. Training loss. 0.003995632752776146 Num fake examples 2413 Num true examples 2707\n",
      "  Batch 2,600  of  44,637.    Elapsed: 0:01:36. Training loss. 0.004785110708326101 Num fake examples 2456 Num true examples 2744\n",
      "  Batch 2,640  of  44,637.    Elapsed: 0:01:38. Training loss. 0.003428083611652255 Num fake examples 2492 Num true examples 2788\n",
      "  Batch 2,680  of  44,637.    Elapsed: 0:01:39. Training loss. 0.004002508707344532 Num fake examples 2531 Num true examples 2829\n",
      "  Batch 2,720  of  44,637.    Elapsed: 0:01:41. Training loss. 0.0016509286360815167 Num fake examples 2571 Num true examples 2869\n",
      "  Batch 2,760  of  44,637.    Elapsed: 0:01:42. Training loss. 0.0038598566316068172 Num fake examples 2611 Num true examples 2909\n",
      "  Batch 2,800  of  44,637.    Elapsed: 0:01:43. Training loss. 0.00314881419762969 Num fake examples 2658 Num true examples 2942\n",
      "  Batch 2,840  of  44,637.    Elapsed: 0:01:45. Training loss. 0.005306140519678593 Num fake examples 2703 Num true examples 2977\n",
      "  Batch 2,880  of  44,637.    Elapsed: 0:01:46. Training loss. 0.0040037487633526325 Num fake examples 2745 Num true examples 3015\n",
      "  Batch 2,920  of  44,637.    Elapsed: 0:01:48. Training loss. 0.0025952905416488647 Num fake examples 2783 Num true examples 3057\n",
      "  Batch 2,960  of  44,637.    Elapsed: 0:01:49. Training loss. 0.0017441677628085017 Num fake examples 2818 Num true examples 3102\n",
      "  Batch 3,000  of  44,637.    Elapsed: 0:01:51. Training loss. 0.0032987231388688087 Num fake examples 2854 Num true examples 3146\n",
      "  Batch 3,040  of  44,637.    Elapsed: 0:01:52. Training loss. 0.003447673050686717 Num fake examples 2893 Num true examples 3187\n",
      "  Batch 3,080  of  44,637.    Elapsed: 0:01:54. Training loss. 0.0032621673308312893 Num fake examples 2927 Num true examples 3233\n",
      "  Batch 3,120  of  44,637.    Elapsed: 0:01:55. Training loss. 0.003202103078365326 Num fake examples 2968 Num true examples 3272\n",
      "  Batch 3,160  of  44,637.    Elapsed: 0:01:57. Training loss. 0.0020405887626111507 Num fake examples 3007 Num true examples 3313\n",
      "  Batch 3,200  of  44,637.    Elapsed: 0:01:58. Training loss. 0.0029708484653383493 Num fake examples 3053 Num true examples 3347\n",
      "  Batch 3,240  of  44,637.    Elapsed: 0:02:00. Training loss. 0.004543691873550415 Num fake examples 3090 Num true examples 3390\n",
      "  Batch 3,280  of  44,637.    Elapsed: 0:02:01. Training loss. 0.0041863140650093555 Num fake examples 3115 Num true examples 3445\n",
      "  Batch 3,320  of  44,637.    Elapsed: 0:02:03. Training loss. 0.0022220471873879433 Num fake examples 3155 Num true examples 3485\n",
      "  Batch 3,360  of  44,637.    Elapsed: 0:02:04. Training loss. 0.0034487962257117033 Num fake examples 3196 Num true examples 3524\n",
      "  Batch 3,400  of  44,637.    Elapsed: 0:02:06. Training loss. 0.0029516927897930145 Num fake examples 3233 Num true examples 3567\n",
      "  Batch 3,440  of  44,637.    Elapsed: 0:02:07. Training loss. 0.0025495390873402357 Num fake examples 3273 Num true examples 3607\n",
      "  Batch 3,480  of  44,637.    Elapsed: 0:02:09. Training loss. 0.0014993064105510712 Num fake examples 3308 Num true examples 3652\n",
      "  Batch 3,520  of  44,637.    Elapsed: 0:02:10. Training loss. 0.0023758055176585913 Num fake examples 3350 Num true examples 3690\n",
      "  Batch 3,560  of  44,637.    Elapsed: 0:02:12. Training loss. 0.0027639667969197035 Num fake examples 3388 Num true examples 3732\n",
      "  Batch 3,600  of  44,637.    Elapsed: 0:02:13. Training loss. 0.0026910670567303896 Num fake examples 3432 Num true examples 3768\n",
      "  Batch 3,640  of  44,637.    Elapsed: 0:02:15. Training loss. 0.00395219586789608 Num fake examples 3469 Num true examples 3811\n",
      "  Batch 3,680  of  44,637.    Elapsed: 0:02:16. Training loss. 0.0030995807610452175 Num fake examples 3511 Num true examples 3849\n",
      "  Batch 3,720  of  44,637.    Elapsed: 0:02:18. Training loss. 0.004824471659958363 Num fake examples 3546 Num true examples 3894\n",
      "  Batch 3,760  of  44,637.    Elapsed: 0:02:19. Training loss. 0.0035194717347621918 Num fake examples 3587 Num true examples 3933\n",
      "  Batch 3,800  of  44,637.    Elapsed: 0:02:21. Training loss. 0.003922787960618734 Num fake examples 3622 Num true examples 3978\n",
      "  Batch 3,840  of  44,637.    Elapsed: 0:02:22. Training loss. 0.003668998135253787 Num fake examples 3658 Num true examples 4022\n",
      "  Batch 3,880  of  44,637.    Elapsed: 0:02:24. Training loss. 0.0025444156490266323 Num fake examples 3707 Num true examples 4053\n",
      "  Batch 3,920  of  44,637.    Elapsed: 0:02:25. Training loss. 0.0034410408698022366 Num fake examples 3746 Num true examples 4094\n",
      "  Batch 3,960  of  44,637.    Elapsed: 0:02:27. Training loss. 0.004506615921854973 Num fake examples 3782 Num true examples 4138\n",
      "  Batch 4,000  of  44,637.    Elapsed: 0:02:28. Training loss. 2.8159403800964355 Num fake examples 3823 Num true examples 4177\n",
      "  Batch 4,040  of  44,637.    Elapsed: 0:02:30. Training loss. 0.004546937998384237 Num fake examples 3864 Num true examples 4216\n",
      "  Batch 4,080  of  44,637.    Elapsed: 0:02:31. Training loss. 0.004684805404394865 Num fake examples 3898 Num true examples 4262\n",
      "  Batch 4,120  of  44,637.    Elapsed: 0:02:33. Training loss. 2.5103085041046143 Num fake examples 3935 Num true examples 4305\n",
      "  Batch 4,160  of  44,637.    Elapsed: 0:02:34. Training loss. 0.004929428920149803 Num fake examples 3969 Num true examples 4351\n",
      "  Batch 4,200  of  44,637.    Elapsed: 0:02:36. Training loss. 0.00791808683425188 Num fake examples 4006 Num true examples 4394\n",
      "  Batch 4,240  of  44,637.    Elapsed: 0:02:37. Training loss. 0.004560508765280247 Num fake examples 4049 Num true examples 4431\n",
      "  Batch 4,280  of  44,637.    Elapsed: 0:02:39. Training loss. 0.0035468055866658688 Num fake examples 4087 Num true examples 4473\n",
      "  Batch 4,320  of  44,637.    Elapsed: 0:02:40. Training loss. 0.003127261996269226 Num fake examples 4133 Num true examples 4507\n",
      "  Batch 4,360  of  44,637.    Elapsed: 0:02:42. Training loss. 0.002339015481993556 Num fake examples 4174 Num true examples 4546\n",
      "  Batch 4,400  of  44,637.    Elapsed: 0:02:43. Training loss. 0.0038521497044712305 Num fake examples 4207 Num true examples 4593\n",
      "  Batch 4,440  of  44,637.    Elapsed: 0:02:45. Training loss. 0.0039514959789812565 Num fake examples 4248 Num true examples 4632\n",
      "  Batch 4,480  of  44,637.    Elapsed: 0:02:46. Training loss. 0.0033954563550651073 Num fake examples 4294 Num true examples 4666\n",
      "  Batch 4,520  of  44,637.    Elapsed: 0:02:47. Training loss. 0.002695652889087796 Num fake examples 4335 Num true examples 4705\n",
      "  Batch 4,560  of  44,637.    Elapsed: 0:02:49. Training loss. 3.2332584857940674 Num fake examples 4372 Num true examples 4748\n",
      "  Batch 4,600  of  44,637.    Elapsed: 0:02:50. Training loss. 0.002854852005839348 Num fake examples 4423 Num true examples 4777\n",
      "  Batch 4,640  of  44,637.    Elapsed: 0:02:52. Training loss. 0.002299717627465725 Num fake examples 4470 Num true examples 4810\n",
      "  Batch 4,680  of  44,637.    Elapsed: 0:02:53. Training loss. 0.0031554282177239656 Num fake examples 4505 Num true examples 4855\n",
      "  Batch 4,720  of  44,637.    Elapsed: 0:02:55. Training loss. 0.003631029510870576 Num fake examples 4543 Num true examples 4897\n",
      "  Batch 4,760  of  44,637.    Elapsed: 0:02:56. Training loss. 0.0033582826144993305 Num fake examples 4579 Num true examples 4941\n",
      "  Batch 4,800  of  44,637.    Elapsed: 0:02:58. Training loss. 0.00303458608686924 Num fake examples 4614 Num true examples 4986\n",
      "  Batch 4,840  of  44,637.    Elapsed: 0:02:59. Training loss. 0.005268662236630917 Num fake examples 4658 Num true examples 5022\n",
      "  Batch 4,880  of  44,637.    Elapsed: 0:03:01. Training loss. 0.00532111618667841 Num fake examples 4703 Num true examples 5057\n",
      "  Batch 4,920  of  44,637.    Elapsed: 0:03:02. Training loss. 0.004243386443704367 Num fake examples 4740 Num true examples 5100\n",
      "  Batch 4,960  of  44,637.    Elapsed: 0:03:04. Training loss. 0.005425932351499796 Num fake examples 4791 Num true examples 5129\n",
      "  Batch 5,000  of  44,637.    Elapsed: 0:03:05. Training loss. 0.004590035416185856 Num fake examples 4828 Num true examples 5172\n",
      "  Batch 5,040  of  44,637.    Elapsed: 0:03:07. Training loss. 0.00482605816796422 Num fake examples 4870 Num true examples 5210\n",
      "  Batch 5,080  of  44,637.    Elapsed: 0:03:08. Training loss. 0.0030861643608659506 Num fake examples 4909 Num true examples 5251\n",
      "  Batch 5,120  of  44,637.    Elapsed: 0:03:10. Training loss. 0.0031645353883504868 Num fake examples 4950 Num true examples 5290\n",
      "  Batch 5,160  of  44,637.    Elapsed: 0:03:11. Training loss. 0.002614542841911316 Num fake examples 4984 Num true examples 5336\n",
      "  Batch 5,200  of  44,637.    Elapsed: 0:03:13. Training loss. 0.00295151025056839 Num fake examples 5028 Num true examples 5372\n",
      "  Batch 5,240  of  44,637.    Elapsed: 0:03:14. Training loss. 0.0036178012378513813 Num fake examples 5068 Num true examples 5412\n",
      "  Batch 5,280  of  44,637.    Elapsed: 0:03:16. Training loss. 0.0032324260100722313 Num fake examples 5105 Num true examples 5455\n",
      "  Batch 5,320  of  44,637.    Elapsed: 0:03:17. Training loss. 0.00212666904553771 Num fake examples 5142 Num true examples 5498\n",
      "  Batch 5,360  of  44,637.    Elapsed: 0:03:19. Training loss. 0.003453231183812022 Num fake examples 5182 Num true examples 5538\n",
      "  Batch 5,400  of  44,637.    Elapsed: 0:03:20. Training loss. 0.0037056622095406055 Num fake examples 5222 Num true examples 5578\n",
      "  Batch 5,440  of  44,637.    Elapsed: 0:03:22. Training loss. 0.0039054385852068663 Num fake examples 5259 Num true examples 5621\n",
      "  Batch 5,480  of  44,637.    Elapsed: 0:03:23. Training loss. 0.004026415757834911 Num fake examples 5305 Num true examples 5655\n",
      "  Batch 5,520  of  44,637.    Elapsed: 0:03:25. Training loss. 0.004963379353284836 Num fake examples 5341 Num true examples 5699\n",
      "  Batch 5,560  of  44,637.    Elapsed: 0:03:26. Training loss. 0.004729928448796272 Num fake examples 5379 Num true examples 5741\n",
      "  Batch 5,600  of  44,637.    Elapsed: 0:03:27. Training loss. 0.004912275820970535 Num fake examples 5414 Num true examples 5786\n",
      "  Batch 5,640  of  44,637.    Elapsed: 0:03:29. Training loss. 0.0034708271268755198 Num fake examples 5453 Num true examples 5827\n",
      "  Batch 5,680  of  44,637.    Elapsed: 0:03:30. Training loss. 0.0031343225855380297 Num fake examples 5494 Num true examples 5866\n",
      "  Batch 5,720  of  44,637.    Elapsed: 0:03:32. Training loss. 0.0031231259927153587 Num fake examples 5525 Num true examples 5915\n",
      "  Batch 5,760  of  44,637.    Elapsed: 0:03:33. Training loss. 0.0023307811934500933 Num fake examples 5570 Num true examples 5950\n",
      "  Batch 5,800  of  44,637.    Elapsed: 0:03:35. Training loss. 0.003544062841683626 Num fake examples 5609 Num true examples 5991\n",
      "  Batch 5,840  of  44,637.    Elapsed: 0:03:36. Training loss. 0.0028582154773175716 Num fake examples 5643 Num true examples 6037\n",
      "  Batch 5,880  of  44,637.    Elapsed: 0:03:38. Training loss. 0.002740531461313367 Num fake examples 5677 Num true examples 6083\n",
      "  Batch 5,920  of  44,637.    Elapsed: 0:03:39. Training loss. 0.004106455948203802 Num fake examples 5717 Num true examples 6123\n",
      "  Batch 5,960  of  44,637.    Elapsed: 0:03:41. Training loss. 0.002306692535057664 Num fake examples 5754 Num true examples 6166\n",
      "  Batch 6,000  of  44,637.    Elapsed: 0:03:42. Training loss. 0.0029096733778715134 Num fake examples 5798 Num true examples 6202\n",
      "  Batch 6,040  of  44,637.    Elapsed: 0:03:44. Training loss. 0.003096657572314143 Num fake examples 5830 Num true examples 6250\n",
      "  Batch 6,080  of  44,637.    Elapsed: 0:03:45. Training loss. 0.0026817729230970144 Num fake examples 5868 Num true examples 6292\n",
      "  Batch 6,120  of  44,637.    Elapsed: 0:03:47. Training loss. 0.0024319649673998356 Num fake examples 5904 Num true examples 6336\n",
      "  Batch 6,160  of  44,637.    Elapsed: 0:03:48. Training loss. 0.003649882273748517 Num fake examples 5937 Num true examples 6383\n",
      "  Batch 6,200  of  44,637.    Elapsed: 0:03:50. Training loss. 3.1743414402008057 Num fake examples 5978 Num true examples 6422\n",
      "  Batch 6,240  of  44,637.    Elapsed: 0:03:51. Training loss. 0.003310044761747122 Num fake examples 6011 Num true examples 6469\n",
      "  Batch 6,280  of  44,637.    Elapsed: 0:03:53. Training loss. 0.003829320427030325 Num fake examples 6050 Num true examples 6510\n",
      "  Batch 6,320  of  44,637.    Elapsed: 0:03:54. Training loss. 0.005230155307799578 Num fake examples 6084 Num true examples 6556\n",
      "  Batch 6,360  of  44,637.    Elapsed: 0:03:56. Training loss. 0.0030107414349913597 Num fake examples 6125 Num true examples 6595\n",
      "  Batch 6,400  of  44,637.    Elapsed: 0:03:57. Training loss. 0.003973240032792091 Num fake examples 6166 Num true examples 6634\n",
      "  Batch 6,440  of  44,637.    Elapsed: 0:03:58. Training loss. 0.0042176018469035625 Num fake examples 6201 Num true examples 6679\n",
      "  Batch 6,480  of  44,637.    Elapsed: 0:04:00. Training loss. 0.002620869316160679 Num fake examples 6239 Num true examples 6721\n",
      "  Batch 6,520  of  44,637.    Elapsed: 0:04:01. Training loss. 0.0024912962689995766 Num fake examples 6274 Num true examples 6766\n",
      "  Batch 6,560  of  44,637.    Elapsed: 0:04:03. Training loss. 0.0022670947946608067 Num fake examples 6310 Num true examples 6810\n",
      "  Batch 6,600  of  44,637.    Elapsed: 0:04:04. Training loss. 0.0016073654405772686 Num fake examples 6344 Num true examples 6856\n",
      "  Batch 6,640  of  44,637.    Elapsed: 0:04:06. Training loss. 0.0031276955269277096 Num fake examples 6384 Num true examples 6896\n",
      "  Batch 6,680  of  44,637.    Elapsed: 0:04:07. Training loss. 0.0028297891840338707 Num fake examples 6423 Num true examples 6937\n",
      "  Batch 6,720  of  44,637.    Elapsed: 0:04:09. Training loss. 0.001991978846490383 Num fake examples 6456 Num true examples 6984\n",
      "  Batch 6,760  of  44,637.    Elapsed: 0:04:10. Training loss. 0.003701801411807537 Num fake examples 6494 Num true examples 7026\n",
      "  Batch 6,800  of  44,637.    Elapsed: 0:04:12. Training loss. 0.0032107364386320114 Num fake examples 6536 Num true examples 7064\n",
      "  Batch 6,840  of  44,637.    Elapsed: 0:04:13. Training loss. 0.0024843751452863216 Num fake examples 6587 Num true examples 7093\n",
      "  Batch 6,880  of  44,637.    Elapsed: 0:04:15. Training loss. 0.0022777635604143143 Num fake examples 6632 Num true examples 7128\n",
      "  Batch 6,920  of  44,637.    Elapsed: 0:04:16. Training loss. 0.0027119414880871773 Num fake examples 6669 Num true examples 7171\n",
      "  Batch 6,960  of  44,637.    Elapsed: 0:04:18. Training loss. 0.003646356053650379 Num fake examples 6712 Num true examples 7208\n",
      "  Batch 7,000  of  44,637.    Elapsed: 0:04:19. Training loss. 0.003356565721333027 Num fake examples 6750 Num true examples 7250\n",
      "  Batch 7,040  of  44,637.    Elapsed: 0:04:21. Training loss. 0.0029612190555781126 Num fake examples 6795 Num true examples 7285\n",
      "  Batch 7,080  of  44,637.    Elapsed: 0:04:22. Training loss. 0.0018977528670802712 Num fake examples 6825 Num true examples 7335\n",
      "  Batch 7,120  of  44,637.    Elapsed: 0:04:24. Training loss. 0.0016450851690024137 Num fake examples 6864 Num true examples 7376\n",
      "  Batch 7,160  of  44,637.    Elapsed: 0:04:25. Training loss. 0.002672296017408371 Num fake examples 6906 Num true examples 7414\n",
      "  Batch 7,200  of  44,637.    Elapsed: 0:04:27. Training loss. 0.0029398640617728233 Num fake examples 6943 Num true examples 7457\n",
      "  Batch 7,240  of  44,637.    Elapsed: 0:04:28. Training loss. 0.0021975627169013023 Num fake examples 6983 Num true examples 7497\n",
      "  Batch 7,280  of  44,637.    Elapsed: 0:04:29. Training loss. 0.0015568914823234081 Num fake examples 7028 Num true examples 7532\n",
      "  Batch 7,320  of  44,637.    Elapsed: 0:04:31. Training loss. 2.8860626220703125 Num fake examples 7069 Num true examples 7571\n",
      "  Batch 7,360  of  44,637.    Elapsed: 0:04:32. Training loss. 0.003086662385612726 Num fake examples 7110 Num true examples 7610\n",
      "  Batch 7,400  of  44,637.    Elapsed: 0:04:34. Training loss. 0.002305092290043831 Num fake examples 7148 Num true examples 7652\n",
      "  Batch 7,440  of  44,637.    Elapsed: 0:04:35. Training loss. 0.002025723922997713 Num fake examples 7187 Num true examples 7693\n",
      "  Batch 7,480  of  44,637.    Elapsed: 0:04:37. Training loss. 0.002474233042448759 Num fake examples 7235 Num true examples 7725\n",
      "  Batch 7,520  of  44,637.    Elapsed: 0:04:38. Training loss. 0.0022677243687212467 Num fake examples 7276 Num true examples 7764\n",
      "  Batch 7,560  of  44,637.    Elapsed: 0:04:40. Training loss. 0.002782663330435753 Num fake examples 7308 Num true examples 7812\n",
      "  Batch 7,600  of  44,637.    Elapsed: 0:04:41. Training loss. 0.0026262514293193817 Num fake examples 7346 Num true examples 7854\n",
      "  Batch 7,640  of  44,637.    Elapsed: 0:04:43. Training loss. 0.002270979341119528 Num fake examples 7388 Num true examples 7892\n",
      "  Batch 7,680  of  44,637.    Elapsed: 0:04:44. Training loss. 0.0021817663218826056 Num fake examples 7429 Num true examples 7931\n",
      "  Batch 7,720  of  44,637.    Elapsed: 0:04:46. Training loss. 0.0025967503897845745 Num fake examples 7464 Num true examples 7976\n",
      "  Batch 7,760  of  44,637.    Elapsed: 0:04:47. Training loss. 0.003212985349819064 Num fake examples 7496 Num true examples 8024\n",
      "  Batch 7,800  of  44,637.    Elapsed: 0:04:49. Training loss. 0.003692308906465769 Num fake examples 7534 Num true examples 8066\n",
      "  Batch 7,840  of  44,637.    Elapsed: 0:04:50. Training loss. 0.0024692569859325886 Num fake examples 7579 Num true examples 8101\n",
      "  Batch 7,880  of  44,637.    Elapsed: 0:04:52. Training loss. 0.0028563393279910088 Num fake examples 7618 Num true examples 8142\n",
      "  Batch 7,920  of  44,637.    Elapsed: 0:04:53. Training loss. 0.005681379698216915 Num fake examples 7661 Num true examples 8179\n",
      "  Batch 7,960  of  44,637.    Elapsed: 0:04:55. Training loss. 0.00322550511918962 Num fake examples 7706 Num true examples 8214\n",
      "  Batch 8,000  of  44,637.    Elapsed: 0:04:56. Training loss. 0.004254017025232315 Num fake examples 7748 Num true examples 8252\n",
      "  Batch 8,040  of  44,637.    Elapsed: 0:04:57. Training loss. 0.0036825588904321194 Num fake examples 7791 Num true examples 8289\n",
      "  Batch 8,080  of  44,637.    Elapsed: 0:04:59. Training loss. 0.003676412161439657 Num fake examples 7830 Num true examples 8330\n",
      "  Batch 8,120  of  44,637.    Elapsed: 0:05:00. Training loss. 3.0852086544036865 Num fake examples 7879 Num true examples 8361\n",
      "  Batch 8,160  of  44,637.    Elapsed: 0:05:02. Training loss. 0.0059307049959897995 Num fake examples 7915 Num true examples 8405\n",
      "  Batch 8,200  of  44,637.    Elapsed: 0:05:03. Training loss. 0.007249405607581139 Num fake examples 7959 Num true examples 8441\n",
      "  Batch 8,240  of  44,637.    Elapsed: 0:05:05. Training loss. 0.0023858663626015186 Num fake examples 8001 Num true examples 8479\n",
      "  Batch 8,280  of  44,637.    Elapsed: 0:05:06. Training loss. 0.0036534348037093878 Num fake examples 8046 Num true examples 8514\n",
      "  Batch 8,320  of  44,637.    Elapsed: 0:05:08. Training loss. 0.00144257303327322 Num fake examples 8081 Num true examples 8559\n",
      "  Batch 8,360  of  44,637.    Elapsed: 0:05:09. Training loss. 0.003826170228421688 Num fake examples 8117 Num true examples 8603\n",
      "  Batch 8,400  of  44,637.    Elapsed: 0:05:11. Training loss. 0.002063797554001212 Num fake examples 8158 Num true examples 8642\n",
      "  Batch 8,440  of  44,637.    Elapsed: 0:05:12. Training loss. 0.00304884510114789 Num fake examples 8200 Num true examples 8680\n",
      "  Batch 8,480  of  44,637.    Elapsed: 0:05:14. Training loss. 0.002309062285348773 Num fake examples 8237 Num true examples 8723\n",
      "  Batch 8,520  of  44,637.    Elapsed: 0:05:15. Training loss. 0.0024936068803071976 Num fake examples 8275 Num true examples 8765\n",
      "  Batch 8,560  of  44,637.    Elapsed: 0:05:17. Training loss. 0.0023495459463447332 Num fake examples 8312 Num true examples 8808\n",
      "  Batch 8,600  of  44,637.    Elapsed: 0:05:18. Training loss. 0.0020332918502390385 Num fake examples 8354 Num true examples 8846\n",
      "  Batch 8,640  of  44,637.    Elapsed: 0:05:20. Training loss. 0.003347797552123666 Num fake examples 8392 Num true examples 8888\n",
      "  Batch 8,680  of  44,637.    Elapsed: 0:05:21. Training loss. 0.0019518082262948155 Num fake examples 8438 Num true examples 8922\n",
      "  Batch 8,720  of  44,637.    Elapsed: 0:05:23. Training loss. 0.0019399586599320173 Num fake examples 8481 Num true examples 8959\n",
      "  Batch 8,760  of  44,637.    Elapsed: 0:05:24. Training loss. 0.0017831521108746529 Num fake examples 8514 Num true examples 9006\n",
      "  Batch 8,800  of  44,637.    Elapsed: 0:05:26. Training loss. 0.0016170234885066748 Num fake examples 8555 Num true examples 9045\n",
      "  Batch 8,840  of  44,637.    Elapsed: 0:05:27. Training loss. 0.002682311227545142 Num fake examples 8593 Num true examples 9087\n",
      "  Batch 8,880  of  44,637.    Elapsed: 0:05:28. Training loss. 0.003130498807877302 Num fake examples 8632 Num true examples 9128\n",
      "  Batch 8,920  of  44,637.    Elapsed: 0:05:30. Training loss. 0.0036702300421893597 Num fake examples 8670 Num true examples 9170\n",
      "  Batch 8,960  of  44,637.    Elapsed: 0:05:32. Training loss. 0.002228720113635063 Num fake examples 8716 Num true examples 9204\n",
      "  Batch 9,000  of  44,637.    Elapsed: 0:05:33. Training loss. 0.0020723058842122555 Num fake examples 8757 Num true examples 9243\n",
      "  Batch 9,040  of  44,637.    Elapsed: 0:05:35. Training loss. 0.00315949902869761 Num fake examples 8803 Num true examples 9277\n",
      "  Batch 9,080  of  44,637.    Elapsed: 0:05:36. Training loss. 0.004713844042271376 Num fake examples 8848 Num true examples 9312\n",
      "  Batch 9,120  of  44,637.    Elapsed: 0:05:38. Training loss. 0.003165370086207986 Num fake examples 8892 Num true examples 9348\n",
      "  Batch 9,160  of  44,637.    Elapsed: 0:05:39. Training loss. 0.0025879153981804848 Num fake examples 8926 Num true examples 9394\n",
      "  Batch 9,200  of  44,637.    Elapsed: 0:05:41. Training loss. 0.002942158142104745 Num fake examples 8963 Num true examples 9437\n",
      "  Batch 9,240  of  44,637.    Elapsed: 0:05:42. Training loss. 0.002348108682781458 Num fake examples 8990 Num true examples 9490\n",
      "  Batch 9,280  of  44,637.    Elapsed: 0:05:44. Training loss. 0.002176176058128476 Num fake examples 9031 Num true examples 9529\n",
      "  Batch 9,320  of  44,637.    Elapsed: 0:05:45. Training loss. 0.0026124040596187115 Num fake examples 9067 Num true examples 9573\n",
      "  Batch 9,360  of  44,637.    Elapsed: 0:05:47. Training loss. 0.0034218209329992533 Num fake examples 9103 Num true examples 9617\n",
      "  Batch 9,400  of  44,637.    Elapsed: 0:05:48. Training loss. 0.003333181608468294 Num fake examples 9148 Num true examples 9652\n",
      "  Batch 9,440  of  44,637.    Elapsed: 0:05:50. Training loss. 0.002766507677733898 Num fake examples 9187 Num true examples 9693\n",
      "  Batch 9,480  of  44,637.    Elapsed: 0:05:51. Training loss. 0.002821027534082532 Num fake examples 9231 Num true examples 9729\n",
      "  Batch 9,520  of  44,637.    Elapsed: 0:05:53. Training loss. 5.589760780334473 Num fake examples 9267 Num true examples 9773\n",
      "  Batch 9,560  of  44,637.    Elapsed: 0:05:54. Training loss. 0.0045011271722614765 Num fake examples 9308 Num true examples 9812\n",
      "  Batch 9,600  of  44,637.    Elapsed: 0:05:56. Training loss. 0.0023325015790760517 Num fake examples 9355 Num true examples 9845\n",
      "  Batch 9,640  of  44,637.    Elapsed: 0:05:57. Training loss. 0.003228280460461974 Num fake examples 9387 Num true examples 9893\n",
      "  Batch 9,680  of  44,637.    Elapsed: 0:05:58. Training loss. 0.004671411123126745 Num fake examples 9415 Num true examples 9945\n",
      "  Batch 9,720  of  44,637.    Elapsed: 0:06:00. Training loss. 0.002637636847794056 Num fake examples 9452 Num true examples 9988\n",
      "  Batch 9,760  of  44,637.    Elapsed: 0:06:01. Training loss. 0.0031935565639287233 Num fake examples 9490 Num true examples 10030\n",
      "  Batch 9,800  of  44,637.    Elapsed: 0:06:03. Training loss. 2.7658896446228027 Num fake examples 9525 Num true examples 10075\n",
      "  Batch 9,840  of  44,637.    Elapsed: 0:06:04. Training loss. 0.004861489869654179 Num fake examples 9566 Num true examples 10114\n",
      "  Batch 9,880  of  44,637.    Elapsed: 0:06:06. Training loss. 0.005376247689127922 Num fake examples 9594 Num true examples 10166\n",
      "  Batch 9,920  of  44,637.    Elapsed: 0:06:07. Training loss. 0.0037313455250114202 Num fake examples 9637 Num true examples 10203\n",
      "  Batch 9,960  of  44,637.    Elapsed: 0:06:09. Training loss. 0.005055698100477457 Num fake examples 9674 Num true examples 10246\n",
      "  Batch 10,000  of  44,637.    Elapsed: 0:06:10. Training loss. 0.004369339440017939 Num fake examples 9712 Num true examples 10288\n",
      "  Batch 10,040  of  44,637.    Elapsed: 0:06:12. Training loss. 0.0036627333611249924 Num fake examples 9755 Num true examples 10325\n",
      "  Batch 10,080  of  44,637.    Elapsed: 0:06:13. Training loss. 2.8501803874969482 Num fake examples 9797 Num true examples 10363\n",
      "  Batch 10,120  of  44,637.    Elapsed: 0:06:15. Training loss. 0.0037237978540360928 Num fake examples 9837 Num true examples 10403\n",
      "  Batch 10,160  of  44,637.    Elapsed: 0:06:16. Training loss. 0.0026758969761431217 Num fake examples 9879 Num true examples 10441\n",
      "  Batch 10,200  of  44,637.    Elapsed: 0:06:18. Training loss. 0.002206642646342516 Num fake examples 9917 Num true examples 10483\n",
      "  Batch 10,240  of  44,637.    Elapsed: 0:06:19. Training loss. 0.002604031004011631 Num fake examples 9959 Num true examples 10521\n",
      "  Batch 10,280  of  44,637.    Elapsed: 0:06:21. Training loss. 0.0028645482379943132 Num fake examples 10006 Num true examples 10554\n",
      "  Batch 10,320  of  44,637.    Elapsed: 0:06:22. Training loss. 0.0032891565933823586 Num fake examples 10043 Num true examples 10597\n",
      "  Batch 10,360  of  44,637.    Elapsed: 0:06:24. Training loss. 0.001557437120936811 Num fake examples 10078 Num true examples 10642\n",
      "  Batch 10,400  of  44,637.    Elapsed: 0:06:25. Training loss. 0.002069303300231695 Num fake examples 10112 Num true examples 10688\n",
      "  Batch 10,440  of  44,637.    Elapsed: 0:06:27. Training loss. 0.0016758221900090575 Num fake examples 10148 Num true examples 10732\n",
      "  Batch 10,480  of  44,637.    Elapsed: 0:06:28. Training loss. 0.0029533598572015762 Num fake examples 10184 Num true examples 10776\n",
      "  Batch 10,520  of  44,637.    Elapsed: 0:06:30. Training loss. 0.00384427048265934 Num fake examples 10222 Num true examples 10818\n",
      "  Batch 10,560  of  44,637.    Elapsed: 0:06:31. Training loss. 3.1162211894989014 Num fake examples 10258 Num true examples 10862\n",
      "  Batch 10,600  of  44,637.    Elapsed: 0:06:33. Training loss. 0.0034487308003008366 Num fake examples 10302 Num true examples 10898\n",
      "  Batch 10,640  of  44,637.    Elapsed: 0:06:34. Training loss. 0.0025079683400690556 Num fake examples 10341 Num true examples 10939\n",
      "  Batch 10,680  of  44,637.    Elapsed: 0:06:35. Training loss. 0.0037051381077617407 Num fake examples 10383 Num true examples 10977\n",
      "  Batch 10,720  of  44,637.    Elapsed: 0:06:37. Training loss. 0.003974516410380602 Num fake examples 10422 Num true examples 11018\n",
      "  Batch 10,760  of  44,637.    Elapsed: 0:06:38. Training loss. 0.005590425804257393 Num fake examples 10458 Num true examples 11062\n",
      "  Batch 10,800  of  44,637.    Elapsed: 0:06:40. Training loss. 0.004542890936136246 Num fake examples 10494 Num true examples 11106\n",
      "  Batch 10,840  of  44,637.    Elapsed: 0:06:41. Training loss. 2.6499040126800537 Num fake examples 10537 Num true examples 11143\n",
      "  Batch 10,880  of  44,637.    Elapsed: 0:06:43. Training loss. 0.0039031910710036755 Num fake examples 10585 Num true examples 11175\n",
      "  Batch 10,920  of  44,637.    Elapsed: 0:06:44. Training loss. 0.003744351677596569 Num fake examples 10625 Num true examples 11215\n",
      "  Batch 10,960  of  44,637.    Elapsed: 0:06:46. Training loss. 0.0030845841392874718 Num fake examples 10666 Num true examples 11254\n",
      "  Batch 11,000  of  44,637.    Elapsed: 0:06:47. Training loss. 0.0029022537637501955 Num fake examples 10704 Num true examples 11296\n",
      "  Batch 11,040  of  44,637.    Elapsed: 0:06:49. Training loss. 0.00314748240634799 Num fake examples 10740 Num true examples 11340\n",
      "  Batch 11,080  of  44,637.    Elapsed: 0:06:50. Training loss. 0.0035697962157428265 Num fake examples 10776 Num true examples 11384\n",
      "  Batch 11,120  of  44,637.    Elapsed: 0:06:52. Training loss. 0.003743209643289447 Num fake examples 10810 Num true examples 11430\n",
      "  Batch 11,160  of  44,637.    Elapsed: 0:06:53. Training loss. 0.0017217001877725124 Num fake examples 10845 Num true examples 11475\n",
      "  Batch 11,200  of  44,637.    Elapsed: 0:06:55. Training loss. 0.0013941301731392741 Num fake examples 10878 Num true examples 11522\n",
      "  Batch 11,240  of  44,637.    Elapsed: 0:06:56. Training loss. 3.1781671047210693 Num fake examples 10923 Num true examples 11557\n",
      "  Batch 11,280  of  44,637.    Elapsed: 0:06:58. Training loss. 0.0030981162562966347 Num fake examples 10964 Num true examples 11596\n",
      "  Batch 11,320  of  44,637.    Elapsed: 0:06:59. Training loss. 0.0019144751131534576 Num fake examples 11005 Num true examples 11635\n",
      "  Batch 11,360  of  44,637.    Elapsed: 0:07:01. Training loss. 0.0025676311925053596 Num fake examples 11049 Num true examples 11671\n",
      "  Batch 11,400  of  44,637.    Elapsed: 0:07:02. Training loss. 0.003095512744039297 Num fake examples 11084 Num true examples 11716\n",
      "  Batch 11,440  of  44,637.    Elapsed: 0:07:04. Training loss. 0.001390756806358695 Num fake examples 11133 Num true examples 11747\n",
      "  Batch 11,480  of  44,637.    Elapsed: 0:07:05. Training loss. 0.0020203450694680214 Num fake examples 11165 Num true examples 11795\n",
      "  Batch 11,520  of  44,637.    Elapsed: 0:07:07. Training loss. 0.002601251471787691 Num fake examples 11205 Num true examples 11835\n",
      "  Batch 11,560  of  44,637.    Elapsed: 0:07:08. Training loss. 0.0035746644716709852 Num fake examples 11246 Num true examples 11874\n",
      "  Batch 11,600  of  44,637.    Elapsed: 0:07:10. Training loss. 0.0019679213874042034 Num fake examples 11283 Num true examples 11917\n",
      "  Batch 11,640  of  44,637.    Elapsed: 0:07:11. Training loss. 0.003993891645222902 Num fake examples 11319 Num true examples 11961\n",
      "  Batch 11,680  of  44,637.    Elapsed: 0:07:13. Training loss. 0.002818646375089884 Num fake examples 11367 Num true examples 11993\n",
      "  Batch 11,720  of  44,637.    Elapsed: 0:07:14. Training loss. 0.0023388387635350227 Num fake examples 11413 Num true examples 12027\n",
      "  Batch 11,760  of  44,637.    Elapsed: 0:07:16. Training loss. 0.003031018655747175 Num fake examples 11449 Num true examples 12071\n",
      "  Batch 11,800  of  44,637.    Elapsed: 0:07:17. Training loss. 0.0021368563175201416 Num fake examples 11489 Num true examples 12111\n",
      "  Batch 11,840  of  44,637.    Elapsed: 0:07:18. Training loss. 0.003000683384016156 Num fake examples 11528 Num true examples 12152\n",
      "  Batch 11,880  of  44,637.    Elapsed: 0:07:20. Training loss. 0.004599772859364748 Num fake examples 11570 Num true examples 12190\n",
      "  Batch 11,920  of  44,637.    Elapsed: 0:07:21. Training loss. 0.004327540285885334 Num fake examples 11616 Num true examples 12224\n",
      "  Batch 11,960  of  44,637.    Elapsed: 0:07:23. Training loss. 0.002294701524078846 Num fake examples 11645 Num true examples 12275\n",
      "  Batch 12,000  of  44,637.    Elapsed: 0:07:24. Training loss. 0.004474743269383907 Num fake examples 11686 Num true examples 12314\n",
      "  Batch 12,040  of  44,637.    Elapsed: 0:07:26. Training loss. 0.005976591259241104 Num fake examples 11721 Num true examples 12359\n",
      "  Batch 12,080  of  44,637.    Elapsed: 0:07:27. Training loss. 0.005817950703203678 Num fake examples 11758 Num true examples 12402\n",
      "  Batch 12,120  of  44,637.    Elapsed: 0:07:29. Training loss. 0.0021801949478685856 Num fake examples 11793 Num true examples 12447\n",
      "  Batch 12,160  of  44,637.    Elapsed: 0:07:30. Training loss. 0.005723142065107822 Num fake examples 11825 Num true examples 12495\n",
      "  Batch 12,200  of  44,637.    Elapsed: 0:07:32. Training loss. 0.005269414279609919 Num fake examples 11855 Num true examples 12545\n",
      "  Batch 12,240  of  44,637.    Elapsed: 0:07:33. Training loss. 0.0017448586877435446 Num fake examples 11892 Num true examples 12588\n",
      "  Batch 12,280  of  44,637.    Elapsed: 0:07:35. Training loss. 0.005045865662395954 Num fake examples 11932 Num true examples 12628\n",
      "  Batch 12,320  of  44,637.    Elapsed: 0:07:36. Training loss. 0.005067821592092514 Num fake examples 11973 Num true examples 12667\n",
      "  Batch 12,360  of  44,637.    Elapsed: 0:07:38. Training loss. 0.004564367234706879 Num fake examples 12011 Num true examples 12709\n",
      "  Batch 12,400  of  44,637.    Elapsed: 0:07:39. Training loss. 0.0018679378554224968 Num fake examples 12052 Num true examples 12748\n",
      "  Batch 12,440  of  44,637.    Elapsed: 0:07:41. Training loss. 0.0018873680382966995 Num fake examples 12090 Num true examples 12790\n",
      "  Batch 12,480  of  44,637.    Elapsed: 0:07:42. Training loss. 0.002059925813227892 Num fake examples 12126 Num true examples 12834\n",
      "  Batch 12,520  of  44,637.    Elapsed: 0:07:43. Training loss. 0.0025672423653304577 Num fake examples 12164 Num true examples 12876\n",
      "  Batch 12,560  of  44,637.    Elapsed: 0:07:45. Training loss. 0.0017677943687886 Num fake examples 12200 Num true examples 12920\n",
      "  Batch 12,600  of  44,637.    Elapsed: 0:07:46. Training loss. 0.0037347073666751385 Num fake examples 12239 Num true examples 12961\n",
      "  Batch 12,640  of  44,637.    Elapsed: 0:07:48. Training loss. 0.002788018202409148 Num fake examples 12274 Num true examples 13006\n",
      "  Batch 12,680  of  44,637.    Elapsed: 0:07:49. Training loss. 0.0027664240915328264 Num fake examples 12310 Num true examples 13050\n",
      "  Batch 12,720  of  44,637.    Elapsed: 0:07:51. Training loss. 0.0048492238856852055 Num fake examples 12355 Num true examples 13085\n",
      "  Batch 12,760  of  44,637.    Elapsed: 0:07:52. Training loss. 0.0032118603121489286 Num fake examples 12394 Num true examples 13126\n",
      "  Batch 12,800  of  44,637.    Elapsed: 0:07:54. Training loss. 0.0010457397438585758 Num fake examples 12428 Num true examples 13172\n",
      "  Batch 12,840  of  44,637.    Elapsed: 0:07:55. Training loss. 0.002566305687651038 Num fake examples 12465 Num true examples 13215\n",
      "  Batch 12,880  of  44,637.    Elapsed: 0:07:57. Training loss. 0.0021406938321888447 Num fake examples 12500 Num true examples 13260\n",
      "  Batch 12,920  of  44,637.    Elapsed: 0:07:58. Training loss. 0.002319506835192442 Num fake examples 12534 Num true examples 13306\n",
      "  Batch 12,960  of  44,637.    Elapsed: 0:08:00. Training loss. 0.0014758878387510777 Num fake examples 12573 Num true examples 13347\n",
      "  Batch 13,000  of  44,637.    Elapsed: 0:08:01. Training loss. 0.00231554196216166 Num fake examples 12607 Num true examples 13393\n",
      "  Batch 13,040  of  44,637.    Elapsed: 0:08:03. Training loss. 0.000766787095926702 Num fake examples 12643 Num true examples 13437\n",
      "  Batch 13,080  of  44,637.    Elapsed: 0:08:04. Training loss. 0.0020349484402686357 Num fake examples 12689 Num true examples 13471\n",
      "  Batch 13,120  of  44,637.    Elapsed: 0:08:06. Training loss. 0.0035825795494019985 Num fake examples 12727 Num true examples 13513\n",
      "  Batch 13,160  of  44,637.    Elapsed: 0:08:07. Training loss. 0.0023087451700121164 Num fake examples 12766 Num true examples 13554\n",
      "  Batch 13,200  of  44,637.    Elapsed: 0:08:09. Training loss. 0.0026116250082850456 Num fake examples 12802 Num true examples 13598\n",
      "  Batch 13,240  of  44,637.    Elapsed: 0:08:10. Training loss. 0.0039605447091162205 Num fake examples 12845 Num true examples 13635\n",
      "  Batch 13,280  of  44,637.    Elapsed: 0:08:11. Training loss. 0.0012839175760746002 Num fake examples 12884 Num true examples 13676\n",
      "  Batch 13,320  of  44,637.    Elapsed: 0:08:13. Training loss. 0.001925197895616293 Num fake examples 12920 Num true examples 13720\n",
      "  Batch 13,360  of  44,637.    Elapsed: 0:08:14. Training loss. 0.002625938504934311 Num fake examples 12953 Num true examples 13767\n",
      "  Batch 13,400  of  44,637.    Elapsed: 0:08:16. Training loss. 0.002289046533405781 Num fake examples 12994 Num true examples 13806\n",
      "  Batch 13,440  of  44,637.    Elapsed: 0:08:17. Training loss. 0.002560251159593463 Num fake examples 13036 Num true examples 13844\n",
      "  Batch 13,480  of  44,637.    Elapsed: 0:08:19. Training loss. 0.0026943751145154238 Num fake examples 13067 Num true examples 13893\n",
      "  Batch 13,520  of  44,637.    Elapsed: 0:08:20. Training loss. 0.003921715542674065 Num fake examples 13103 Num true examples 13937\n",
      "  Batch 13,560  of  44,637.    Elapsed: 0:08:22. Training loss. 0.002830363577231765 Num fake examples 13144 Num true examples 13976\n",
      "  Batch 13,600  of  44,637.    Elapsed: 0:08:23. Training loss. 0.0029791342094540596 Num fake examples 13174 Num true examples 14026\n",
      "  Batch 13,640  of  44,637.    Elapsed: 0:08:25. Training loss. 0.002324705943465233 Num fake examples 13211 Num true examples 14069\n",
      "  Batch 13,680  of  44,637.    Elapsed: 0:08:26. Training loss. 0.004210931248962879 Num fake examples 13253 Num true examples 14107\n",
      "  Batch 13,720  of  44,637.    Elapsed: 0:08:28. Training loss. 0.003149063792079687 Num fake examples 13286 Num true examples 14154\n",
      "  Batch 13,760  of  44,637.    Elapsed: 0:08:29. Training loss. 0.0032838652841746807 Num fake examples 13324 Num true examples 14196\n",
      "  Batch 13,800  of  44,637.    Elapsed: 0:08:31. Training loss. 0.002857271581888199 Num fake examples 13367 Num true examples 14233\n",
      "  Batch 13,840  of  44,637.    Elapsed: 0:08:32. Training loss. 2.492535352706909 Num fake examples 13406 Num true examples 14274\n",
      "  Batch 13,880  of  44,637.    Elapsed: 0:08:34. Training loss. 0.005757882725447416 Num fake examples 13446 Num true examples 14314\n",
      "  Batch 13,920  of  44,637.    Elapsed: 0:08:35. Training loss. 0.004485510289669037 Num fake examples 13488 Num true examples 14352\n",
      "  Batch 13,960  of  44,637.    Elapsed: 0:08:37. Training loss. 0.004426581785082817 Num fake examples 13532 Num true examples 14388\n",
      "  Batch 14,000  of  44,637.    Elapsed: 0:08:38. Training loss. 0.0032323584891855717 Num fake examples 13577 Num true examples 14423\n",
      "  Batch 14,040  of  44,637.    Elapsed: 0:08:40. Training loss. 0.0049223224632442 Num fake examples 13614 Num true examples 14466\n",
      "  Batch 14,080  of  44,637.    Elapsed: 0:08:41. Training loss. 0.002477007219567895 Num fake examples 13647 Num true examples 14513\n",
      "  Batch 14,120  of  44,637.    Elapsed: 0:08:43. Training loss. 0.0028469027020037174 Num fake examples 13695 Num true examples 14545\n",
      "  Batch 14,160  of  44,637.    Elapsed: 0:08:44. Training loss. 0.0041828639805316925 Num fake examples 13738 Num true examples 14582\n",
      "  Batch 14,200  of  44,637.    Elapsed: 0:08:46. Training loss. 0.0032235514372587204 Num fake examples 13777 Num true examples 14623\n",
      "  Batch 14,240  of  44,637.    Elapsed: 0:08:47. Training loss. 0.003577183000743389 Num fake examples 13822 Num true examples 14658\n",
      "  Batch 14,280  of  44,637.    Elapsed: 0:08:49. Training loss. 0.004141719080507755 Num fake examples 13860 Num true examples 14700\n",
      "  Batch 14,320  of  44,637.    Elapsed: 0:08:50. Training loss. 0.003015127032995224 Num fake examples 13902 Num true examples 14738\n",
      "  Batch 14,360  of  44,637.    Elapsed: 0:08:52. Training loss. 0.002576940692961216 Num fake examples 13946 Num true examples 14774\n",
      "  Batch 14,400  of  44,637.    Elapsed: 0:08:53. Training loss. 0.0032548995222896338 Num fake examples 13990 Num true examples 14810\n",
      "  Batch 14,440  of  44,637.    Elapsed: 0:08:55. Training loss. 0.002144232392311096 Num fake examples 14032 Num true examples 14848\n",
      "  Batch 14,480  of  44,637.    Elapsed: 0:08:56. Training loss. 0.0036813723854720592 Num fake examples 14065 Num true examples 14895\n",
      "  Batch 14,520  of  44,637.    Elapsed: 0:08:58. Training loss. 0.0036862355191260576 Num fake examples 14101 Num true examples 14939\n",
      "  Batch 14,560  of  44,637.    Elapsed: 0:08:59. Training loss. 0.002745271660387516 Num fake examples 14132 Num true examples 14988\n",
      "  Batch 14,600  of  44,637.    Elapsed: 0:09:01. Training loss. 0.005333428271114826 Num fake examples 14165 Num true examples 15035\n",
      "  Batch 14,640  of  44,637.    Elapsed: 0:09:02. Training loss. 0.003066963516175747 Num fake examples 14214 Num true examples 15066\n",
      "  Batch 14,680  of  44,637.    Elapsed: 0:09:04. Training loss. 0.0020877658389508724 Num fake examples 14253 Num true examples 15107\n",
      "  Batch 14,720  of  44,637.    Elapsed: 0:09:05. Training loss. 0.002467035548761487 Num fake examples 14298 Num true examples 15142\n",
      "  Batch 14,760  of  44,637.    Elapsed: 0:09:07. Training loss. 0.003126326482743025 Num fake examples 14331 Num true examples 15189\n",
      "  Batch 14,800  of  44,637.    Elapsed: 0:09:08. Training loss. 0.0023861511144787073 Num fake examples 14371 Num true examples 15229\n",
      "  Batch 14,840  of  44,637.    Elapsed: 0:09:09. Training loss. 0.0026232481468468904 Num fake examples 14407 Num true examples 15273\n",
      "  Batch 14,880  of  44,637.    Elapsed: 0:09:11. Training loss. 0.0019238984677940607 Num fake examples 14446 Num true examples 15314\n",
      "  Batch 14,920  of  44,637.    Elapsed: 0:09:12. Training loss. 0.002980973804369569 Num fake examples 14484 Num true examples 15356\n",
      "  Batch 14,960  of  44,637.    Elapsed: 0:09:14. Training loss. 0.002583290683105588 Num fake examples 14522 Num true examples 15398\n",
      "  Batch 15,000  of  44,637.    Elapsed: 0:09:15. Training loss. 0.0020161352586001158 Num fake examples 14558 Num true examples 15442\n",
      "  Batch 15,040  of  44,637.    Elapsed: 0:09:17. Training loss. 0.004843398928642273 Num fake examples 14598 Num true examples 15482\n",
      "  Batch 15,080  of  44,637.    Elapsed: 0:09:18. Training loss. 0.001429290510714054 Num fake examples 14637 Num true examples 15523\n",
      "  Batch 15,120  of  44,637.    Elapsed: 0:09:20. Training loss. 0.004566018935292959 Num fake examples 14683 Num true examples 15557\n",
      "  Batch 15,160  of  44,637.    Elapsed: 0:09:21. Training loss. 0.005328383296728134 Num fake examples 14722 Num true examples 15598\n",
      "  Batch 15,200  of  44,637.    Elapsed: 0:09:23. Training loss. 0.007132266648113728 Num fake examples 14759 Num true examples 15641\n",
      "  Batch 15,240  of  44,637.    Elapsed: 0:09:24. Training loss. 0.0035154183860868216 Num fake examples 14794 Num true examples 15686\n",
      "  Batch 15,280  of  44,637.    Elapsed: 0:09:26. Training loss. 0.0037105297669768333 Num fake examples 14841 Num true examples 15719\n",
      "  Batch 15,320  of  44,637.    Elapsed: 0:09:27. Training loss. 0.003325491212308407 Num fake examples 14875 Num true examples 15765\n",
      "  Batch 15,360  of  44,637.    Elapsed: 0:09:29. Training loss. 0.0026418061461299658 Num fake examples 14910 Num true examples 15810\n",
      "  Batch 15,400  of  44,637.    Elapsed: 0:09:30. Training loss. 0.0028164000250399113 Num fake examples 14941 Num true examples 15859\n",
      "  Batch 15,440  of  44,637.    Elapsed: 0:09:32. Training loss. 0.005769379902631044 Num fake examples 14980 Num true examples 15900\n",
      "  Batch 15,480  of  44,637.    Elapsed: 0:09:33. Training loss. 0.004598026629537344 Num fake examples 15019 Num true examples 15941\n",
      "  Batch 15,520  of  44,637.    Elapsed: 0:09:35. Training loss. 0.005719440057873726 Num fake examples 15068 Num true examples 15972\n",
      "  Batch 15,560  of  44,637.    Elapsed: 0:09:36. Training loss. 0.0031337980180978775 Num fake examples 15109 Num true examples 16011\n",
      "  Batch 15,600  of  44,637.    Elapsed: 0:09:38. Training loss. 0.006062177009880543 Num fake examples 15160 Num true examples 16040\n",
      "  Batch 15,640  of  44,637.    Elapsed: 0:09:39. Training loss. 0.0034362045116722584 Num fake examples 15194 Num true examples 16086\n",
      "  Batch 15,680  of  44,637.    Elapsed: 0:09:41. Training loss. 0.0031879686284810305 Num fake examples 15239 Num true examples 16121\n",
      "  Batch 15,720  of  44,637.    Elapsed: 0:09:42. Training loss. 0.003953611943870783 Num fake examples 15277 Num true examples 16163\n",
      "  Batch 15,760  of  44,637.    Elapsed: 0:09:43. Training loss. 0.0031888694502413273 Num fake examples 15315 Num true examples 16205\n",
      "  Batch 15,800  of  44,637.    Elapsed: 0:09:45. Training loss. 0.0016759672435000539 Num fake examples 15356 Num true examples 16244\n",
      "  Batch 15,840  of  44,637.    Elapsed: 0:09:46. Training loss. 0.0016772442031651735 Num fake examples 15395 Num true examples 16285\n",
      "  Batch 15,880  of  44,637.    Elapsed: 0:09:48. Training loss. 0.0020417720079421997 Num fake examples 15437 Num true examples 16323\n",
      "  Batch 15,920  of  44,637.    Elapsed: 0:09:49. Training loss. 0.0018562489422038198 Num fake examples 15472 Num true examples 16368\n",
      "  Batch 15,960  of  44,637.    Elapsed: 0:09:51. Training loss. 0.0024496098048985004 Num fake examples 15506 Num true examples 16414\n",
      "  Batch 16,000  of  44,637.    Elapsed: 0:09:52. Training loss. 0.0017151993233710527 Num fake examples 15543 Num true examples 16457\n",
      "  Batch 16,040  of  44,637.    Elapsed: 0:09:54. Training loss. 0.0017666226485744119 Num fake examples 15577 Num true examples 16503\n",
      "  Batch 16,080  of  44,637.    Elapsed: 0:09:55. Training loss. 0.0017509221797809005 Num fake examples 15613 Num true examples 16547\n",
      "  Batch 16,120  of  44,637.    Elapsed: 0:09:57. Training loss. 0.001978695159777999 Num fake examples 15657 Num true examples 16583\n",
      "  Batch 16,160  of  44,637.    Elapsed: 0:09:58. Training loss. 0.00288667855784297 Num fake examples 15693 Num true examples 16627\n",
      "  Batch 16,200  of  44,637.    Elapsed: 0:10:00. Training loss. 0.00238737347535789 Num fake examples 15741 Num true examples 16659\n",
      "  Batch 16,240  of  44,637.    Elapsed: 0:10:01. Training loss. 0.0027923944871872663 Num fake examples 15777 Num true examples 16703\n",
      "  Batch 16,280  of  44,637.    Elapsed: 0:10:03. Training loss. 0.002973531372845173 Num fake examples 15820 Num true examples 16740\n",
      "  Batch 16,320  of  44,637.    Elapsed: 0:10:04. Training loss. 0.002500999951735139 Num fake examples 15868 Num true examples 16772\n",
      "  Batch 16,360  of  44,637.    Elapsed: 0:10:06. Training loss. 0.002361827762797475 Num fake examples 15904 Num true examples 16816\n",
      "  Batch 16,400  of  44,637.    Elapsed: 0:10:07. Training loss. 0.0021624909713864326 Num fake examples 15949 Num true examples 16851\n",
      "  Batch 16,440  of  44,637.    Elapsed: 0:10:09. Training loss. 0.003192551899701357 Num fake examples 15993 Num true examples 16887\n",
      "  Batch 16,480  of  44,637.    Elapsed: 0:10:10. Training loss. 0.0036800981033593416 Num fake examples 16035 Num true examples 16925\n",
      "  Batch 16,520  of  44,637.    Elapsed: 0:10:11. Training loss. 0.003701311070472002 Num fake examples 16080 Num true examples 16960\n",
      "  Batch 16,560  of  44,637.    Elapsed: 0:10:13. Training loss. 0.0019908652175217867 Num fake examples 16123 Num true examples 16997\n",
      "  Batch 16,600  of  44,637.    Elapsed: 0:10:14. Training loss. 0.002111345762386918 Num fake examples 16165 Num true examples 17035\n",
      "  Batch 16,640  of  44,637.    Elapsed: 0:10:16. Training loss. 0.002592326607555151 Num fake examples 16205 Num true examples 17075\n",
      "  Batch 16,680  of  44,637.    Elapsed: 0:10:17. Training loss. 0.0021215297747403383 Num fake examples 16238 Num true examples 17122\n",
      "  Batch 16,720  of  44,637.    Elapsed: 0:10:19. Training loss. 0.001631752122193575 Num fake examples 16280 Num true examples 17160\n",
      "  Batch 16,760  of  44,637.    Elapsed: 0:10:20. Training loss. 0.002636880613863468 Num fake examples 16326 Num true examples 17194\n",
      "  Batch 16,800  of  44,637.    Elapsed: 0:10:22. Training loss. 0.0014408394927158952 Num fake examples 16370 Num true examples 17230\n",
      "  Batch 16,840  of  44,637.    Elapsed: 0:10:23. Training loss. 0.0030687768012285233 Num fake examples 16405 Num true examples 17275\n",
      "  Batch 16,880  of  44,637.    Elapsed: 0:10:25. Training loss. 0.0029252925887703896 Num fake examples 16438 Num true examples 17322\n",
      "  Batch 16,920  of  44,637.    Elapsed: 0:10:26. Training loss. 0.0020690690726041794 Num fake examples 16472 Num true examples 17368\n",
      "  Batch 16,960  of  44,637.    Elapsed: 0:10:28. Training loss. 0.0021480433642864227 Num fake examples 16505 Num true examples 17415\n",
      "  Batch 17,000  of  44,637.    Elapsed: 0:10:29. Training loss. 0.002845854964107275 Num fake examples 16553 Num true examples 17447\n",
      "  Batch 17,040  of  44,637.    Elapsed: 0:10:31. Training loss. 2.963425636291504 Num fake examples 16588 Num true examples 17492\n",
      "  Batch 17,080  of  44,637.    Elapsed: 0:10:32. Training loss. 0.004712295718491077 Num fake examples 16630 Num true examples 17530\n",
      "  Batch 17,120  of  44,637.    Elapsed: 0:10:34. Training loss. 0.0029337750747799873 Num fake examples 16665 Num true examples 17575\n",
      "  Batch 17,160  of  44,637.    Elapsed: 0:10:35. Training loss. 0.0036095869727432728 Num fake examples 16700 Num true examples 17620\n",
      "  Batch 17,200  of  44,637.    Elapsed: 0:10:36. Training loss. 0.003920540679246187 Num fake examples 16745 Num true examples 17655\n",
      "  Batch 17,240  of  44,637.    Elapsed: 0:10:38. Training loss. 0.002756614936515689 Num fake examples 16786 Num true examples 17694\n",
      "  Batch 17,280  of  44,637.    Elapsed: 0:10:39. Training loss. 0.0036047131288796663 Num fake examples 16828 Num true examples 17732\n",
      "  Batch 17,320  of  44,637.    Elapsed: 0:10:41. Training loss. 0.0028171821031719446 Num fake examples 16868 Num true examples 17772\n",
      "  Batch 17,360  of  44,637.    Elapsed: 0:10:42. Training loss. 0.002967218402773142 Num fake examples 16905 Num true examples 17815\n",
      "  Batch 17,400  of  44,637.    Elapsed: 0:10:44. Training loss. 0.004598772153258324 Num fake examples 16942 Num true examples 17858\n",
      "  Batch 17,440  of  44,637.    Elapsed: 0:10:45. Training loss. 0.004146137740463018 Num fake examples 16975 Num true examples 17905\n",
      "  Batch 17,480  of  44,637.    Elapsed: 0:10:47. Training loss. 0.004899531602859497 Num fake examples 17013 Num true examples 17947\n",
      "  Batch 17,520  of  44,637.    Elapsed: 0:10:48. Training loss. 0.0028675924986600876 Num fake examples 17052 Num true examples 17988\n",
      "  Batch 17,560  of  44,637.    Elapsed: 0:10:50. Training loss. 0.0040303003042936325 Num fake examples 17091 Num true examples 18029\n",
      "  Batch 17,600  of  44,637.    Elapsed: 0:10:51. Training loss. 0.0033064554445445538 Num fake examples 17122 Num true examples 18078\n",
      "  Batch 17,640  of  44,637.    Elapsed: 0:10:53. Training loss. 0.002689190674573183 Num fake examples 17160 Num true examples 18120\n",
      "  Batch 17,680  of  44,637.    Elapsed: 0:10:54. Training loss. 0.0029956160578876734 Num fake examples 17199 Num true examples 18161\n",
      "  Batch 17,720  of  44,637.    Elapsed: 0:10:56. Training loss. 0.0025897915475070477 Num fake examples 17240 Num true examples 18200\n",
      "  Batch 17,760  of  44,637.    Elapsed: 0:10:57. Training loss. 0.00294669927097857 Num fake examples 17279 Num true examples 18241\n",
      "  Batch 17,800  of  44,637.    Elapsed: 0:10:59. Training loss. 0.005408986005932093 Num fake examples 17316 Num true examples 18284\n",
      "  Batch 17,840  of  44,637.    Elapsed: 0:11:00. Training loss. 0.0033928859047591686 Num fake examples 17352 Num true examples 18328\n",
      "  Batch 17,880  of  44,637.    Elapsed: 0:11:02. Training loss. 0.002530748024582863 Num fake examples 17389 Num true examples 18371\n",
      "  Batch 17,920  of  44,637.    Elapsed: 0:11:03. Training loss. 0.0023778784088790417 Num fake examples 17427 Num true examples 18413\n",
      "  Batch 17,960  of  44,637.    Elapsed: 0:11:05. Training loss. 0.0032513528130948544 Num fake examples 17466 Num true examples 18454\n",
      "  Batch 18,000  of  44,637.    Elapsed: 0:11:06. Training loss. 0.003249546978622675 Num fake examples 17504 Num true examples 18496\n",
      "  Batch 18,040  of  44,637.    Elapsed: 0:11:07. Training loss. 0.003389038611203432 Num fake examples 17544 Num true examples 18536\n",
      "  Batch 18,080  of  44,637.    Elapsed: 0:11:09. Training loss. 0.0032143639400601387 Num fake examples 17587 Num true examples 18573\n",
      "  Batch 18,120  of  44,637.    Elapsed: 0:11:10. Training loss. 0.0036405399441719055 Num fake examples 17626 Num true examples 18614\n",
      "  Batch 18,160  of  44,637.    Elapsed: 0:11:12. Training loss. 0.0029675234109163284 Num fake examples 17667 Num true examples 18653\n",
      "  Batch 18,200  of  44,637.    Elapsed: 0:11:13. Training loss. 0.003990015480667353 Num fake examples 17704 Num true examples 18696\n",
      "  Batch 18,240  of  44,637.    Elapsed: 0:11:15. Training loss. 0.0033614812418818474 Num fake examples 17745 Num true examples 18735\n",
      "  Batch 18,280  of  44,637.    Elapsed: 0:11:16. Training loss. 0.0045141298323869705 Num fake examples 17784 Num true examples 18776\n",
      "  Batch 18,320  of  44,637.    Elapsed: 0:11:18. Training loss. 0.003383588744327426 Num fake examples 17821 Num true examples 18819\n",
      "  Batch 18,360  of  44,637.    Elapsed: 0:11:19. Training loss. 0.0030295480974018574 Num fake examples 17862 Num true examples 18858\n",
      "  Batch 18,400  of  44,637.    Elapsed: 0:11:21. Training loss. 0.0028474684804677963 Num fake examples 17894 Num true examples 18906\n",
      "  Batch 18,440  of  44,637.    Elapsed: 0:11:22. Training loss. 0.0023232013918459415 Num fake examples 17938 Num true examples 18942\n",
      "  Batch 18,480  of  44,637.    Elapsed: 0:11:24. Training loss. 0.0022757910192012787 Num fake examples 17975 Num true examples 18985\n",
      "  Batch 18,520  of  44,637.    Elapsed: 0:11:25. Training loss. 0.005017073825001717 Num fake examples 18024 Num true examples 19016\n",
      "  Batch 18,560  of  44,637.    Elapsed: 0:11:27. Training loss. 0.004622681066393852 Num fake examples 18060 Num true examples 19060\n",
      "  Batch 18,600  of  44,637.    Elapsed: 0:11:28. Training loss. 0.003927947022020817 Num fake examples 18092 Num true examples 19108\n",
      "  Batch 18,640  of  44,637.    Elapsed: 0:11:30. Training loss. 0.0036766338162124157 Num fake examples 18126 Num true examples 19154\n",
      "  Batch 18,680  of  44,637.    Elapsed: 0:11:31. Training loss. 0.004024384543299675 Num fake examples 18167 Num true examples 19193\n",
      "  Batch 18,720  of  44,637.    Elapsed: 0:11:33. Training loss. 0.004496892914175987 Num fake examples 18206 Num true examples 19234\n",
      "  Batch 18,760  of  44,637.    Elapsed: 0:11:34. Training loss. 0.00391834182664752 Num fake examples 18251 Num true examples 19269\n",
      "  Batch 18,800  of  44,637.    Elapsed: 0:11:36. Training loss. 0.0033954184036701918 Num fake examples 18292 Num true examples 19308\n",
      "  Batch 18,840  of  44,637.    Elapsed: 0:11:37. Training loss. 0.003392208367586136 Num fake examples 18337 Num true examples 19343\n",
      "  Batch 18,880  of  44,637.    Elapsed: 0:11:39. Training loss. 0.0031165576074272394 Num fake examples 18369 Num true examples 19391\n",
      "  Batch 18,920  of  44,637.    Elapsed: 0:11:40. Training loss. 0.004563428461551666 Num fake examples 18408 Num true examples 19432\n",
      "  Batch 18,960  of  44,637.    Elapsed: 0:11:42. Training loss. 0.0031762775033712387 Num fake examples 18439 Num true examples 19481\n",
      "  Batch 19,000  of  44,637.    Elapsed: 0:11:43. Training loss. 0.0026788446120917797 Num fake examples 18470 Num true examples 19530\n",
      "  Batch 19,040  of  44,637.    Elapsed: 0:11:45. Training loss. 0.0034962687641382217 Num fake examples 18506 Num true examples 19574\n",
      "  Batch 19,080  of  44,637.    Elapsed: 0:11:46. Training loss. 0.002139241434633732 Num fake examples 18546 Num true examples 19614\n",
      "  Batch 19,120  of  44,637.    Elapsed: 0:11:48. Training loss. 0.0029089185409247875 Num fake examples 18589 Num true examples 19651\n",
      "  Batch 19,160  of  44,637.    Elapsed: 0:11:49. Training loss. 3.0505332946777344 Num fake examples 18628 Num true examples 19692\n",
      "  Batch 19,200  of  44,637.    Elapsed: 0:11:51. Training loss. 0.003331325249746442 Num fake examples 18666 Num true examples 19734\n",
      "  Batch 19,240  of  44,637.    Elapsed: 0:11:52. Training loss. 0.0027896303217858076 Num fake examples 18705 Num true examples 19775\n",
      "  Batch 19,280  of  44,637.    Elapsed: 0:11:54. Training loss. 2.9135823249816895 Num fake examples 18755 Num true examples 19805\n",
      "  Batch 19,320  of  44,637.    Elapsed: 0:11:55. Training loss. 0.0029910001903772354 Num fake examples 18799 Num true examples 19841\n",
      "  Batch 19,360  of  44,637.    Elapsed: 0:11:57. Training loss. 0.003638730151578784 Num fake examples 18838 Num true examples 19882\n",
      "  Batch 19,400  of  44,637.    Elapsed: 0:11:58. Training loss. 0.004309876821935177 Num fake examples 18872 Num true examples 19928\n",
      "  Batch 19,440  of  44,637.    Elapsed: 0:12:00. Training loss. 0.004032252822071314 Num fake examples 18904 Num true examples 19976\n",
      "  Batch 19,480  of  44,637.    Elapsed: 0:12:01. Training loss. 0.004868706222623587 Num fake examples 18945 Num true examples 20015\n",
      "  Batch 19,520  of  44,637.    Elapsed: 0:12:02. Training loss. 0.0035726374480873346 Num fake examples 18983 Num true examples 20057\n",
      "  Batch 19,560  of  44,637.    Elapsed: 0:12:04. Training loss. 0.0037245338317006826 Num fake examples 19019 Num true examples 20101\n",
      "  Batch 19,600  of  44,637.    Elapsed: 0:12:05. Training loss. 0.003347958903759718 Num fake examples 19058 Num true examples 20142\n",
      "  Batch 19,640  of  44,637.    Elapsed: 0:12:07. Training loss. 0.003282000543549657 Num fake examples 19102 Num true examples 20178\n",
      "  Batch 19,680  of  44,637.    Elapsed: 0:12:08. Training loss. 0.004006360657513142 Num fake examples 19142 Num true examples 20218\n",
      "  Batch 19,720  of  44,637.    Elapsed: 0:12:10. Training loss. 0.003920539282262325 Num fake examples 19183 Num true examples 20257\n",
      "  Batch 19,760  of  44,637.    Elapsed: 0:12:11. Training loss. 0.002748605329543352 Num fake examples 19226 Num true examples 20294\n",
      "  Batch 19,800  of  44,637.    Elapsed: 0:12:13. Training loss. 0.0037916896399110556 Num fake examples 19270 Num true examples 20330\n",
      "  Batch 19,840  of  44,637.    Elapsed: 0:12:14. Training loss. 0.0029988689348101616 Num fake examples 19311 Num true examples 20369\n",
      "  Batch 19,880  of  44,637.    Elapsed: 0:12:16. Training loss. 0.0031159266363829374 Num fake examples 19342 Num true examples 20418\n",
      "  Batch 19,920  of  44,637.    Elapsed: 0:12:17. Training loss. 0.0026553012430667877 Num fake examples 19374 Num true examples 20466\n",
      "  Batch 19,960  of  44,637.    Elapsed: 0:12:19. Training loss. 0.002738021546974778 Num fake examples 19409 Num true examples 20511\n",
      "  Batch 20,000  of  44,637.    Elapsed: 0:12:20. Training loss. 0.003490836825221777 Num fake examples 19449 Num true examples 20551\n",
      "  Batch 20,040  of  44,637.    Elapsed: 0:12:22. Training loss. 0.0036443111021071672 Num fake examples 19474 Num true examples 20606\n",
      "  Batch 20,080  of  44,637.    Elapsed: 0:12:23. Training loss. 0.005211466457694769 Num fake examples 19512 Num true examples 20648\n",
      "  Batch 20,120  of  44,637.    Elapsed: 0:12:25. Training loss. 0.0034190663136541843 Num fake examples 19556 Num true examples 20684\n",
      "  Batch 20,160  of  44,637.    Elapsed: 0:12:26. Training loss. 0.0047338493168354034 Num fake examples 19599 Num true examples 20721\n",
      "  Batch 20,200  of  44,637.    Elapsed: 0:12:28. Training loss. 0.004461230244487524 Num fake examples 19640 Num true examples 20760\n",
      "  Batch 20,240  of  44,637.    Elapsed: 0:12:29. Training loss. 0.0036091217771172523 Num fake examples 19680 Num true examples 20800\n",
      "  Batch 20,280  of  44,637.    Elapsed: 0:12:31. Training loss. 0.003338751383125782 Num fake examples 19715 Num true examples 20845\n",
      "  Batch 20,320  of  44,637.    Elapsed: 0:12:32. Training loss. 0.004325221758335829 Num fake examples 19754 Num true examples 20886\n",
      "  Batch 20,360  of  44,637.    Elapsed: 0:12:34. Training loss. 0.0022279464174062014 Num fake examples 19792 Num true examples 20928\n",
      "  Batch 20,400  of  44,637.    Elapsed: 0:12:35. Training loss. 0.0026060771197080612 Num fake examples 19836 Num true examples 20964\n",
      "  Batch 20,440  of  44,637.    Elapsed: 0:12:36. Training loss. 0.0036323159001767635 Num fake examples 19868 Num true examples 21012\n",
      "  Batch 20,480  of  44,637.    Elapsed: 0:12:38. Training loss. 0.004647059366106987 Num fake examples 19908 Num true examples 21052\n",
      "  Batch 20,520  of  44,637.    Elapsed: 0:12:39. Training loss. 0.002775347325950861 Num fake examples 19943 Num true examples 21097\n",
      "  Batch 20,560  of  44,637.    Elapsed: 0:12:41. Training loss. 0.0029994091019034386 Num fake examples 19989 Num true examples 21131\n",
      "  Batch 20,600  of  44,637.    Elapsed: 0:12:42. Training loss. 0.0031484353821724653 Num fake examples 20021 Num true examples 21179\n",
      "  Batch 20,640  of  44,637.    Elapsed: 0:12:44. Training loss. 0.004752239212393761 Num fake examples 20065 Num true examples 21215\n",
      "  Batch 20,680  of  44,637.    Elapsed: 0:12:45. Training loss. 0.004839773289859295 Num fake examples 20108 Num true examples 21252\n",
      "  Batch 20,720  of  44,637.    Elapsed: 0:12:47. Training loss. 0.004184193909168243 Num fake examples 20150 Num true examples 21290\n",
      "  Batch 20,760  of  44,637.    Elapsed: 0:12:48. Training loss. 0.0042218659073114395 Num fake examples 20199 Num true examples 21321\n",
      "  Batch 20,800  of  44,637.    Elapsed: 0:12:50. Training loss. 0.004611987620592117 Num fake examples 20233 Num true examples 21367\n",
      "  Batch 20,840  of  44,637.    Elapsed: 0:12:51. Training loss. 0.0054378570057451725 Num fake examples 20277 Num true examples 21403\n",
      "  Batch 20,880  of  44,637.    Elapsed: 0:12:53. Training loss. 0.004052899312227964 Num fake examples 20323 Num true examples 21437\n",
      "  Batch 20,920  of  44,637.    Elapsed: 0:12:54. Training loss. 0.003442703280597925 Num fake examples 20353 Num true examples 21487\n",
      "  Batch 20,960  of  44,637.    Elapsed: 0:12:56. Training loss. 0.0036965433973819017 Num fake examples 20393 Num true examples 21527\n",
      "  Batch 21,000  of  44,637.    Elapsed: 0:12:57. Training loss. 0.0028037468437105417 Num fake examples 20428 Num true examples 21572\n",
      "  Batch 21,040  of  44,637.    Elapsed: 0:12:59. Training loss. 0.0035139252431690693 Num fake examples 20463 Num true examples 21617\n",
      "  Batch 21,080  of  44,637.    Elapsed: 0:13:00. Training loss. 0.0016191392205655575 Num fake examples 20498 Num true examples 21662\n",
      "  Batch 21,120  of  44,637.    Elapsed: 0:13:02. Training loss. 0.004809257574379444 Num fake examples 20536 Num true examples 21704\n",
      "  Batch 21,160  of  44,637.    Elapsed: 0:13:03. Training loss. 0.003624732606112957 Num fake examples 20580 Num true examples 21740\n",
      "  Batch 21,200  of  44,637.    Elapsed: 0:13:05. Training loss. 0.003070038976147771 Num fake examples 20622 Num true examples 21778\n",
      "  Batch 21,240  of  44,637.    Elapsed: 0:13:06. Training loss. 0.0022922151256352663 Num fake examples 20649 Num true examples 21831\n",
      "  Batch 21,280  of  44,637.    Elapsed: 0:13:08. Training loss. 0.0026425719261169434 Num fake examples 20689 Num true examples 21871\n",
      "  Batch 21,320  of  44,637.    Elapsed: 0:13:09. Training loss. 0.0037371967919170856 Num fake examples 20725 Num true examples 21915\n",
      "  Batch 21,360  of  44,637.    Elapsed: 0:13:10. Training loss. 0.003040943294763565 Num fake examples 20763 Num true examples 21957\n",
      "  Batch 21,400  of  44,637.    Elapsed: 0:13:12. Training loss. 0.0021710526198148727 Num fake examples 20805 Num true examples 21995\n",
      "  Batch 21,440  of  44,637.    Elapsed: 0:13:13. Training loss. 0.0042581986635923386 Num fake examples 20843 Num true examples 22037\n",
      "  Batch 21,480  of  44,637.    Elapsed: 0:13:15. Training loss. 0.0016271625645458698 Num fake examples 20879 Num true examples 22081\n",
      "  Batch 21,520  of  44,637.    Elapsed: 0:13:16. Training loss. 0.0031128795817494392 Num fake examples 20921 Num true examples 22119\n",
      "  Batch 21,560  of  44,637.    Elapsed: 0:13:18. Training loss. 0.0027594100683927536 Num fake examples 20960 Num true examples 22160\n",
      "  Batch 21,600  of  44,637.    Elapsed: 0:13:19. Training loss. 0.002763190306723118 Num fake examples 21002 Num true examples 22198\n",
      "  Batch 21,640  of  44,637.    Elapsed: 0:13:21. Training loss. 0.003917314577847719 Num fake examples 21044 Num true examples 22236\n",
      "  Batch 21,680  of  44,637.    Elapsed: 0:13:22. Training loss. 0.003260102355852723 Num fake examples 21080 Num true examples 22280\n",
      "  Batch 21,720  of  44,637.    Elapsed: 0:13:24. Training loss. 0.0016156061319634318 Num fake examples 21117 Num true examples 22323\n",
      "  Batch 21,760  of  44,637.    Elapsed: 0:13:25. Training loss. 0.003950960002839565 Num fake examples 21156 Num true examples 22364\n",
      "  Batch 21,800  of  44,637.    Elapsed: 0:13:27. Training loss. 0.0028265942819416523 Num fake examples 21200 Num true examples 22400\n",
      "  Batch 21,840  of  44,637.    Elapsed: 0:13:28. Training loss. 0.0026076966896653175 Num fake examples 21234 Num true examples 22446\n",
      "  Batch 21,880  of  44,637.    Elapsed: 0:13:30. Training loss. 0.002028694376349449 Num fake examples 21275 Num true examples 22485\n",
      "  Batch 21,920  of  44,637.    Elapsed: 0:13:31. Training loss. 0.002491661813110113 Num fake examples 21319 Num true examples 22521\n",
      "  Batch 21,960  of  44,637.    Elapsed: 0:13:33. Training loss. 0.004441756755113602 Num fake examples 21361 Num true examples 22559\n",
      "  Batch 22,000  of  44,637.    Elapsed: 0:13:34. Training loss. 2.7493677139282227 Num fake examples 21405 Num true examples 22595\n",
      "  Batch 22,040  of  44,637.    Elapsed: 0:13:36. Training loss. 0.0034846693743020296 Num fake examples 21448 Num true examples 22632\n",
      "  Batch 22,080  of  44,637.    Elapsed: 0:13:37. Training loss. 0.003032672218978405 Num fake examples 21487 Num true examples 22673\n",
      "  Batch 22,120  of  44,637.    Elapsed: 0:13:39. Training loss. 0.0029546956066042185 Num fake examples 21528 Num true examples 22712\n",
      "  Batch 22,160  of  44,637.    Elapsed: 0:13:40. Training loss. 0.003427923657000065 Num fake examples 21569 Num true examples 22751\n",
      "  Batch 22,200  of  44,637.    Elapsed: 0:13:41. Training loss. 0.002848492469638586 Num fake examples 21611 Num true examples 22789\n",
      "  Batch 22,240  of  44,637.    Elapsed: 0:13:43. Training loss. 0.002537570195272565 Num fake examples 21642 Num true examples 22838\n",
      "  Batch 22,280  of  44,637.    Elapsed: 0:13:44. Training loss. 0.0017625419422984123 Num fake examples 21676 Num true examples 22884\n",
      "  Batch 22,320  of  44,637.    Elapsed: 0:13:46. Training loss. 0.0018837715033441782 Num fake examples 21712 Num true examples 22928\n",
      "  Batch 22,360  of  44,637.    Elapsed: 0:13:47. Training loss. 0.004338247235864401 Num fake examples 21752 Num true examples 22968\n",
      "  Batch 22,400  of  44,637.    Elapsed: 0:13:49. Training loss. 0.003031779546290636 Num fake examples 21800 Num true examples 23000\n",
      "  Batch 22,440  of  44,637.    Elapsed: 0:13:50. Training loss. 0.003420816734433174 Num fake examples 21838 Num true examples 23042\n",
      "  Batch 22,480  of  44,637.    Elapsed: 0:13:52. Training loss. 0.0049298787489533424 Num fake examples 21882 Num true examples 23078\n",
      "  Batch 22,520  of  44,637.    Elapsed: 0:13:53. Training loss. 0.0037680810783058405 Num fake examples 21909 Num true examples 23131\n",
      "  Batch 22,560  of  44,637.    Elapsed: 0:13:55. Training loss. 0.004019887186586857 Num fake examples 21947 Num true examples 23173\n",
      "  Batch 22,600  of  44,637.    Elapsed: 0:13:56. Training loss. 0.0021003251895308495 Num fake examples 21984 Num true examples 23216\n",
      "  Batch 22,640  of  44,637.    Elapsed: 0:13:58. Training loss. 0.002838053274899721 Num fake examples 22032 Num true examples 23248\n",
      "  Batch 22,680  of  44,637.    Elapsed: 0:13:59. Training loss. 0.003062147181481123 Num fake examples 22067 Num true examples 23293\n",
      "  Batch 22,720  of  44,637.    Elapsed: 0:14:01. Training loss. 0.002434268593788147 Num fake examples 22103 Num true examples 23337\n",
      "  Batch 22,760  of  44,637.    Elapsed: 0:14:02. Training loss. 0.0022374752443283796 Num fake examples 22138 Num true examples 23382\n",
      "  Batch 22,800  of  44,637.    Elapsed: 0:14:04. Training loss. 0.00291048944927752 Num fake examples 22181 Num true examples 23419\n",
      "  Batch 22,840  of  44,637.    Elapsed: 0:14:05. Training loss. 0.0037730594631284475 Num fake examples 22220 Num true examples 23460\n",
      "  Batch 22,880  of  44,637.    Elapsed: 0:14:07. Training loss. 0.0038130227476358414 Num fake examples 22256 Num true examples 23504\n",
      "  Batch 22,920  of  44,637.    Elapsed: 0:14:08. Training loss. 0.002899169921875 Num fake examples 22298 Num true examples 23542\n",
      "  Batch 22,960  of  44,637.    Elapsed: 0:14:10. Training loss. 0.0027356906794011593 Num fake examples 22346 Num true examples 23574\n",
      "  Batch 23,000  of  44,637.    Elapsed: 0:14:11. Training loss. 0.0038459324277937412 Num fake examples 22378 Num true examples 23622\n",
      "  Batch 23,040  of  44,637.    Elapsed: 0:14:13. Training loss. 0.002481865929439664 Num fake examples 22415 Num true examples 23665\n",
      "  Batch 23,080  of  44,637.    Elapsed: 0:14:14. Training loss. 0.0034185429103672504 Num fake examples 22457 Num true examples 23703\n",
      "  Batch 23,120  of  44,637.    Elapsed: 0:14:15. Training loss. 0.004024402238428593 Num fake examples 22494 Num true examples 23746\n",
      "  Batch 23,160  of  44,637.    Elapsed: 0:14:17. Training loss. 0.0023619525600224733 Num fake examples 22531 Num true examples 23789\n",
      "  Batch 23,200  of  44,637.    Elapsed: 0:14:18. Training loss. 0.002607499249279499 Num fake examples 22582 Num true examples 23818\n",
      "  Batch 23,240  of  44,637.    Elapsed: 0:14:20. Training loss. 0.002531923819333315 Num fake examples 22628 Num true examples 23852\n",
      "  Batch 23,280  of  44,637.    Elapsed: 0:14:21. Training loss. 0.0025987348053604364 Num fake examples 22669 Num true examples 23891\n",
      "  Batch 23,320  of  44,637.    Elapsed: 0:14:23. Training loss. 0.0022734138183295727 Num fake examples 22711 Num true examples 23929\n",
      "  Batch 23,360  of  44,637.    Elapsed: 0:14:24. Training loss. 2.5724995136260986 Num fake examples 22753 Num true examples 23967\n",
      "  Batch 23,400  of  44,637.    Elapsed: 0:14:26. Training loss. 0.003987953066825867 Num fake examples 22789 Num true examples 24011\n",
      "  Batch 23,440  of  44,637.    Elapsed: 0:14:27. Training loss. 0.006075761280953884 Num fake examples 22833 Num true examples 24047\n",
      "  Batch 23,480  of  44,637.    Elapsed: 0:14:29. Training loss. 0.002354840748012066 Num fake examples 22877 Num true examples 24083\n",
      "  Batch 23,520  of  44,637.    Elapsed: 0:14:30. Training loss. 0.006295330356806517 Num fake examples 22920 Num true examples 24120\n",
      "  Batch 23,560  of  44,637.    Elapsed: 0:14:32. Training loss. 0.003789951093494892 Num fake examples 22965 Num true examples 24155\n",
      "  Batch 23,600  of  44,637.    Elapsed: 0:14:33. Training loss. 0.0022116657346487045 Num fake examples 23004 Num true examples 24196\n",
      "  Batch 23,640  of  44,637.    Elapsed: 0:14:35. Training loss. 0.005345192737877369 Num fake examples 23044 Num true examples 24236\n",
      "  Batch 23,680  of  44,637.    Elapsed: 0:14:36. Training loss. 0.0036030777264386415 Num fake examples 23086 Num true examples 24274\n",
      "  Batch 23,720  of  44,637.    Elapsed: 0:14:38. Training loss. 0.0031021188478916883 Num fake examples 23138 Num true examples 24302\n",
      "  Batch 23,760  of  44,637.    Elapsed: 0:14:39. Training loss. 0.0027615572325885296 Num fake examples 23174 Num true examples 24346\n",
      "  Batch 23,800  of  44,637.    Elapsed: 0:14:41. Training loss. 0.0019378925208002329 Num fake examples 23212 Num true examples 24388\n",
      "  Batch 23,840  of  44,637.    Elapsed: 0:14:42. Training loss. 0.0020249621011316776 Num fake examples 23243 Num true examples 24437\n",
      "  Batch 23,880  of  44,637.    Elapsed: 0:14:44. Training loss. 0.002220762427896261 Num fake examples 23284 Num true examples 24476\n",
      "  Batch 23,920  of  44,637.    Elapsed: 0:14:45. Training loss. 0.0024433699436485767 Num fake examples 23313 Num true examples 24527\n",
      "  Batch 23,960  of  44,637.    Elapsed: 0:14:46. Training loss. 0.0016981320222839713 Num fake examples 23345 Num true examples 24575\n",
      "  Batch 24,000  of  44,637.    Elapsed: 0:14:48. Training loss. 0.002620038343593478 Num fake examples 23376 Num true examples 24624\n",
      "  Batch 24,040  of  44,637.    Elapsed: 0:14:49. Training loss. 0.003456088714301586 Num fake examples 23420 Num true examples 24660\n",
      "  Batch 24,080  of  44,637.    Elapsed: 0:14:51. Training loss. 0.002260881243273616 Num fake examples 23463 Num true examples 24697\n",
      "  Batch 24,120  of  44,637.    Elapsed: 0:14:52. Training loss. 0.0023671064991503954 Num fake examples 23505 Num true examples 24735\n",
      "  Batch 24,160  of  44,637.    Elapsed: 0:14:54. Training loss. 0.002814862411469221 Num fake examples 23546 Num true examples 24774\n",
      "  Batch 24,200  of  44,637.    Elapsed: 0:14:55. Training loss. 0.00269988807849586 Num fake examples 23581 Num true examples 24819\n",
      "  Batch 24,240  of  44,637.    Elapsed: 0:14:57. Training loss. 0.0034346459433436394 Num fake examples 23619 Num true examples 24861\n",
      "  Batch 24,280  of  44,637.    Elapsed: 0:14:58. Training loss. 0.002500073751434684 Num fake examples 23664 Num true examples 24896\n",
      "  Batch 24,320  of  44,637.    Elapsed: 0:15:00. Training loss. 0.0037326004821807146 Num fake examples 23695 Num true examples 24945\n",
      "  Batch 24,360  of  44,637.    Elapsed: 0:15:01. Training loss. 0.002478563692420721 Num fake examples 23726 Num true examples 24994\n",
      "  Batch 24,400  of  44,637.    Elapsed: 0:15:03. Training loss. 0.0033095269463956356 Num fake examples 23765 Num true examples 25035\n",
      "  Batch 24,440  of  44,637.    Elapsed: 0:15:04. Training loss. 0.0037301350384950638 Num fake examples 23805 Num true examples 25075\n",
      "  Batch 24,480  of  44,637.    Elapsed: 0:15:06. Training loss. 0.003833787515759468 Num fake examples 23849 Num true examples 25111\n",
      "  Batch 24,520  of  44,637.    Elapsed: 0:15:07. Training loss. 0.0030734199099242687 Num fake examples 23888 Num true examples 25152\n",
      "  Batch 24,560  of  44,637.    Elapsed: 0:15:09. Training loss. 0.003953204490244389 Num fake examples 23927 Num true examples 25193\n",
      "  Batch 24,600  of  44,637.    Elapsed: 0:15:10. Training loss. 2.896338939666748 Num fake examples 23969 Num true examples 25231\n",
      "  Batch 24,640  of  44,637.    Elapsed: 0:15:12. Training loss. 0.004328959155827761 Num fake examples 24009 Num true examples 25271\n",
      "  Batch 24,680  of  44,637.    Elapsed: 0:15:13. Training loss. 0.004574668128043413 Num fake examples 24040 Num true examples 25320\n",
      "  Batch 24,720  of  44,637.    Elapsed: 0:15:15. Training loss. 0.003982485271990299 Num fake examples 24073 Num true examples 25367\n",
      "  Batch 24,760  of  44,637.    Elapsed: 0:15:16. Training loss. 0.003159917425364256 Num fake examples 24108 Num true examples 25412\n",
      "  Batch 24,800  of  44,637.    Elapsed: 0:15:18. Training loss. 0.006162365432828665 Num fake examples 24154 Num true examples 25446\n",
      "  Batch 24,840  of  44,637.    Elapsed: 0:15:19. Training loss. 0.002949049696326256 Num fake examples 24205 Num true examples 25475\n",
      "  Batch 24,880  of  44,637.    Elapsed: 0:15:20. Training loss. 0.0034646561834961176 Num fake examples 24245 Num true examples 25515\n",
      "  Batch 24,920  of  44,637.    Elapsed: 0:15:22. Training loss. 0.004103157203644514 Num fake examples 24285 Num true examples 25555\n",
      "  Batch 24,960  of  44,637.    Elapsed: 0:15:23. Training loss. 0.004589999094605446 Num fake examples 24320 Num true examples 25600\n",
      "  Batch 25,000  of  44,637.    Elapsed: 0:15:25. Training loss. 0.0025061238557100296 Num fake examples 24358 Num true examples 25642\n",
      "  Batch 25,040  of  44,637.    Elapsed: 0:15:27. Training loss. 0.0026614321395754814 Num fake examples 24391 Num true examples 25689\n",
      "  Batch 25,080  of  44,637.    Elapsed: 0:15:28. Training loss. 0.004718903452157974 Num fake examples 24433 Num true examples 25727\n",
      "  Batch 25,120  of  44,637.    Elapsed: 0:15:30. Training loss. 0.002823054324835539 Num fake examples 24467 Num true examples 25773\n",
      "  Batch 25,160  of  44,637.    Elapsed: 0:15:31. Training loss. 0.0024350909516215324 Num fake examples 24508 Num true examples 25812\n",
      "  Batch 25,200  of  44,637.    Elapsed: 0:15:32. Training loss. 0.0034000088926404715 Num fake examples 24544 Num true examples 25856\n",
      "  Batch 25,240  of  44,637.    Elapsed: 0:15:34. Training loss. 0.002665940672159195 Num fake examples 24585 Num true examples 25895\n",
      "  Batch 25,280  of  44,637.    Elapsed: 0:15:35. Training loss. 0.0028983550146222115 Num fake examples 24623 Num true examples 25937\n",
      "  Batch 25,320  of  44,637.    Elapsed: 0:15:37. Training loss. 0.003115725237876177 Num fake examples 24661 Num true examples 25979\n",
      "  Batch 25,360  of  44,637.    Elapsed: 0:15:38. Training loss. 0.0029304830823093653 Num fake examples 24691 Num true examples 26029\n",
      "  Batch 25,400  of  44,637.    Elapsed: 0:15:40. Training loss. 0.0019099668134003878 Num fake examples 24734 Num true examples 26066\n",
      "  Batch 25,440  of  44,637.    Elapsed: 0:15:41. Training loss. 0.0030010263435542583 Num fake examples 24772 Num true examples 26108\n",
      "  Batch 25,480  of  44,637.    Elapsed: 0:15:43. Training loss. 0.0019190507009625435 Num fake examples 24824 Num true examples 26136\n",
      "  Batch 25,520  of  44,637.    Elapsed: 0:15:44. Training loss. 0.0028773974627256393 Num fake examples 24864 Num true examples 26176\n",
      "  Batch 25,560  of  44,637.    Elapsed: 0:15:46. Training loss. 0.00352276349440217 Num fake examples 24901 Num true examples 26219\n",
      "  Batch 25,600  of  44,637.    Elapsed: 0:15:47. Training loss. 0.0026617455296218395 Num fake examples 24940 Num true examples 26260\n",
      "  Batch 25,640  of  44,637.    Elapsed: 0:15:49. Training loss. 0.0033819919917732477 Num fake examples 24979 Num true examples 26301\n",
      "  Batch 25,680  of  44,637.    Elapsed: 0:15:50. Training loss. 0.0020728097297251225 Num fake examples 25015 Num true examples 26345\n",
      "  Batch 25,720  of  44,637.    Elapsed: 0:15:52. Training loss. 0.002656961092725396 Num fake examples 25052 Num true examples 26388\n",
      "  Batch 25,760  of  44,637.    Elapsed: 0:15:53. Training loss. 0.0025510392151772976 Num fake examples 25088 Num true examples 26432\n",
      "  Batch 25,800  of  44,637.    Elapsed: 0:15:55. Training loss. 0.002630708273500204 Num fake examples 25131 Num true examples 26469\n",
      "  Batch 25,840  of  44,637.    Elapsed: 0:15:56. Training loss. 2.935387134552002 Num fake examples 25157 Num true examples 26523\n",
      "  Batch 25,880  of  44,637.    Elapsed: 0:15:58. Training loss. 0.0016220503021031618 Num fake examples 25195 Num true examples 26565\n",
      "  Batch 25,920  of  44,637.    Elapsed: 0:16:00. Training loss. 0.0018689343705773354 Num fake examples 25235 Num true examples 26605\n",
      "  Batch 25,960  of  44,637.    Elapsed: 0:16:01. Training loss. 0.0014706287765875459 Num fake examples 25272 Num true examples 26648\n",
      "  Batch 26,000  of  44,637.    Elapsed: 0:16:03. Training loss. 0.0019645169377326965 Num fake examples 25306 Num true examples 26694\n",
      "  Batch 26,040  of  44,637.    Elapsed: 0:16:04. Training loss. 0.0026420815847814083 Num fake examples 25339 Num true examples 26741\n",
      "  Batch 26,080  of  44,637.    Elapsed: 0:16:05. Training loss. 0.0033877913374453783 Num fake examples 25377 Num true examples 26783\n",
      "  Batch 26,120  of  44,637.    Elapsed: 0:16:07. Training loss. 0.0027773417532444 Num fake examples 25417 Num true examples 26823\n",
      "  Batch 26,160  of  44,637.    Elapsed: 0:16:08. Training loss. 0.0023503953125327826 Num fake examples 25457 Num true examples 26863\n",
      "  Batch 26,200  of  44,637.    Elapsed: 0:16:10. Training loss. 0.0028052283450961113 Num fake examples 25498 Num true examples 26902\n",
      "  Batch 26,240  of  44,637.    Elapsed: 0:16:11. Training loss. 0.002703806385397911 Num fake examples 25539 Num true examples 26941\n",
      "  Batch 26,280  of  44,637.    Elapsed: 0:16:13. Training loss. 0.002579965628683567 Num fake examples 25586 Num true examples 26974\n",
      "  Batch 26,320  of  44,637.    Elapsed: 0:16:14. Training loss. 0.0022492147982120514 Num fake examples 25627 Num true examples 27013\n",
      "  Batch 26,360  of  44,637.    Elapsed: 0:16:16. Training loss. 0.0035472833551466465 Num fake examples 25668 Num true examples 27052\n",
      "  Batch 26,400  of  44,637.    Elapsed: 0:16:17. Training loss. 0.003009392414242029 Num fake examples 25706 Num true examples 27094\n",
      "  Batch 26,440  of  44,637.    Elapsed: 0:16:19. Training loss. 0.0033689909614622593 Num fake examples 25737 Num true examples 27143\n",
      "  Batch 26,480  of  44,637.    Elapsed: 0:16:20. Training loss. 0.002235868712887168 Num fake examples 25783 Num true examples 27177\n",
      "  Batch 26,520  of  44,637.    Elapsed: 0:16:22. Training loss. 2.9444632530212402 Num fake examples 25823 Num true examples 27217\n",
      "  Batch 26,560  of  44,637.    Elapsed: 0:16:23. Training loss. 0.0042333342134952545 Num fake examples 25862 Num true examples 27258\n",
      "  Batch 26,600  of  44,637.    Elapsed: 0:16:25. Training loss. 2.8806216716766357 Num fake examples 25900 Num true examples 27300\n",
      "  Batch 26,640  of  44,637.    Elapsed: 0:16:26. Training loss. 0.005087190307676792 Num fake examples 25941 Num true examples 27339\n",
      "  Batch 26,680  of  44,637.    Elapsed: 0:16:28. Training loss. 0.005384177900850773 Num fake examples 25975 Num true examples 27385\n",
      "  Batch 26,720  of  44,637.    Elapsed: 0:16:29. Training loss. 0.005157759413123131 Num fake examples 26017 Num true examples 27423\n",
      "  Batch 26,760  of  44,637.    Elapsed: 0:16:31. Training loss. 0.004323064349591732 Num fake examples 26057 Num true examples 27463\n",
      "  Batch 26,800  of  44,637.    Elapsed: 0:16:32. Training loss. 0.005536607466638088 Num fake examples 26092 Num true examples 27508\n",
      "  Batch 26,840  of  44,637.    Elapsed: 0:16:34. Training loss. 0.004447504412382841 Num fake examples 26133 Num true examples 27547\n",
      "  Batch 26,880  of  44,637.    Elapsed: 0:16:35. Training loss. 0.004274942446500063 Num fake examples 26181 Num true examples 27579\n",
      "  Batch 26,920  of  44,637.    Elapsed: 0:16:37. Training loss. 0.004189955536276102 Num fake examples 26215 Num true examples 27625\n",
      "  Batch 26,960  of  44,637.    Elapsed: 0:16:38. Training loss. 0.0033607701770961285 Num fake examples 26257 Num true examples 27663\n",
      "  Batch 27,000  of  44,637.    Elapsed: 0:16:40. Training loss. 0.0037540849298238754 Num fake examples 26299 Num true examples 27701\n",
      "  Batch 27,040  of  44,637.    Elapsed: 0:16:41. Training loss. 0.002893830882385373 Num fake examples 26337 Num true examples 27743\n",
      "  Batch 27,080  of  44,637.    Elapsed: 0:16:42. Training loss. 0.0033120247535407543 Num fake examples 26375 Num true examples 27785\n",
      "  Batch 27,120  of  44,637.    Elapsed: 0:16:44. Training loss. 0.0028911293484270573 Num fake examples 26417 Num true examples 27823\n",
      "  Batch 27,160  of  44,637.    Elapsed: 0:16:45. Training loss. 0.0039308080449700356 Num fake examples 26461 Num true examples 27859\n",
      "  Batch 27,200  of  44,637.    Elapsed: 0:16:47. Training loss. 0.003012759843841195 Num fake examples 26502 Num true examples 27898\n",
      "  Batch 27,240  of  44,637.    Elapsed: 0:16:48. Training loss. 0.002456080401316285 Num fake examples 26530 Num true examples 27950\n",
      "  Batch 27,280  of  44,637.    Elapsed: 0:16:50. Training loss. 0.0025002891197800636 Num fake examples 26573 Num true examples 27987\n",
      "  Batch 27,320  of  44,637.    Elapsed: 0:16:51. Training loss. 0.002229588571935892 Num fake examples 26614 Num true examples 28026\n",
      "  Batch 27,360  of  44,637.    Elapsed: 0:16:53. Training loss. 0.002336474135518074 Num fake examples 26649 Num true examples 28071\n",
      "  Batch 27,400  of  44,637.    Elapsed: 0:16:54. Training loss. 0.003081320784986019 Num fake examples 26689 Num true examples 28111\n",
      "  Batch 27,440  of  44,637.    Elapsed: 0:16:56. Training loss. 0.0024852496571838856 Num fake examples 26719 Num true examples 28161\n",
      "  Batch 27,480  of  44,637.    Elapsed: 0:16:57. Training loss. 0.003253166563808918 Num fake examples 26761 Num true examples 28199\n",
      "  Batch 27,520  of  44,637.    Elapsed: 0:16:59. Training loss. 0.003416180144995451 Num fake examples 26800 Num true examples 28240\n",
      "  Batch 27,560  of  44,637.    Elapsed: 0:17:00. Training loss. 0.0033956144470721483 Num fake examples 26837 Num true examples 28283\n",
      "  Batch 27,600  of  44,637.    Elapsed: 0:17:02. Training loss. 0.004386098124086857 Num fake examples 26879 Num true examples 28321\n",
      "  Batch 27,640  of  44,637.    Elapsed: 0:17:03. Training loss. 0.004696539603173733 Num fake examples 26915 Num true examples 28365\n",
      "  Batch 27,680  of  44,637.    Elapsed: 0:17:05. Training loss. 0.0030067870393395424 Num fake examples 26949 Num true examples 28411\n",
      "  Batch 27,720  of  44,637.    Elapsed: 0:17:06. Training loss. 0.004004514776170254 Num fake examples 26984 Num true examples 28456\n",
      "  Batch 27,760  of  44,637.    Elapsed: 0:17:08. Training loss. 0.004181587137281895 Num fake examples 27025 Num true examples 28495\n",
      "  Batch 27,800  of  44,637.    Elapsed: 0:17:09. Training loss. 0.0066461144015192986 Num fake examples 27071 Num true examples 28529\n",
      "  Batch 27,840  of  44,637.    Elapsed: 0:17:11. Training loss. 0.005706688389182091 Num fake examples 27109 Num true examples 28571\n",
      "  Batch 27,880  of  44,637.    Elapsed: 0:17:12. Training loss. 0.005883051548153162 Num fake examples 27152 Num true examples 28608\n",
      "  Batch 27,920  of  44,637.    Elapsed: 0:17:13. Training loss. 0.004874766804277897 Num fake examples 27188 Num true examples 28652\n",
      "  Batch 27,960  of  44,637.    Elapsed: 0:17:15. Training loss. 0.004240978043526411 Num fake examples 27234 Num true examples 28686\n",
      "  Batch 28,000  of  44,637.    Elapsed: 0:17:16. Training loss. 0.00444856146350503 Num fake examples 27278 Num true examples 28722\n",
      "  Batch 28,040  of  44,637.    Elapsed: 0:17:18. Training loss. 0.003311176784336567 Num fake examples 27318 Num true examples 28762\n",
      "  Batch 28,080  of  44,637.    Elapsed: 0:17:19. Training loss. 2.6272690296173096 Num fake examples 27348 Num true examples 28812\n",
      "  Batch 28,120  of  44,637.    Elapsed: 0:17:21. Training loss. 0.004094172269105911 Num fake examples 27379 Num true examples 28861\n",
      "  Batch 28,160  of  44,637.    Elapsed: 0:17:22. Training loss. 0.0027034247759729624 Num fake examples 27422 Num true examples 28898\n",
      "  Batch 28,200  of  44,637.    Elapsed: 0:17:24. Training loss. 0.0028718155808746815 Num fake examples 27458 Num true examples 28942\n",
      "  Batch 28,240  of  44,637.    Elapsed: 0:17:25. Training loss. 0.004067535977810621 Num fake examples 27500 Num true examples 28980\n",
      "  Batch 28,280  of  44,637.    Elapsed: 0:17:27. Training loss. 0.0027401710394769907 Num fake examples 27540 Num true examples 29020\n",
      "  Batch 28,320  of  44,637.    Elapsed: 0:17:28. Training loss. 2.625230550765991 Num fake examples 27591 Num true examples 29049\n",
      "  Batch 28,360  of  44,637.    Elapsed: 0:17:30. Training loss. 0.0024246841203421354 Num fake examples 27638 Num true examples 29082\n",
      "  Batch 28,400  of  44,637.    Elapsed: 0:17:31. Training loss. 0.003918908536434174 Num fake examples 27675 Num true examples 29125\n",
      "  Batch 28,440  of  44,637.    Elapsed: 0:17:33. Training loss. 0.0019379311706870794 Num fake examples 27721 Num true examples 29159\n",
      "  Batch 28,480  of  44,637.    Elapsed: 0:17:34. Training loss. 0.0026047166902571917 Num fake examples 27756 Num true examples 29204\n",
      "  Batch 28,520  of  44,637.    Elapsed: 0:17:36. Training loss. 0.006007262971252203 Num fake examples 27791 Num true examples 29249\n",
      "  Batch 28,560  of  44,637.    Elapsed: 0:17:37. Training loss. 0.006485277786850929 Num fake examples 27830 Num true examples 29290\n",
      "  Batch 28,600  of  44,637.    Elapsed: 0:17:39. Training loss. 0.004373252857476473 Num fake examples 27866 Num true examples 29334\n",
      "  Batch 28,640  of  44,637.    Elapsed: 0:17:40. Training loss. 2.582857608795166 Num fake examples 27905 Num true examples 29375\n",
      "  Batch 28,680  of  44,637.    Elapsed: 0:17:42. Training loss. 0.004567607771605253 Num fake examples 27945 Num true examples 29415\n",
      "  Batch 28,720  of  44,637.    Elapsed: 0:17:43. Training loss. 0.004057897254824638 Num fake examples 27987 Num true examples 29453\n",
      "  Batch 28,760  of  44,637.    Elapsed: 0:17:45. Training loss. 0.005544556304812431 Num fake examples 28031 Num true examples 29489\n",
      "  Batch 28,800  of  44,637.    Elapsed: 0:17:46. Training loss. 0.004678185097873211 Num fake examples 28072 Num true examples 29528\n",
      "  Batch 28,840  of  44,637.    Elapsed: 0:17:48. Training loss. 0.003285748418420553 Num fake examples 28108 Num true examples 29572\n",
      "  Batch 28,880  of  44,637.    Elapsed: 0:17:49. Training loss. 0.004589994437992573 Num fake examples 28147 Num true examples 29613\n",
      "  Batch 28,920  of  44,637.    Elapsed: 0:17:51. Training loss. 0.003444922622293234 Num fake examples 28183 Num true examples 29657\n",
      "  Batch 28,960  of  44,637.    Elapsed: 0:17:52. Training loss. 0.0034549953415989876 Num fake examples 28220 Num true examples 29700\n",
      "  Batch 29,000  of  44,637.    Elapsed: 0:17:54. Training loss. 3.1209707260131836 Num fake examples 28251 Num true examples 29749\n",
      "  Batch 29,040  of  44,637.    Elapsed: 0:17:55. Training loss. 0.004183446988463402 Num fake examples 28291 Num true examples 29789\n",
      "  Batch 29,080  of  44,637.    Elapsed: 0:17:57. Training loss. 0.0029904665425419807 Num fake examples 28325 Num true examples 29835\n",
      "  Batch 29,120  of  44,637.    Elapsed: 0:17:58. Training loss. 0.003458523191511631 Num fake examples 28361 Num true examples 29879\n",
      "  Batch 29,160  of  44,637.    Elapsed: 0:17:59. Training loss. 0.002057911129668355 Num fake examples 28396 Num true examples 29924\n",
      "  Batch 29,200  of  44,637.    Elapsed: 0:18:01. Training loss. 0.003355370368808508 Num fake examples 28435 Num true examples 29965\n",
      "  Batch 29,240  of  44,637.    Elapsed: 0:18:02. Training loss. 0.0024415203370153904 Num fake examples 28474 Num true examples 30006\n",
      "  Batch 29,280  of  44,637.    Elapsed: 0:18:04. Training loss. 0.002812975784763694 Num fake examples 28508 Num true examples 30052\n",
      "  Batch 29,320  of  44,637.    Elapsed: 0:18:05. Training loss. 0.003525158856064081 Num fake examples 28560 Num true examples 30080\n",
      "  Batch 29,360  of  44,637.    Elapsed: 0:18:07. Training loss. 0.0031618759967386723 Num fake examples 28607 Num true examples 30113\n",
      "  Batch 29,400  of  44,637.    Elapsed: 0:18:08. Training loss. 0.002469716127961874 Num fake examples 28640 Num true examples 30160\n",
      "  Batch 29,440  of  44,637.    Elapsed: 0:18:10. Training loss. 0.0022986901458352804 Num fake examples 28685 Num true examples 30195\n",
      "  Batch 29,480  of  44,637.    Elapsed: 0:18:11. Training loss. 0.0017437199130654335 Num fake examples 28722 Num true examples 30238\n",
      "  Batch 29,520  of  44,637.    Elapsed: 0:18:13. Training loss. 0.002523998497053981 Num fake examples 28765 Num true examples 30275\n",
      "  Batch 29,560  of  44,637.    Elapsed: 0:18:14. Training loss. 0.00255126366391778 Num fake examples 28804 Num true examples 30316\n",
      "  Batch 29,600  of  44,637.    Elapsed: 0:18:16. Training loss. 0.002674509771168232 Num fake examples 28845 Num true examples 30355\n",
      "  Batch 29,640  of  44,637.    Elapsed: 0:18:17. Training loss. 0.003493632422760129 Num fake examples 28878 Num true examples 30402\n",
      "  Batch 29,680  of  44,637.    Elapsed: 0:18:19. Training loss. 2.7873048782348633 Num fake examples 28921 Num true examples 30439\n",
      "  Batch 29,720  of  44,637.    Elapsed: 0:18:20. Training loss. 0.007492161821573973 Num fake examples 28963 Num true examples 30477\n",
      "  Batch 29,760  of  44,637.    Elapsed: 0:18:22. Training loss. 0.005662302486598492 Num fake examples 28998 Num true examples 30522\n",
      "  Batch 29,800  of  44,637.    Elapsed: 0:18:23. Training loss. 0.002608475275337696 Num fake examples 29028 Num true examples 30572\n",
      "  Batch 29,840  of  44,637.    Elapsed: 0:18:25. Training loss. 0.007007970474660397 Num fake examples 29069 Num true examples 30611\n",
      "  Batch 29,880  of  44,637.    Elapsed: 0:18:26. Training loss. 0.0028193483594805002 Num fake examples 29105 Num true examples 30655\n",
      "  Batch 29,920  of  44,637.    Elapsed: 0:18:28. Training loss. 0.0019259373657405376 Num fake examples 29144 Num true examples 30696\n",
      "  Batch 29,960  of  44,637.    Elapsed: 0:18:29. Training loss. 0.001423860783688724 Num fake examples 29176 Num true examples 30744\n",
      "  Batch 30,000  of  44,637.    Elapsed: 0:18:30. Training loss. 0.0017201247392222285 Num fake examples 29214 Num true examples 30786\n",
      "  Batch 30,040  of  44,637.    Elapsed: 0:18:32. Training loss. 0.003938682377338409 Num fake examples 29247 Num true examples 30833\n",
      "  Batch 30,080  of  44,637.    Elapsed: 0:18:33. Training loss. 0.0018593610730022192 Num fake examples 29284 Num true examples 30876\n",
      "  Batch 30,120  of  44,637.    Elapsed: 0:18:35. Training loss. 0.002300495281815529 Num fake examples 29323 Num true examples 30917\n",
      "  Batch 30,160  of  44,637.    Elapsed: 0:18:36. Training loss. 0.004586471244692802 Num fake examples 29359 Num true examples 30961\n",
      "  Batch 30,200  of  44,637.    Elapsed: 0:18:38. Training loss. 0.00267592235468328 Num fake examples 29392 Num true examples 31008\n",
      "  Batch 30,240  of  44,637.    Elapsed: 0:18:40. Training loss. 0.0021626579109579325 Num fake examples 29433 Num true examples 31047\n",
      "  Batch 30,280  of  44,637.    Elapsed: 0:18:41. Training loss. 0.003082095645368099 Num fake examples 29471 Num true examples 31089\n",
      "  Batch 30,320  of  44,637.    Elapsed: 0:18:43. Training loss. 0.0015513970283791423 Num fake examples 29510 Num true examples 31130\n",
      "  Batch 30,360  of  44,637.    Elapsed: 0:18:44. Training loss. 0.003331297542899847 Num fake examples 29547 Num true examples 31173\n",
      "  Batch 30,400  of  44,637.    Elapsed: 0:18:45. Training loss. 0.002764785662293434 Num fake examples 29584 Num true examples 31216\n",
      "  Batch 30,440  of  44,637.    Elapsed: 0:18:47. Training loss. 0.0037067814264446497 Num fake examples 29616 Num true examples 31264\n",
      "  Batch 30,480  of  44,637.    Elapsed: 0:18:48. Training loss. 0.0039056111127138138 Num fake examples 29646 Num true examples 31314\n",
      "  Batch 30,520  of  44,637.    Elapsed: 0:18:50. Training loss. 0.002483299234881997 Num fake examples 29682 Num true examples 31358\n",
      "  Batch 30,560  of  44,637.    Elapsed: 0:18:51. Training loss. 0.0031236528884619474 Num fake examples 29709 Num true examples 31411\n",
      "  Batch 30,600  of  44,637.    Elapsed: 0:18:53. Training loss. 0.004146498162299395 Num fake examples 29756 Num true examples 31444\n",
      "  Batch 30,640  of  44,637.    Elapsed: 0:18:54. Training loss. 0.00494556687772274 Num fake examples 29798 Num true examples 31482\n",
      "  Batch 30,680  of  44,637.    Elapsed: 0:18:56. Training loss. 0.0013438232708722353 Num fake examples 29841 Num true examples 31519\n",
      "  Batch 30,720  of  44,637.    Elapsed: 0:18:57. Training loss. 0.004279419779777527 Num fake examples 29882 Num true examples 31558\n",
      "  Batch 30,760  of  44,637.    Elapsed: 0:18:59. Training loss. 0.0019318382255733013 Num fake examples 29917 Num true examples 31603\n",
      "  Batch 30,800  of  44,637.    Elapsed: 0:19:00. Training loss. 0.001969698118045926 Num fake examples 29958 Num true examples 31642\n",
      "  Batch 30,840  of  44,637.    Elapsed: 0:19:02. Training loss. 0.001501089776866138 Num fake examples 29989 Num true examples 31691\n",
      "  Batch 30,880  of  44,637.    Elapsed: 0:19:03. Training loss. 0.002068354282528162 Num fake examples 30035 Num true examples 31725\n",
      "  Batch 30,920  of  44,637.    Elapsed: 0:19:05. Training loss. 3.0547776222229004 Num fake examples 30076 Num true examples 31764\n",
      "  Batch 30,960  of  44,637.    Elapsed: 0:19:06. Training loss. 0.0027113950345665216 Num fake examples 30112 Num true examples 31808\n",
      "  Batch 31,000  of  44,637.    Elapsed: 0:19:08. Training loss. 0.002641580067574978 Num fake examples 30149 Num true examples 31851\n",
      "  Batch 31,040  of  44,637.    Elapsed: 0:19:09. Training loss. 0.0024073179811239243 Num fake examples 30175 Num true examples 31905\n",
      "  Batch 31,080  of  44,637.    Elapsed: 0:19:11. Training loss. 0.0019623038824647665 Num fake examples 30215 Num true examples 31945\n",
      "  Batch 31,120  of  44,637.    Elapsed: 0:19:12. Training loss. 0.0020365554373711348 Num fake examples 30260 Num true examples 31980\n",
      "  Batch 31,160  of  44,637.    Elapsed: 0:19:14. Training loss. 0.003079542890191078 Num fake examples 30297 Num true examples 32023\n",
      "  Batch 31,200  of  44,637.    Elapsed: 0:19:15. Training loss. 0.002287761541083455 Num fake examples 30340 Num true examples 32060\n",
      "  Batch 31,240  of  44,637.    Elapsed: 0:19:17. Training loss. 0.004097444470971823 Num fake examples 30380 Num true examples 32100\n",
      "  Batch 31,280  of  44,637.    Elapsed: 0:19:18. Training loss. 0.004460737109184265 Num fake examples 30414 Num true examples 32146\n",
      "  Batch 31,320  of  44,637.    Elapsed: 0:19:19. Training loss. 0.004152678418904543 Num fake examples 30447 Num true examples 32193\n",
      "  Batch 31,360  of  44,637.    Elapsed: 0:19:21. Training loss. 0.0023262137547135353 Num fake examples 30488 Num true examples 32232\n",
      "  Batch 31,400  of  44,637.    Elapsed: 0:19:22. Training loss. 0.0020081656984984875 Num fake examples 30521 Num true examples 32279\n",
      "  Batch 31,440  of  44,637.    Elapsed: 0:19:24. Training loss. 0.0018077550921589136 Num fake examples 30548 Num true examples 32332\n",
      "  Batch 31,480  of  44,637.    Elapsed: 0:19:25. Training loss. 0.0015297298086807132 Num fake examples 30586 Num true examples 32374\n",
      "  Batch 31,520  of  44,637.    Elapsed: 0:19:27. Training loss. 0.0026663723401725292 Num fake examples 30626 Num true examples 32414\n",
      "  Batch 31,560  of  44,637.    Elapsed: 0:19:28. Training loss. 0.002328784903511405 Num fake examples 30666 Num true examples 32454\n",
      "  Batch 31,600  of  44,637.    Elapsed: 0:19:30. Training loss. 0.0020319297909736633 Num fake examples 30703 Num true examples 32497\n",
      "  Batch 31,640  of  44,637.    Elapsed: 0:19:31. Training loss. 0.003210389520972967 Num fake examples 30748 Num true examples 32532\n",
      "  Batch 31,680  of  44,637.    Elapsed: 0:19:33. Training loss. 0.0033969860523939133 Num fake examples 30784 Num true examples 32576\n",
      "  Batch 31,720  of  44,637.    Elapsed: 0:19:34. Training loss. 2.9294273853302 Num fake examples 30828 Num true examples 32612\n",
      "  Batch 31,760  of  44,637.    Elapsed: 0:19:36. Training loss. 0.00225432263687253 Num fake examples 30870 Num true examples 32650\n",
      "  Batch 31,800  of  44,637.    Elapsed: 0:19:37. Training loss. 0.003333087544888258 Num fake examples 30904 Num true examples 32696\n",
      "  Batch 31,840  of  44,637.    Elapsed: 0:19:39. Training loss. 0.0036439409013837576 Num fake examples 30945 Num true examples 32735\n",
      "  Batch 31,880  of  44,637.    Elapsed: 0:19:40. Training loss. 0.002730047330260277 Num fake examples 30984 Num true examples 32776\n",
      "  Batch 31,920  of  44,637.    Elapsed: 0:19:42. Training loss. 0.00323348306119442 Num fake examples 31026 Num true examples 32814\n",
      "  Batch 31,960  of  44,637.    Elapsed: 0:19:43. Training loss. 0.0029521312098950148 Num fake examples 31065 Num true examples 32855\n",
      "  Batch 32,000  of  44,637.    Elapsed: 0:19:45. Training loss. 0.002887025708332658 Num fake examples 31097 Num true examples 32903\n",
      "  Batch 32,040  of  44,637.    Elapsed: 0:19:46. Training loss. 0.0027235604356974363 Num fake examples 31134 Num true examples 32946\n",
      "  Batch 32,080  of  44,637.    Elapsed: 0:19:48. Training loss. 0.0029186243191361427 Num fake examples 31178 Num true examples 32982\n",
      "  Batch 32,120  of  44,637.    Elapsed: 0:19:49. Training loss. 0.002024755347520113 Num fake examples 31223 Num true examples 33017\n",
      "  Batch 32,160  of  44,637.    Elapsed: 0:19:50. Training loss. 0.00189373386092484 Num fake examples 31263 Num true examples 33057\n",
      "  Batch 32,200  of  44,637.    Elapsed: 0:19:52. Training loss. 0.002518302761018276 Num fake examples 31303 Num true examples 33097\n",
      "  Batch 32,240  of  44,637.    Elapsed: 0:19:53. Training loss. 0.0024363920092582703 Num fake examples 31340 Num true examples 33140\n",
      "  Batch 32,280  of  44,637.    Elapsed: 0:19:55. Training loss. 0.0028268678579479456 Num fake examples 31371 Num true examples 33189\n",
      "  Batch 32,320  of  44,637.    Elapsed: 0:19:56. Training loss. 0.002188039943575859 Num fake examples 31416 Num true examples 33224\n",
      "  Batch 32,360  of  44,637.    Elapsed: 0:19:58. Training loss. 0.0022279773838818073 Num fake examples 31455 Num true examples 33265\n",
      "  Batch 32,400  of  44,637.    Elapsed: 0:19:59. Training loss. 0.00184662698302418 Num fake examples 31495 Num true examples 33305\n",
      "  Batch 32,440  of  44,637.    Elapsed: 0:20:01. Training loss. 0.0022759605199098587 Num fake examples 31535 Num true examples 33345\n",
      "  Batch 32,480  of  44,637.    Elapsed: 0:20:02. Training loss. 0.0017583867302164435 Num fake examples 31570 Num true examples 33390\n",
      "  Batch 32,520  of  44,637.    Elapsed: 0:20:04. Training loss. 0.002240282017737627 Num fake examples 31610 Num true examples 33430\n",
      "  Batch 32,560  of  44,637.    Elapsed: 0:20:05. Training loss. 0.0033455267548561096 Num fake examples 31651 Num true examples 33469\n",
      "  Batch 32,600  of  44,637.    Elapsed: 0:20:07. Training loss. 2.8260154724121094 Num fake examples 31682 Num true examples 33518\n",
      "  Batch 32,640  of  44,637.    Elapsed: 0:20:08. Training loss. 0.003771732794120908 Num fake examples 31717 Num true examples 33563\n",
      "  Batch 32,680  of  44,637.    Elapsed: 0:20:10. Training loss. 0.003460942767560482 Num fake examples 31755 Num true examples 33605\n",
      "  Batch 32,720  of  44,637.    Elapsed: 0:20:11. Training loss. 0.0038639786653220654 Num fake examples 31786 Num true examples 33654\n",
      "  Batch 32,760  of  44,637.    Elapsed: 0:20:13. Training loss. 0.005095184780657291 Num fake examples 31814 Num true examples 33706\n",
      "  Batch 32,800  of  44,637.    Elapsed: 0:20:14. Training loss. 0.004670326132327318 Num fake examples 31852 Num true examples 33748\n",
      "  Batch 32,840  of  44,637.    Elapsed: 0:20:16. Training loss. 0.004040258005261421 Num fake examples 31896 Num true examples 33784\n",
      "  Batch 32,880  of  44,637.    Elapsed: 0:20:17. Training loss. 0.004346450790762901 Num fake examples 31934 Num true examples 33826\n",
      "  Batch 32,920  of  44,637.    Elapsed: 0:20:19. Training loss. 0.003578957635909319 Num fake examples 31985 Num true examples 33855\n",
      "  Batch 32,960  of  44,637.    Elapsed: 0:20:20. Training loss. 0.002850345801562071 Num fake examples 32019 Num true examples 33901\n",
      "  Batch 33,000  of  44,637.    Elapsed: 0:20:22. Training loss. 0.0031351223587989807 Num fake examples 32066 Num true examples 33934\n",
      "  Batch 33,040  of  44,637.    Elapsed: 0:20:23. Training loss. 0.0041866181418299675 Num fake examples 32109 Num true examples 33971\n",
      "  Batch 33,080  of  44,637.    Elapsed: 0:20:25. Training loss. 0.0035973500926047564 Num fake examples 32148 Num true examples 34012\n",
      "  Batch 33,120  of  44,637.    Elapsed: 0:20:26. Training loss. 0.004049322102218866 Num fake examples 32194 Num true examples 34046\n",
      "  Batch 33,160  of  44,637.    Elapsed: 0:20:27. Training loss. 0.0038611486088484526 Num fake examples 32236 Num true examples 34084\n",
      "  Batch 33,200  of  44,637.    Elapsed: 0:20:29. Training loss. 0.003709175856783986 Num fake examples 32270 Num true examples 34130\n",
      "  Batch 33,240  of  44,637.    Elapsed: 0:20:30. Training loss. 0.004325572866946459 Num fake examples 32309 Num true examples 34171\n",
      "  Batch 33,280  of  44,637.    Elapsed: 0:20:32. Training loss. 0.002609646413475275 Num fake examples 32348 Num true examples 34212\n",
      "  Batch 33,320  of  44,637.    Elapsed: 0:20:33. Training loss. 0.004970151465386152 Num fake examples 32386 Num true examples 34254\n",
      "  Batch 33,360  of  44,637.    Elapsed: 0:20:35. Training loss. 0.0035591446794569492 Num fake examples 32421 Num true examples 34299\n",
      "  Batch 33,400  of  44,637.    Elapsed: 0:20:36. Training loss. 0.004401201847940683 Num fake examples 32459 Num true examples 34341\n",
      "  Batch 33,440  of  44,637.    Elapsed: 0:20:38. Training loss. 0.003381076268851757 Num fake examples 32493 Num true examples 34387\n",
      "  Batch 33,480  of  44,637.    Elapsed: 0:20:39. Training loss. 0.004272423684597015 Num fake examples 32525 Num true examples 34435\n",
      "  Batch 33,520  of  44,637.    Elapsed: 0:20:41. Training loss. 0.003201231360435486 Num fake examples 32562 Num true examples 34478\n",
      "  Batch 33,560  of  44,637.    Elapsed: 0:20:42. Training loss. 0.0023799999617040157 Num fake examples 32592 Num true examples 34528\n",
      "  Batch 33,600  of  44,637.    Elapsed: 0:20:44. Training loss. 0.00443948432803154 Num fake examples 32635 Num true examples 34565\n",
      "  Batch 33,640  of  44,637.    Elapsed: 0:20:45. Training loss. 0.003647038247436285 Num fake examples 32678 Num true examples 34602\n",
      "  Batch 33,680  of  44,637.    Elapsed: 0:20:47. Training loss. 0.004457952920347452 Num fake examples 32718 Num true examples 34642\n",
      "  Batch 33,720  of  44,637.    Elapsed: 0:20:48. Training loss. 0.002575366757810116 Num fake examples 32765 Num true examples 34675\n",
      "  Batch 33,760  of  44,637.    Elapsed: 0:20:50. Training loss. 0.00205808412283659 Num fake examples 32804 Num true examples 34716\n",
      "  Batch 33,800  of  44,637.    Elapsed: 0:20:51. Training loss. 0.0014756211312487721 Num fake examples 32848 Num true examples 34752\n",
      "  Batch 33,840  of  44,637.    Elapsed: 0:20:53. Training loss. 0.0031623104587197304 Num fake examples 32888 Num true examples 34792\n",
      "  Batch 33,880  of  44,637.    Elapsed: 0:20:54. Training loss. 0.0030204232316464186 Num fake examples 32931 Num true examples 34829\n",
      "  Batch 33,920  of  44,637.    Elapsed: 0:20:56. Training loss. 0.0052634417079389095 Num fake examples 32974 Num true examples 34866\n",
      "  Batch 33,960  of  44,637.    Elapsed: 0:20:57. Training loss. 0.0035805897787213326 Num fake examples 33009 Num true examples 34911\n",
      "  Batch 34,000  of  44,637.    Elapsed: 0:20:59. Training loss. 0.004151307046413422 Num fake examples 33047 Num true examples 34953\n",
      "  Batch 34,040  of  44,637.    Elapsed: 0:21:00. Training loss. 0.0028770340140908957 Num fake examples 33088 Num true examples 34992\n",
      "  Batch 34,080  of  44,637.    Elapsed: 0:21:01. Training loss. 0.0026058233343064785 Num fake examples 33119 Num true examples 35041\n",
      "  Batch 34,120  of  44,637.    Elapsed: 0:21:03. Training loss. 0.002491371240466833 Num fake examples 33151 Num true examples 35089\n",
      "  Batch 34,160  of  44,637.    Elapsed: 0:21:04. Training loss. 0.0030561452731490135 Num fake examples 33192 Num true examples 35128\n",
      "  Batch 34,200  of  44,637.    Elapsed: 0:21:06. Training loss. 0.002099475357681513 Num fake examples 33225 Num true examples 35175\n",
      "  Batch 34,240  of  44,637.    Elapsed: 0:21:07. Training loss. 0.003953003324568272 Num fake examples 33275 Num true examples 35205\n",
      "  Batch 34,280  of  44,637.    Elapsed: 0:21:09. Training loss. 0.00366949918679893 Num fake examples 33321 Num true examples 35239\n",
      "  Batch 34,320  of  44,637.    Elapsed: 0:21:10. Training loss. 0.004006512463092804 Num fake examples 33360 Num true examples 35280\n",
      "  Batch 34,360  of  44,637.    Elapsed: 0:21:12. Training loss. 0.0032838715706020594 Num fake examples 33401 Num true examples 35319\n",
      "  Batch 34,400  of  44,637.    Elapsed: 0:21:13. Training loss. 0.0036562925670295954 Num fake examples 33439 Num true examples 35361\n",
      "  Batch 34,440  of  44,637.    Elapsed: 0:21:15. Training loss. 0.0034715342335402966 Num fake examples 33474 Num true examples 35406\n",
      "  Batch 34,480  of  44,637.    Elapsed: 0:21:16. Training loss. 0.004678111523389816 Num fake examples 33507 Num true examples 35453\n",
      "  Batch 34,520  of  44,637.    Elapsed: 0:21:18. Training loss. 0.00395174790173769 Num fake examples 33550 Num true examples 35490\n",
      "  Batch 34,560  of  44,637.    Elapsed: 0:21:19. Training loss. 0.003591926535591483 Num fake examples 33593 Num true examples 35527\n",
      "  Batch 34,600  of  44,637.    Elapsed: 0:21:21. Training loss. 0.004684635438024998 Num fake examples 33630 Num true examples 35570\n",
      "  Batch 34,640  of  44,637.    Elapsed: 0:21:22. Training loss. 0.005360074806958437 Num fake examples 33668 Num true examples 35612\n",
      "  Batch 34,680  of  44,637.    Elapsed: 0:21:24. Training loss. 0.005099944770336151 Num fake examples 33709 Num true examples 35651\n",
      "  Batch 34,720  of  44,637.    Elapsed: 0:21:25. Training loss. 0.004227618221193552 Num fake examples 33757 Num true examples 35683\n",
      "  Batch 34,760  of  44,637.    Elapsed: 0:21:27. Training loss. 0.004559163935482502 Num fake examples 33796 Num true examples 35724\n",
      "  Batch 34,800  of  44,637.    Elapsed: 0:21:28. Training loss. 0.004855308681726456 Num fake examples 33839 Num true examples 35761\n",
      "  Batch 34,840  of  44,637.    Elapsed: 0:21:30. Training loss. 0.004262552596628666 Num fake examples 33877 Num true examples 35803\n",
      "  Batch 34,880  of  44,637.    Elapsed: 0:21:31. Training loss. 3.007673501968384 Num fake examples 33920 Num true examples 35840\n",
      "  Batch 34,920  of  44,637.    Elapsed: 0:21:33. Training loss. 0.0030279108323156834 Num fake examples 33963 Num true examples 35877\n",
      "  Batch 34,960  of  44,637.    Elapsed: 0:21:34. Training loss. 0.004580933600664139 Num fake examples 33998 Num true examples 35922\n",
      "  Batch 35,000  of  44,637.    Elapsed: 0:21:36. Training loss. 0.0014198150020092726 Num fake examples 34038 Num true examples 35962\n",
      "  Batch 35,040  of  44,637.    Elapsed: 0:21:37. Training loss. 3.0913217067718506 Num fake examples 34074 Num true examples 36006\n",
      "  Batch 35,080  of  44,637.    Elapsed: 0:21:39. Training loss. 2.7739455699920654 Num fake examples 34114 Num true examples 36046\n",
      "  Batch 35,120  of  44,637.    Elapsed: 0:21:40. Training loss. 0.0036230357363820076 Num fake examples 34158 Num true examples 36082\n",
      "  Batch 35,160  of  44,637.    Elapsed: 0:21:42. Training loss. 0.0034712236374616623 Num fake examples 34203 Num true examples 36117\n",
      "  Batch 35,200  of  44,637.    Elapsed: 0:21:43. Training loss. 0.002364447806030512 Num fake examples 34239 Num true examples 36161\n",
      "  Batch 35,240  of  44,637.    Elapsed: 0:21:45. Training loss. 2.8872408866882324 Num fake examples 34281 Num true examples 36199\n",
      "  Batch 35,280  of  44,637.    Elapsed: 0:21:46. Training loss. 2.640432357788086 Num fake examples 34320 Num true examples 36240\n",
      "  Batch 35,320  of  44,637.    Elapsed: 0:21:47. Training loss. 0.004665870685130358 Num fake examples 34359 Num true examples 36281\n",
      "  Batch 35,360  of  44,637.    Elapsed: 0:21:49. Training loss. 0.004332322161644697 Num fake examples 34400 Num true examples 36320\n",
      "  Batch 35,400  of  44,637.    Elapsed: 0:21:50. Training loss. 0.0044619180262088776 Num fake examples 34447 Num true examples 36353\n",
      "  Batch 35,440  of  44,637.    Elapsed: 0:21:52. Training loss. 0.004964446183294058 Num fake examples 34483 Num true examples 36397\n",
      "  Batch 35,480  of  44,637.    Elapsed: 0:21:53. Training loss. 0.002902394160628319 Num fake examples 34528 Num true examples 36432\n",
      "  Batch 35,520  of  44,637.    Elapsed: 0:21:55. Training loss. 0.003082320559769869 Num fake examples 34567 Num true examples 36473\n",
      "  Batch 35,560  of  44,637.    Elapsed: 0:21:56. Training loss. 0.0031244875863194466 Num fake examples 34615 Num true examples 36505\n",
      "  Batch 35,600  of  44,637.    Elapsed: 0:21:58. Training loss. 0.002460101619362831 Num fake examples 34646 Num true examples 36554\n",
      "  Batch 35,640  of  44,637.    Elapsed: 0:21:59. Training loss. 0.00471209641546011 Num fake examples 34687 Num true examples 36593\n",
      "  Batch 35,680  of  44,637.    Elapsed: 0:22:01. Training loss. 2.8524537086486816 Num fake examples 34724 Num true examples 36636\n",
      "  Batch 35,720  of  44,637.    Elapsed: 0:22:02. Training loss. 0.002826453186571598 Num fake examples 34771 Num true examples 36669\n",
      "  Batch 35,760  of  44,637.    Elapsed: 0:22:04. Training loss. 0.002677817130461335 Num fake examples 34812 Num true examples 36708\n",
      "  Batch 35,800  of  44,637.    Elapsed: 0:22:05. Training loss. 0.003562456462532282 Num fake examples 34856 Num true examples 36744\n",
      "  Batch 35,840  of  44,637.    Elapsed: 0:22:07. Training loss. 0.0041832393035292625 Num fake examples 34900 Num true examples 36780\n",
      "  Batch 35,880  of  44,637.    Elapsed: 0:22:08. Training loss. 0.002378700766712427 Num fake examples 34939 Num true examples 36821\n",
      "  Batch 35,920  of  44,637.    Elapsed: 0:22:10. Training loss. 0.0017566847382113338 Num fake examples 34981 Num true examples 36859\n",
      "  Batch 35,960  of  44,637.    Elapsed: 0:22:11. Training loss. 0.0028268727473914623 Num fake examples 35018 Num true examples 36902\n",
      "  Batch 36,000  of  44,637.    Elapsed: 0:22:13. Training loss. 0.0023961213883012533 Num fake examples 35055 Num true examples 36945\n",
      "  Batch 36,040  of  44,637.    Elapsed: 0:22:14. Training loss. 0.00196961872279644 Num fake examples 35093 Num true examples 36987\n",
      "  Batch 36,080  of  44,637.    Elapsed: 0:22:16. Training loss. 0.002742178738117218 Num fake examples 35134 Num true examples 37026\n",
      "  Batch 36,120  of  44,637.    Elapsed: 0:22:17. Training loss. 0.003852153429761529 Num fake examples 35170 Num true examples 37070\n",
      "  Batch 36,160  of  44,637.    Elapsed: 0:22:18. Training loss. 0.005114313215017319 Num fake examples 35209 Num true examples 37111\n",
      "  Batch 36,200  of  44,637.    Elapsed: 0:22:20. Training loss. 0.0042220065370202065 Num fake examples 35250 Num true examples 37150\n",
      "  Batch 36,240  of  44,637.    Elapsed: 0:22:22. Training loss. 0.002960462123155594 Num fake examples 35288 Num true examples 37192\n",
      "  Batch 36,280  of  44,637.    Elapsed: 0:22:23. Training loss. 0.0036039073020219803 Num fake examples 35322 Num true examples 37238\n",
      "  Batch 36,320  of  44,637.    Elapsed: 0:22:25. Training loss. 0.0025380714796483517 Num fake examples 35361 Num true examples 37279\n",
      "  Batch 36,360  of  44,637.    Elapsed: 0:22:26. Training loss. 0.0032587265595793724 Num fake examples 35401 Num true examples 37319\n",
      "  Batch 36,400  of  44,637.    Elapsed: 0:22:28. Training loss. 0.003897994989529252 Num fake examples 35439 Num true examples 37361\n",
      "  Batch 36,440  of  44,637.    Elapsed: 0:22:29. Training loss. 0.003139179665595293 Num fake examples 35477 Num true examples 37403\n",
      "  Batch 36,480  of  44,637.    Elapsed: 0:22:31. Training loss. 0.002470203675329685 Num fake examples 35522 Num true examples 37438\n",
      "  Batch 36,520  of  44,637.    Elapsed: 0:22:32. Training loss. 0.0022067928221076727 Num fake examples 35567 Num true examples 37473\n",
      "  Batch 36,560  of  44,637.    Elapsed: 0:22:34. Training loss. 0.0026509827002882957 Num fake examples 35606 Num true examples 37514\n",
      "  Batch 36,600  of  44,637.    Elapsed: 0:22:35. Training loss. 0.0027863371651619673 Num fake examples 35643 Num true examples 37557\n",
      "  Batch 36,640  of  44,637.    Elapsed: 0:22:37. Training loss. 0.002089854795485735 Num fake examples 35681 Num true examples 37599\n",
      "  Batch 36,680  of  44,637.    Elapsed: 0:22:38. Training loss. 0.001598939998075366 Num fake examples 35717 Num true examples 37643\n",
      "  Batch 36,720  of  44,637.    Elapsed: 0:22:39. Training loss. 0.0031650192104279995 Num fake examples 35758 Num true examples 37682\n",
      "  Batch 36,760  of  44,637.    Elapsed: 0:22:41. Training loss. 2.8723056316375732 Num fake examples 35797 Num true examples 37723\n",
      "  Batch 36,800  of  44,637.    Elapsed: 0:22:42. Training loss. 0.0027798176743090153 Num fake examples 35835 Num true examples 37765\n",
      "  Batch 36,840  of  44,637.    Elapsed: 0:22:44. Training loss. 0.0025838837027549744 Num fake examples 35879 Num true examples 37801\n",
      "  Batch 36,880  of  44,637.    Elapsed: 0:22:45. Training loss. 0.004190689884126186 Num fake examples 35921 Num true examples 37839\n",
      "  Batch 36,920  of  44,637.    Elapsed: 0:22:47. Training loss. 0.002512858249247074 Num fake examples 35965 Num true examples 37875\n",
      "  Batch 36,960  of  44,637.    Elapsed: 0:22:48. Training loss. 0.0044972775503993034 Num fake examples 36002 Num true examples 37918\n",
      "  Batch 37,000  of  44,637.    Elapsed: 0:22:50. Training loss. 3.4259798526763916 Num fake examples 36046 Num true examples 37954\n",
      "  Batch 37,040  of  44,637.    Elapsed: 0:22:51. Training loss. 0.00362710515037179 Num fake examples 36088 Num true examples 37992\n",
      "  Batch 37,080  of  44,637.    Elapsed: 0:22:53. Training loss. 0.003446762915700674 Num fake examples 36125 Num true examples 38035\n",
      "  Batch 37,120  of  44,637.    Elapsed: 0:22:54. Training loss. 0.0027375915087759495 Num fake examples 36157 Num true examples 38083\n",
      "  Batch 37,160  of  44,637.    Elapsed: 0:22:56. Training loss. 0.0036599209997802973 Num fake examples 36196 Num true examples 38124\n",
      "  Batch 37,200  of  44,637.    Elapsed: 0:22:57. Training loss. 0.0029194331727921963 Num fake examples 36234 Num true examples 38166\n",
      "  Batch 37,240  of  44,637.    Elapsed: 0:22:59. Training loss. 0.0031173555180430412 Num fake examples 36274 Num true examples 38206\n",
      "  Batch 37,280  of  44,637.    Elapsed: 0:23:00. Training loss. 0.0024861167185008526 Num fake examples 36318 Num true examples 38242\n",
      "  Batch 37,320  of  44,637.    Elapsed: 0:23:02. Training loss. 0.002379879355430603 Num fake examples 36364 Num true examples 38276\n",
      "  Batch 37,360  of  44,637.    Elapsed: 0:23:03. Training loss. 0.001761936116963625 Num fake examples 36398 Num true examples 38322\n",
      "  Batch 37,400  of  44,637.    Elapsed: 0:23:05. Training loss. 0.003326545236632228 Num fake examples 36440 Num true examples 38360\n",
      "  Batch 37,440  of  44,637.    Elapsed: 0:23:06. Training loss. 0.0022927802056074142 Num fake examples 36483 Num true examples 38397\n",
      "  Batch 37,480  of  44,637.    Elapsed: 0:23:08. Training loss. 0.0024385196156799793 Num fake examples 36520 Num true examples 38440\n",
      "  Batch 37,520  of  44,637.    Elapsed: 0:23:09. Training loss. 0.0034689970780164003 Num fake examples 36559 Num true examples 38481\n",
      "  Batch 37,560  of  44,637.    Elapsed: 0:23:11. Training loss. 0.0022491745185106993 Num fake examples 36596 Num true examples 38524\n",
      "  Batch 37,600  of  44,637.    Elapsed: 0:23:12. Training loss. 0.0027026147581636906 Num fake examples 36630 Num true examples 38570\n",
      "  Batch 37,640  of  44,637.    Elapsed: 0:23:14. Training loss. 0.0017337675672024488 Num fake examples 36664 Num true examples 38616\n",
      "  Batch 37,680  of  44,637.    Elapsed: 0:23:15. Training loss. 0.0024089128710329533 Num fake examples 36704 Num true examples 38656\n",
      "  Batch 37,720  of  44,637.    Elapsed: 0:23:16. Training loss. 0.003025293815881014 Num fake examples 36745 Num true examples 38695\n",
      "  Batch 37,760  of  44,637.    Elapsed: 0:23:18. Training loss. 0.0025481609627604485 Num fake examples 36781 Num true examples 38739\n",
      "  Batch 37,800  of  44,637.    Elapsed: 0:23:19. Training loss. 0.0022326947655528784 Num fake examples 36822 Num true examples 38778\n",
      "  Batch 37,840  of  44,637.    Elapsed: 0:23:21. Training loss. 0.0024275637697428465 Num fake examples 36864 Num true examples 38816\n",
      "  Batch 37,880  of  44,637.    Elapsed: 0:23:22. Training loss. 0.001843682606704533 Num fake examples 36904 Num true examples 38856\n",
      "  Batch 37,920  of  44,637.    Elapsed: 0:23:24. Training loss. 0.0023706897627562284 Num fake examples 36942 Num true examples 38898\n",
      "  Batch 37,960  of  44,637.    Elapsed: 0:23:25. Training loss. 0.0017646474298089743 Num fake examples 36986 Num true examples 38934\n",
      "  Batch 38,000  of  44,637.    Elapsed: 0:23:27. Training loss. 0.0022876227740198374 Num fake examples 37020 Num true examples 38980\n",
      "  Batch 38,040  of  44,637.    Elapsed: 0:23:28. Training loss. 0.003888999344781041 Num fake examples 37054 Num true examples 39026\n",
      "  Batch 38,080  of  44,637.    Elapsed: 0:23:30. Training loss. 0.003994208760559559 Num fake examples 37090 Num true examples 39070\n",
      "  Batch 38,120  of  44,637.    Elapsed: 0:23:31. Training loss. 0.0030589490197598934 Num fake examples 37130 Num true examples 39110\n",
      "  Batch 38,160  of  44,637.    Elapsed: 0:23:33. Training loss. 0.0027183801867067814 Num fake examples 37168 Num true examples 39152\n",
      "  Batch 38,200  of  44,637.    Elapsed: 0:23:34. Training loss. 0.0022229556925594807 Num fake examples 37203 Num true examples 39197\n",
      "  Batch 38,240  of  44,637.    Elapsed: 0:23:36. Training loss. 0.0023497221991419792 Num fake examples 37236 Num true examples 39244\n",
      "  Batch 38,280  of  44,637.    Elapsed: 0:23:37. Training loss. 0.0020161462016403675 Num fake examples 37270 Num true examples 39290\n",
      "  Batch 38,320  of  44,637.    Elapsed: 0:23:39. Training loss. 0.002500338014215231 Num fake examples 37308 Num true examples 39332\n",
      "  Batch 38,360  of  44,637.    Elapsed: 0:23:40. Training loss. 0.0024196552112698555 Num fake examples 37356 Num true examples 39364\n",
      "  Batch 38,400  of  44,637.    Elapsed: 0:23:42. Training loss. 0.0018913345411419868 Num fake examples 37396 Num true examples 39404\n",
      "  Batch 38,440  of  44,637.    Elapsed: 0:23:43. Training loss. 0.0020529499743133783 Num fake examples 37439 Num true examples 39441\n",
      "  Batch 38,480  of  44,637.    Elapsed: 0:23:45. Training loss. 0.00261922599747777 Num fake examples 37481 Num true examples 39479\n",
      "  Batch 38,520  of  44,637.    Elapsed: 0:23:46. Training loss. 2.9135570526123047 Num fake examples 37519 Num true examples 39521\n",
      "  Batch 38,560  of  44,637.    Elapsed: 0:23:47. Training loss. 0.002703774254769087 Num fake examples 37554 Num true examples 39566\n",
      "  Batch 38,600  of  44,637.    Elapsed: 0:23:49. Training loss. 3.021641254425049 Num fake examples 37595 Num true examples 39605\n",
      "  Batch 38,640  of  44,637.    Elapsed: 0:23:50. Training loss. 0.0032851099967956543 Num fake examples 37630 Num true examples 39650\n",
      "  Batch 38,680  of  44,637.    Elapsed: 0:23:52. Training loss. 0.001729839015752077 Num fake examples 37682 Num true examples 39678\n",
      "  Batch 38,720  of  44,637.    Elapsed: 0:23:53. Training loss. 0.00286682415753603 Num fake examples 37723 Num true examples 39717\n",
      "  Batch 38,760  of  44,637.    Elapsed: 0:23:55. Training loss. 0.0017534223152324557 Num fake examples 37766 Num true examples 39754\n",
      "  Batch 38,800  of  44,637.    Elapsed: 0:23:56. Training loss. 0.002335475292056799 Num fake examples 37801 Num true examples 39799\n",
      "  Batch 38,840  of  44,637.    Elapsed: 0:23:58. Training loss. 0.0020005565602332354 Num fake examples 37839 Num true examples 39841\n",
      "  Batch 38,880  of  44,637.    Elapsed: 0:23:59. Training loss. 0.0019364599138498306 Num fake examples 37877 Num true examples 39883\n",
      "  Batch 38,920  of  44,637.    Elapsed: 0:24:01. Training loss. 0.002573758829385042 Num fake examples 37908 Num true examples 39932\n",
      "  Batch 38,960  of  44,637.    Elapsed: 0:24:02. Training loss. 0.0025754603557288647 Num fake examples 37950 Num true examples 39970\n",
      "  Batch 39,000  of  44,637.    Elapsed: 0:24:04. Training loss. 0.002376966178417206 Num fake examples 37989 Num true examples 40011\n",
      "  Batch 39,040  of  44,637.    Elapsed: 0:24:05. Training loss. 0.0026357725728303194 Num fake examples 38037 Num true examples 40043\n",
      "  Batch 39,080  of  44,637.    Elapsed: 0:24:07. Training loss. 0.0025336346589028835 Num fake examples 38083 Num true examples 40077\n",
      "  Batch 39,120  of  44,637.    Elapsed: 0:24:08. Training loss. 0.0020226850174367428 Num fake examples 38124 Num true examples 40116\n",
      "  Batch 39,160  of  44,637.    Elapsed: 0:24:10. Training loss. 0.002399928867816925 Num fake examples 38162 Num true examples 40158\n",
      "  Batch 39,200  of  44,637.    Elapsed: 0:24:11. Training loss. 3.112147092819214 Num fake examples 38200 Num true examples 40200\n",
      "  Batch 39,240  of  44,637.    Elapsed: 0:24:13. Training loss. 0.003087718039751053 Num fake examples 38247 Num true examples 40233\n",
      "  Batch 39,280  of  44,637.    Elapsed: 0:24:14. Training loss. 0.003930371720343828 Num fake examples 38291 Num true examples 40269\n",
      "  Batch 39,320  of  44,637.    Elapsed: 0:24:16. Training loss. 0.00420620571821928 Num fake examples 38324 Num true examples 40316\n",
      "  Batch 39,360  of  44,637.    Elapsed: 0:24:17. Training loss. 0.00341349421069026 Num fake examples 38362 Num true examples 40358\n",
      "  Batch 39,400  of  44,637.    Elapsed: 0:24:19. Training loss. 0.005792224779725075 Num fake examples 38396 Num true examples 40404\n",
      "  Batch 39,440  of  44,637.    Elapsed: 0:24:20. Training loss. 0.005409096367657185 Num fake examples 38439 Num true examples 40441\n",
      "  Batch 39,480  of  44,637.    Elapsed: 0:24:22. Training loss. 0.00605913158506155 Num fake examples 38478 Num true examples 40482\n",
      "  Batch 39,520  of  44,637.    Elapsed: 0:24:23. Training loss. 0.0036811090540140867 Num fake examples 38520 Num true examples 40520\n",
      "  Batch 39,560  of  44,637.    Elapsed: 0:24:25. Training loss. 0.003512015100568533 Num fake examples 38560 Num true examples 40560\n",
      "  Batch 39,600  of  44,637.    Elapsed: 0:24:26. Training loss. 0.0034474313724786043 Num fake examples 38602 Num true examples 40598\n",
      "  Batch 39,640  of  44,637.    Elapsed: 0:24:27. Training loss. 0.003888241946697235 Num fake examples 38645 Num true examples 40635\n",
      "  Batch 39,680  of  44,637.    Elapsed: 0:24:29. Training loss. 0.003236890072003007 Num fake examples 38686 Num true examples 40674\n",
      "  Batch 39,720  of  44,637.    Elapsed: 0:24:30. Training loss. 0.0028337868861854076 Num fake examples 38725 Num true examples 40715\n",
      "  Batch 39,760  of  44,637.    Elapsed: 0:24:32. Training loss. 0.0032606301829218864 Num fake examples 38776 Num true examples 40744\n",
      "  Batch 39,800  of  44,637.    Elapsed: 0:24:33. Training loss. 0.0035942362155765295 Num fake examples 38822 Num true examples 40778\n",
      "  Batch 39,840  of  44,637.    Elapsed: 0:24:35. Training loss. 0.002866515424102545 Num fake examples 38857 Num true examples 40823\n",
      "  Batch 39,880  of  44,637.    Elapsed: 0:24:36. Training loss. 0.0027185757644474506 Num fake examples 38893 Num true examples 40867\n",
      "  Batch 39,920  of  44,637.    Elapsed: 0:24:38. Training loss. 0.0028166770935058594 Num fake examples 38930 Num true examples 40910\n",
      "  Batch 39,960  of  44,637.    Elapsed: 0:24:40. Training loss. 0.002014926401898265 Num fake examples 38965 Num true examples 40955\n",
      "  Batch 40,000  of  44,637.    Elapsed: 0:24:41. Training loss. 0.002240633126348257 Num fake examples 39004 Num true examples 40996\n",
      "  Batch 40,040  of  44,637.    Elapsed: 0:24:43. Training loss. 2.8903098106384277 Num fake examples 39037 Num true examples 41043\n",
      "  Batch 40,080  of  44,637.    Elapsed: 0:24:44. Training loss. 0.001347570912912488 Num fake examples 39081 Num true examples 41079\n",
      "  Batch 40,120  of  44,637.    Elapsed: 0:24:46. Training loss. 3.3118340969085693 Num fake examples 39118 Num true examples 41122\n",
      "  Batch 40,160  of  44,637.    Elapsed: 0:24:47. Training loss. 0.0023456879425793886 Num fake examples 39164 Num true examples 41156\n",
      "  Batch 40,200  of  44,637.    Elapsed: 0:24:49. Training loss. 0.0027034934610128403 Num fake examples 39204 Num true examples 41196\n",
      "  Batch 40,240  of  44,637.    Elapsed: 0:24:50. Training loss. 0.0033008577302098274 Num fake examples 39239 Num true examples 41241\n",
      "  Batch 40,280  of  44,637.    Elapsed: 0:24:52. Training loss. 3.0803050994873047 Num fake examples 39283 Num true examples 41277\n",
      "  Batch 40,320  of  44,637.    Elapsed: 0:24:53. Training loss. 0.0019659982062876225 Num fake examples 39319 Num true examples 41321\n",
      "  Batch 40,360  of  44,637.    Elapsed: 0:24:55. Training loss. 0.0019495063461363316 Num fake examples 39356 Num true examples 41364\n",
      "  Batch 40,400  of  44,637.    Elapsed: 0:24:56. Training loss. 0.0027762316167354584 Num fake examples 39390 Num true examples 41410\n",
      "  Batch 40,440  of  44,637.    Elapsed: 0:24:58. Training loss. 2.9950194358825684 Num fake examples 39423 Num true examples 41457\n",
      "  Batch 40,480  of  44,637.    Elapsed: 0:24:59. Training loss. 0.00274268863722682 Num fake examples 39462 Num true examples 41498\n",
      "  Batch 40,520  of  44,637.    Elapsed: 0:25:01. Training loss. 0.0030482192523777485 Num fake examples 39492 Num true examples 41548\n",
      "  Batch 40,560  of  44,637.    Elapsed: 0:25:02. Training loss. 0.0029841845389455557 Num fake examples 39528 Num true examples 41592\n",
      "  Batch 40,600  of  44,637.    Elapsed: 0:25:04. Training loss. 0.002889588475227356 Num fake examples 39571 Num true examples 41629\n",
      "  Batch 40,640  of  44,637.    Elapsed: 0:25:06. Training loss. 0.0030346978455781937 Num fake examples 39610 Num true examples 41670\n",
      "  Batch 40,680  of  44,637.    Elapsed: 0:25:07. Training loss. 0.004205230623483658 Num fake examples 39654 Num true examples 41706\n",
      "  Batch 40,720  of  44,637.    Elapsed: 0:25:08. Training loss. 0.007370392791926861 Num fake examples 39687 Num true examples 41753\n",
      "  Batch 40,760  of  44,637.    Elapsed: 0:25:10. Training loss. 0.00511223403736949 Num fake examples 39730 Num true examples 41790\n",
      "  Batch 40,800  of  44,637.    Elapsed: 0:25:12. Training loss. 2.5471208095550537 Num fake examples 39769 Num true examples 41831\n",
      "  Batch 40,840  of  44,637.    Elapsed: 0:25:13. Training loss. 0.004398937802761793 Num fake examples 39803 Num true examples 41877\n",
      "  Batch 40,880  of  44,637.    Elapsed: 0:25:15. Training loss. 0.00395937217399478 Num fake examples 39848 Num true examples 41912\n",
      "  Batch 40,920  of  44,637.    Elapsed: 0:25:16. Training loss. 0.004037117585539818 Num fake examples 39882 Num true examples 41958\n",
      "  Batch 40,960  of  44,637.    Elapsed: 0:25:18. Training loss. 0.005026592873036861 Num fake examples 39924 Num true examples 41996\n",
      "  Batch 41,000  of  44,637.    Elapsed: 0:25:19. Training loss. 0.004112160764634609 Num fake examples 39967 Num true examples 42033\n",
      "  Batch 41,040  of  44,637.    Elapsed: 0:25:21. Training loss. 0.005568603985011578 Num fake examples 40013 Num true examples 42067\n",
      "  Batch 41,080  of  44,637.    Elapsed: 0:25:22. Training loss. 0.004243147559463978 Num fake examples 40058 Num true examples 42102\n",
      "  Batch 41,120  of  44,637.    Elapsed: 0:25:24. Training loss. 0.004892522003501654 Num fake examples 40101 Num true examples 42139\n",
      "  Batch 41,160  of  44,637.    Elapsed: 0:25:25. Training loss. 0.0020699547603726387 Num fake examples 40139 Num true examples 42181\n",
      "  Batch 41,200  of  44,637.    Elapsed: 0:25:27. Training loss. 0.006166223436594009 Num fake examples 40186 Num true examples 42214\n",
      "  Batch 41,240  of  44,637.    Elapsed: 0:25:28. Training loss. 0.003890128107741475 Num fake examples 40222 Num true examples 42258\n",
      "  Batch 41,280  of  44,637.    Elapsed: 0:25:30. Training loss. 0.006748157553374767 Num fake examples 40256 Num true examples 42304\n",
      "  Batch 41,320  of  44,637.    Elapsed: 0:25:31. Training loss. 0.004553417209535837 Num fake examples 40289 Num true examples 42351\n",
      "  Batch 41,360  of  44,637.    Elapsed: 0:25:33. Training loss. 0.0040635522454977036 Num fake examples 40322 Num true examples 42398\n",
      "  Batch 41,400  of  44,637.    Elapsed: 0:25:34. Training loss. 2.6694929599761963 Num fake examples 40360 Num true examples 42440\n",
      "  Batch 41,440  of  44,637.    Elapsed: 0:25:36. Training loss. 0.0034508281387388706 Num fake examples 40399 Num true examples 42481\n",
      "  Batch 41,480  of  44,637.    Elapsed: 0:25:37. Training loss. 0.003685438772663474 Num fake examples 40438 Num true examples 42522\n",
      "  Batch 41,520  of  44,637.    Elapsed: 0:25:38. Training loss. 0.004573323298245668 Num fake examples 40473 Num true examples 42567\n",
      "  Batch 41,560  of  44,637.    Elapsed: 0:25:40. Training loss. 0.0042754877358675 Num fake examples 40523 Num true examples 42597\n",
      "  Batch 41,600  of  44,637.    Elapsed: 0:25:41. Training loss. 0.0032116619404405355 Num fake examples 40563 Num true examples 42637\n",
      "  Batch 41,640  of  44,637.    Elapsed: 0:25:43. Training loss. 0.002503136405721307 Num fake examples 40602 Num true examples 42678\n",
      "  Batch 41,680  of  44,637.    Elapsed: 0:25:44. Training loss. 0.0031146863475441933 Num fake examples 40640 Num true examples 42720\n",
      "  Batch 41,720  of  44,637.    Elapsed: 0:25:46. Training loss. 0.0029212774243205786 Num fake examples 40681 Num true examples 42759\n",
      "  Batch 41,760  of  44,637.    Elapsed: 0:25:47. Training loss. 0.003218097845092416 Num fake examples 40713 Num true examples 42807\n",
      "  Batch 41,800  of  44,637.    Elapsed: 0:25:49. Training loss. 0.0027486553881317377 Num fake examples 40751 Num true examples 42849\n",
      "  Batch 41,840  of  44,637.    Elapsed: 0:25:50. Training loss. 0.003204196458682418 Num fake examples 40794 Num true examples 42886\n",
      "  Batch 41,880  of  44,637.    Elapsed: 0:25:52. Training loss. 0.0027688806876540184 Num fake examples 40840 Num true examples 42920\n",
      "  Batch 41,920  of  44,637.    Elapsed: 0:25:53. Training loss. 0.002979062031954527 Num fake examples 40878 Num true examples 42962\n",
      "  Batch 41,960  of  44,637.    Elapsed: 0:25:55. Training loss. 0.0022801165468990803 Num fake examples 40907 Num true examples 43013\n",
      "  Batch 42,000  of  44,637.    Elapsed: 0:25:56. Training loss. 0.0028168633580207825 Num fake examples 40951 Num true examples 43049\n",
      "  Batch 42,040  of  44,637.    Elapsed: 0:25:58. Training loss. 0.0038364005740731955 Num fake examples 40991 Num true examples 43089\n",
      "  Batch 42,080  of  44,637.    Elapsed: 0:25:59. Training loss. 0.003460297826677561 Num fake examples 41035 Num true examples 43125\n",
      "  Batch 42,120  of  44,637.    Elapsed: 0:26:01. Training loss. 0.001750021823681891 Num fake examples 41069 Num true examples 43171\n",
      "  Batch 42,160  of  44,637.    Elapsed: 0:26:02. Training loss. 3.0954480171203613 Num fake examples 41111 Num true examples 43209\n",
      "  Batch 42,200  of  44,637.    Elapsed: 0:26:04. Training loss. 0.002869650721549988 Num fake examples 41149 Num true examples 43251\n",
      "  Batch 42,240  of  44,637.    Elapsed: 0:26:05. Training loss. 0.0035961344838142395 Num fake examples 41191 Num true examples 43289\n",
      "  Batch 42,280  of  44,637.    Elapsed: 0:26:07. Training loss. 0.004120892845094204 Num fake examples 41230 Num true examples 43330\n",
      "  Batch 42,320  of  44,637.    Elapsed: 0:26:08. Training loss. 0.0028978644404560328 Num fake examples 41266 Num true examples 43374\n",
      "  Batch 42,360  of  44,637.    Elapsed: 0:26:10. Training loss. 0.0026985346339643 Num fake examples 41305 Num true examples 43415\n",
      "  Batch 42,400  of  44,637.    Elapsed: 0:26:11. Training loss. 0.002862348686903715 Num fake examples 41336 Num true examples 43464\n",
      "  Batch 42,440  of  44,637.    Elapsed: 0:26:13. Training loss. 0.004221969284117222 Num fake examples 41372 Num true examples 43508\n",
      "  Batch 42,480  of  44,637.    Elapsed: 0:26:14. Training loss. 0.0031475680880248547 Num fake examples 41413 Num true examples 43547\n",
      "  Batch 42,520  of  44,637.    Elapsed: 0:26:16. Training loss. 0.003205222310498357 Num fake examples 41451 Num true examples 43589\n",
      "  Batch 42,560  of  44,637.    Elapsed: 0:26:17. Training loss. 0.0033254059962928295 Num fake examples 41486 Num true examples 43634\n",
      "  Batch 42,600  of  44,637.    Elapsed: 0:26:19. Training loss. 0.004159859381616116 Num fake examples 41525 Num true examples 43675\n",
      "  Batch 42,640  of  44,637.    Elapsed: 0:26:20. Training loss. 0.002933294977992773 Num fake examples 41563 Num true examples 43717\n",
      "  Batch 42,680  of  44,637.    Elapsed: 0:26:22. Training loss. 0.0038511937018483877 Num fake examples 41605 Num true examples 43755\n",
      "  Batch 42,720  of  44,637.    Elapsed: 0:26:23. Training loss. 0.00442337105050683 Num fake examples 41644 Num true examples 43796\n",
      "  Batch 42,760  of  44,637.    Elapsed: 0:26:25. Training loss. 0.003920701332390308 Num fake examples 41674 Num true examples 43846\n",
      "  Batch 42,800  of  44,637.    Elapsed: 0:26:26. Training loss. 0.0035247476771473885 Num fake examples 41717 Num true examples 43883\n",
      "  Batch 42,840  of  44,637.    Elapsed: 0:26:28. Training loss. 0.00293427100405097 Num fake examples 41755 Num true examples 43925\n",
      "  Batch 42,880  of  44,637.    Elapsed: 0:26:29. Training loss. 0.0024421964772045612 Num fake examples 41796 Num true examples 43964\n",
      "  Batch 42,920  of  44,637.    Elapsed: 0:26:31. Training loss. 0.002631774637848139 Num fake examples 41841 Num true examples 43999\n",
      "  Batch 42,960  of  44,637.    Elapsed: 0:26:32. Training loss. 0.00256137503311038 Num fake examples 41876 Num true examples 44044\n",
      "  Batch 43,000  of  44,637.    Elapsed: 0:26:34. Training loss. 3.112977981567383 Num fake examples 41913 Num true examples 44087\n",
      "  Batch 43,040  of  44,637.    Elapsed: 0:26:35. Training loss. 0.003341366769745946 Num fake examples 41953 Num true examples 44127\n",
      "  Batch 43,080  of  44,637.    Elapsed: 0:26:37. Training loss. 0.00261736661195755 Num fake examples 41996 Num true examples 44164\n",
      "  Batch 43,120  of  44,637.    Elapsed: 0:26:38. Training loss. 0.0026261722669005394 Num fake examples 42026 Num true examples 44214\n",
      "  Batch 43,160  of  44,637.    Elapsed: 0:26:40. Training loss. 0.003908462356775999 Num fake examples 42058 Num true examples 44262\n",
      "  Batch 43,200  of  44,637.    Elapsed: 0:26:41. Training loss. 0.0022919524926692247 Num fake examples 42101 Num true examples 44299\n",
      "  Batch 43,240  of  44,637.    Elapsed: 0:26:43. Training loss. 0.002009976888075471 Num fake examples 42141 Num true examples 44339\n",
      "  Batch 43,280  of  44,637.    Elapsed: 0:26:44. Training loss. 0.0022170506417751312 Num fake examples 42177 Num true examples 44383\n",
      "  Batch 43,320  of  44,637.    Elapsed: 0:26:46. Training loss. 0.0018129495438188314 Num fake examples 42211 Num true examples 44429\n",
      "  Batch 43,360  of  44,637.    Elapsed: 0:26:47. Training loss. 0.002613607794046402 Num fake examples 42244 Num true examples 44476\n",
      "  Batch 43,400  of  44,637.    Elapsed: 0:26:49. Training loss. 0.00203654239885509 Num fake examples 42288 Num true examples 44512\n",
      "  Batch 43,440  of  44,637.    Elapsed: 0:26:50. Training loss. 0.0022341059520840645 Num fake examples 42328 Num true examples 44552\n",
      "  Batch 43,480  of  44,637.    Elapsed: 0:26:51. Training loss. 3.102918863296509 Num fake examples 42362 Num true examples 44598\n",
      "  Batch 43,520  of  44,637.    Elapsed: 0:26:53. Training loss. 0.003250128123909235 Num fake examples 42399 Num true examples 44641\n",
      "  Batch 43,560  of  44,637.    Elapsed: 0:26:54. Training loss. 0.003034741384908557 Num fake examples 42437 Num true examples 44683\n",
      "  Batch 43,600  of  44,637.    Elapsed: 0:26:56. Training loss. 0.00214404403232038 Num fake examples 42479 Num true examples 44721\n",
      "  Batch 43,640  of  44,637.    Elapsed: 0:26:57. Training loss. 0.0018464400200173259 Num fake examples 42528 Num true examples 44752\n",
      "  Batch 43,680  of  44,637.    Elapsed: 0:26:59. Training loss. 0.0021492200903594494 Num fake examples 42575 Num true examples 44785\n",
      "  Batch 43,720  of  44,637.    Elapsed: 0:27:00. Training loss. 0.002311813412234187 Num fake examples 42616 Num true examples 44824\n",
      "  Batch 43,760  of  44,637.    Elapsed: 0:27:02. Training loss. 0.001983420457690954 Num fake examples 42655 Num true examples 44865\n",
      "  Batch 43,800  of  44,637.    Elapsed: 0:27:03. Training loss. 0.0035193869844079018 Num fake examples 42687 Num true examples 44913\n",
      "  Batch 43,840  of  44,637.    Elapsed: 0:27:05. Training loss. 0.0032260289881378412 Num fake examples 42725 Num true examples 44955\n",
      "  Batch 43,880  of  44,637.    Elapsed: 0:27:06. Training loss. 0.0027043968439102173 Num fake examples 42769 Num true examples 44991\n",
      "  Batch 43,920  of  44,637.    Elapsed: 0:27:08. Training loss. 0.0028782968875020742 Num fake examples 42810 Num true examples 45030\n",
      "  Batch 43,960  of  44,637.    Elapsed: 0:27:09. Training loss. 0.0018956023268401623 Num fake examples 42852 Num true examples 45068\n",
      "  Batch 44,000  of  44,637.    Elapsed: 0:27:11. Training loss. 0.0022698224056512117 Num fake examples 42893 Num true examples 45107\n",
      "  Batch 44,040  of  44,637.    Elapsed: 0:27:12. Training loss. 0.0030852658674120903 Num fake examples 42927 Num true examples 45153\n",
      "  Batch 44,080  of  44,637.    Elapsed: 0:27:14. Training loss. 0.0030030396301299334 Num fake examples 42963 Num true examples 45197\n",
      "  Batch 44,120  of  44,637.    Elapsed: 0:27:15. Training loss. 2.903632164001465 Num fake examples 42999 Num true examples 45241\n",
      "  Batch 44,160  of  44,637.    Elapsed: 0:27:17. Training loss. 0.00381736783310771 Num fake examples 43034 Num true examples 45286\n",
      "  Batch 44,200  of  44,637.    Elapsed: 0:27:18. Training loss. 0.004712305031716824 Num fake examples 43079 Num true examples 45321\n",
      "  Batch 44,240  of  44,637.    Elapsed: 0:27:20. Training loss. 0.0030721258372068405 Num fake examples 43117 Num true examples 45363\n",
      "  Batch 44,280  of  44,637.    Elapsed: 0:27:21. Training loss. 0.004941303748637438 Num fake examples 43148 Num true examples 45412\n",
      "  Batch 44,320  of  44,637.    Elapsed: 0:27:23. Training loss. 0.0036711834836751223 Num fake examples 43190 Num true examples 45450\n",
      "  Batch 44,360  of  44,637.    Elapsed: 0:27:24. Training loss. 0.0032206240575760603 Num fake examples 43230 Num true examples 45490\n",
      "  Batch 44,400  of  44,637.    Elapsed: 0:27:26. Training loss. 0.002679704688489437 Num fake examples 43264 Num true examples 45536\n",
      "  Batch 44,440  of  44,637.    Elapsed: 0:27:27. Training loss. 0.0026013231836259365 Num fake examples 43300 Num true examples 45580\n",
      "  Batch 44,480  of  44,637.    Elapsed: 0:27:29. Training loss. 0.0022171372547745705 Num fake examples 43339 Num true examples 45621\n",
      "  Batch 44,520  of  44,637.    Elapsed: 0:27:30. Training loss. 0.0021564881317317486 Num fake examples 43370 Num true examples 45670\n",
      "  Batch 44,560  of  44,637.    Elapsed: 0:27:32. Training loss. 0.0020569360349327326 Num fake examples 43409 Num true examples 45711\n",
      "  Batch 44,600  of  44,637.    Elapsed: 0:27:33. Training loss. 0.0019823708571493626 Num fake examples 43446 Num true examples 45754\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epcoh took: 0:27:34\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.97\n",
      "  Validation Loss: 0.20\n",
      "  Validation took: 0:01:40\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:58:36 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "# model.to(device)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "for x in range(1): # this will only run once and will run with distilBert instead of Bert\n",
    "    x = 1\n",
    "\n",
    "# for x in range(2): # commented out to skip first training as it takes 4 hours\n",
    "    \n",
    "    if x == 1:\n",
    "        model = d_model\n",
    "        train_dataloader = d_train_dataloader\n",
    "        validation_dataloader = d_validation_dataloader\n",
    "        optimizer = d_optimizer\n",
    "        \n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    \n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "        \n",
    "        # Perform one full pass over the training set.\n",
    "    \n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "    \n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "    \n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "    \n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        total_fake_examples = 0\n",
    "        total_true_examples = 0\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # if step > 2000:\n",
    "            #     break\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                \n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Training loss. {:} Num fake examples {:} Num true examples {:}'.format(step, len(train_dataloader), elapsed, train_loss,total_fake_examples, total_true_examples ))\n",
    "    \n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(torch.int64).to(device)\n",
    "            total_fake_examples += (b_labels == 1).sum().item()\n",
    "            total_true_examples += (b_labels == 0).sum().item()\n",
    "            #print (f\"{b_labels.shape=}\")\n",
    "            b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "            #print (b_input_ids.shape, b_labels.shape, b_input_mask.shape, b_labels_one_hot.shape, b_labels_one_hot.dtype)\n",
    "    \n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because \n",
    "            # accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            model.zero_grad()        \n",
    "    \n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "            output = model(b_input_ids, \n",
    "                                 # token_type_ids=None, \n",
    "                                 attention_mask=b_input_mask, \n",
    "                                 labels=b_labels_one_hot)\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "    \n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            train_loss= loss.item()\n",
    "            total_train_loss += train_loss\n",
    "    \n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "    \n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "            #print (f\"Training loss\", loss.item())\n",
    "    \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "            \n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "    \n",
    "        t0 = time.time()\n",
    "    \n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "    \n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "    \n",
    "        # Evaluate data for one epoch\n",
    "        \n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # if step > 2000:\n",
    "            #     break\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(torch.int64).to(device)\n",
    "            b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "            \n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "    \n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                \n",
    "                output = model(b_input_ids, \n",
    "                                       # token_type_ids=None, \n",
    "                                       attention_mask=b_input_mask,\n",
    "                                       labels=b_labels_one_hot)\n",
    "                loss = output.loss\n",
    "                logits = output.logits\n",
    "    \n",
    "    \n",
    "                \n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "    \n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            \n",
    "    \n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        \n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "    \n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "        #Save model checkpoint\n",
    "        model.save_pretrained(SAVE_DIR)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b07547f-297f-46ae-bf8c-70d04108106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch  Training Loss  Valid. Loss  Valid. Accur. Training Time  \\\n",
      "0      1       0.183926     0.181807       0.970161       0:26:56   \n",
      "1      2       0.183948     0.183410       0.970161       0:27:58   \n",
      "2      3       0.184093     0.191016       0.970161       0:29:15   \n",
      "3      4       0.184358     0.195673       0.970161       0:27:34   \n",
      "\n",
      "  Validation Time  \n",
      "0         0:01:40  \n",
      "1         0:01:44  \n",
      "2         0:01:47  \n",
      "3         0:01:40  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e5cc0 th {\n",
       "  background-color: #f2f2f2;\n",
       "  text-align: center;\n",
       "}\n",
       "#T_e5cc0 td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e5cc0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e5cc0_level0_col0\" class=\"col_heading level0 col0\" >epoch</th>\n",
       "      <th id=\"T_e5cc0_level0_col1\" class=\"col_heading level0 col1\" >Training Loss</th>\n",
       "      <th id=\"T_e5cc0_level0_col2\" class=\"col_heading level0 col2\" >Valid. Loss</th>\n",
       "      <th id=\"T_e5cc0_level0_col3\" class=\"col_heading level0 col3\" >Valid. Accur.</th>\n",
       "      <th id=\"T_e5cc0_level0_col4\" class=\"col_heading level0 col4\" >Training Time</th>\n",
       "      <th id=\"T_e5cc0_level0_col5\" class=\"col_heading level0 col5\" >Validation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e5cc0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e5cc0_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_e5cc0_row0_col1\" class=\"data row0 col1\" >0.183926</td>\n",
       "      <td id=\"T_e5cc0_row0_col2\" class=\"data row0 col2\" >0.181807</td>\n",
       "      <td id=\"T_e5cc0_row0_col3\" class=\"data row0 col3\" >0.970161</td>\n",
       "      <td id=\"T_e5cc0_row0_col4\" class=\"data row0 col4\" >0:26:56</td>\n",
       "      <td id=\"T_e5cc0_row0_col5\" class=\"data row0 col5\" >0:01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5cc0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e5cc0_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_e5cc0_row1_col1\" class=\"data row1 col1\" >0.183948</td>\n",
       "      <td id=\"T_e5cc0_row1_col2\" class=\"data row1 col2\" >0.183410</td>\n",
       "      <td id=\"T_e5cc0_row1_col3\" class=\"data row1 col3\" >0.970161</td>\n",
       "      <td id=\"T_e5cc0_row1_col4\" class=\"data row1 col4\" >0:27:58</td>\n",
       "      <td id=\"T_e5cc0_row1_col5\" class=\"data row1 col5\" >0:01:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5cc0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e5cc0_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_e5cc0_row2_col1\" class=\"data row2 col1\" >0.184093</td>\n",
       "      <td id=\"T_e5cc0_row2_col2\" class=\"data row2 col2\" >0.191016</td>\n",
       "      <td id=\"T_e5cc0_row2_col3\" class=\"data row2 col3\" >0.970161</td>\n",
       "      <td id=\"T_e5cc0_row2_col4\" class=\"data row2 col4\" >0:29:15</td>\n",
       "      <td id=\"T_e5cc0_row2_col5\" class=\"data row2 col5\" >0:01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e5cc0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e5cc0_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_e5cc0_row3_col1\" class=\"data row3 col1\" >0.184358</td>\n",
       "      <td id=\"T_e5cc0_row3_col2\" class=\"data row3 col2\" >0.195673</td>\n",
       "      <td id=\"T_e5cc0_row3_col3\" class=\"data row3 col3\" >0.970161</td>\n",
       "      <td id=\"T_e5cc0_row3_col4\" class=\"data row3 col4\" >0:27:34</td>\n",
       "      <td id=\"T_e5cc0_row3_col5\" class=\"data row3 col5\" >0:01:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2ac51b19e20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0:26:56', '0:27:58', '0:29:15', '0:27:34']\n",
      "[datetime.datetime(1900, 1, 1, 0, 26, 56), datetime.datetime(1900, 1, 1, 0, 27, 58), datetime.datetime(1900, 1, 1, 0, 29, 15), datetime.datetime(1900, 1, 1, 0, 27, 34)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "# import dataframe_image as dfi\n",
    "\n",
    "ts_df = pd.DataFrame(training_stats)\n",
    "print(ts_df)\n",
    "styled_df = ts_df.style.set_table_styles([\n",
    "    {'selector': 'th', 'props': [('background-color', '#f2f2f2'), ('text-align', 'center')]},\n",
    "    {'selector': 'td', 'props': [('border', '1px solid black')]}\n",
    "])\n",
    "display(styled_df)\n",
    "# dfi.export(styled_df, 'styled_df.png') \n",
    "\n",
    "ep = [d['epoch'] for d in training_stats]\n",
    "\n",
    "tl = [d['Training Loss'] for d in training_stats]\n",
    "vl = [d['Valid. Loss'] for d in training_stats]\n",
    "va = [d['Valid. Accur.'] for d in training_stats]\n",
    "\n",
    "tt = [d['Training Time'] for d in training_stats]\n",
    "print(tt)\n",
    "tt = [datetime.strptime(item,'%H:%M:%S') for item in tt]\n",
    "print(tt)\n",
    "vt = [d['Validation Time'] for d in training_stats]\n",
    "vt = [datetime.strptime(item,'%H:%M:%S') for item in vt]\n",
    "\n",
    "\n",
    "plt.plot(ep, tl)\n",
    "plt.plot(ep, vl)\n",
    "plt.xticks(ep)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('BERT Training and Validation Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig('DistilBERTLoss_3.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1.0, decimals=2))\n",
    "ax.plot(ep, va)\n",
    "plt.xticks(ep)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('BERT Validation Accuracy')\n",
    "plt.savefig('DistilBERTAcc_3.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.yaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S',))\n",
    "ax.yaxis.set_minor_locator(mticker.AutoMinorLocator())\n",
    "ax.plot(ep, tt)\n",
    "ax.plot(ep, vt)\n",
    "plt.xticks(ep)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Time')\n",
    "plt.title('BERT Train and Validation Time')\n",
    "plt.legend(['Train', 'Validation'], loc='center left')\n",
    "plt.savefig('DistilBERTTime_3.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.legend(['Train', 'Validation'], loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6bb68-246e-4093-afb2-cf4363067b8a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "873821bc-1f66-44fe-acd0-7a312c358621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abate\\anaconda3\\envs\\test_env_gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 5.3244, -5.3105]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "def encode(sentence):\n",
    "    return tokenizer.encode_plus(\n",
    "                        sentence,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 410,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "encoded_dict = encode(sentences[SENTENCE_INDEX])\n",
    "input_id = encoded_dict['input_ids']\n",
    "attention_mask = encoded_dict['attention_mask']\n",
    "print (input_id.shape)\n",
    "model.eval()\n",
    "output = model(\n",
    "            input_id.cuda(),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=attention_mask.cuda(), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdd7d7-aae7-4b04-afcc-664161c3e215",
   "metadata": {},
   "source": [
    "## Using validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7064487-72c4-43ba-a7ac-128221794554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a534156a-198b-495a-94b8-20b3e28f10a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 410])\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[nan, nan]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SENTENCE_INDEX = 5000\n",
    "model.eval()\n",
    "print (torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0).shape)\n",
    "output = model(torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(), dim=0),\n",
    "            token_type_ids=None, \n",
    "            attention_mask=torch.unsqueeze(val_dataset[SENTENCE_INDEX][0].cuda(),dim=0), return_dict=True)\n",
    "print (output)\n",
    "print (labels[SENTENCE_INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca1659b2-4081-4161-b331-c0092badb90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.3243, -5.3206],\n",
      "        [-5.2057,  5.1967]], device='cuda:0') tensor([0, 1], device='cuda:0')\n",
      "tensor([[ 5.3234, -5.3117],\n",
      "        [ 5.3278, -5.3160]], device='cuda:0') tensor([0, 0], device='cuda:0')\n",
      "tensor([[ 5.3254, -5.3102],\n",
      "        [ 5.3244, -5.3161]], device='cuda:0') tensor([0, 0], device='cuda:0')\n",
      "tensor([[-5.1353,  5.1972],\n",
      "        [ 5.3283, -5.3043]], device='cuda:0') tensor([1, 0], device='cuda:0')\n",
      "tensor([[ 5.3247, -5.3024],\n",
      "        [-5.2182,  5.2226]], device='cuda:0') tensor([0, 1], device='cuda:0')\n",
      "tensor([[-5.2218,  5.2306],\n",
      "        [ 5.3256, -5.3067]], device='cuda:0') tensor([1, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(validation_dataloader):\n",
    "    if step > 5:\n",
    "        break\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(torch.int64).to(device)\n",
    "    b_labels_one_hot = torch.nn.functional.one_hot(b_labels, num_classes=2).float()\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        \n",
    "        output = model(b_input_ids, \n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=b_input_mask,\n",
    "                               labels=b_labels_one_hot)\n",
    "        loss = output.loss\n",
    "        logits = output.logits\n",
    "        print (logits, b_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f170c-91e6-4153-a3cf-872a6550bf69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
